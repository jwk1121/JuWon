[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JuWon Kwon",
    "section": "",
    "text": "바이오메디컬공학을 전공하고 있는 학부생 입니다.\n초음파와 딥러닝을 공부하고 있습니다."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "JuWon Kwon",
    "section": "",
    "text": "바이오메디컬공학을 전공하고 있는 학부생 입니다.\n초음파와 딥러닝을 공부하고 있습니다."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "rstat101.html",
    "href": "rstat101.html",
    "title": "Tutorial",
    "section": "",
    "text": "test12414입니\n123123다"
  },
  {
    "objectID": "tutorial_1.html",
    "href": "tutorial_1.html",
    "title": "test123",
    "section": "",
    "text": "안녕하세요 권주원입니다\nqmd test 진행중입니다"
  },
  {
    "objectID": "docs/rstat/rstat101.html",
    "href": "docs/rstat/rstat101.html",
    "title": "Tutorial",
    "section": "",
    "text": "test12414입니\n123123다"
  },
  {
    "objectID": "docs/rstat/rstat_lecture.html",
    "href": "docs/rstat/rstat_lecture.html",
    "title": "rstat lecture",
    "section": "",
    "text": "1강\n2강\n3강"
  },
  {
    "objectID": "docs/rstat/index.html",
    "href": "docs/rstat/index.html",
    "title": "Quarto blog",
    "section": "",
    "text": "CNN\n\n\nCNN part1 test\n\n\n\n\nDL\n\n\nCNN\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nTutorial\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/rstat/posts/rstat101.html",
    "href": "docs/rstat/posts/rstat101.html",
    "title": "Tutorial",
    "section": "",
    "text": "test12414입니\n123123다"
  },
  {
    "objectID": "docs/rstat/posts/rstat_lecture.html",
    "href": "docs/rstat/posts/rstat_lecture.html",
    "title": "rstat lecture",
    "section": "",
    "text": "1강\n2강\n3강"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html",
    "href": "docs/rstat/posts/DL_CNN.html",
    "title": "CNN",
    "section": "",
    "text": "toc:true\n\n\nimport torch \nimport torchvision\nfrom fastai.vision.all import * \nimport time\n\n\n\n간략하게 CNN의 구조를 설명하자면 - conv layer: 커널층(선형변환처럼 feature를 늘려주는 역할) - ReLU layer: 비선형을 추가해 표현력을 늘려주는 역할 - pooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n# (굳이 (2,2) 이런식으로 안해도 됨 어차피 윈도우 사이즈는 정사각행렬이므로 상수하나만 입력해도 됨됨)\n\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv.weight.data = torch.tensor([[[[0.25, 0.25],[0.25,0.25]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]), tensor([0.]))\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임) - 해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음 - 윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함 - 첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용 - 두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n\n\n\n\n사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.\n\\(ReLU(x) = \\max(0,x)\\)\n\n\n\n\n\npooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\nmaxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_maxpooling(_X) \n\ntensor([[[ 6.,  8.],\n         [16., 18.]]])\n\n\n\nmaxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\npooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n\n이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다\n\n\n\n\n\n\n\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02&lt;00:00]\n    \n    \n\n\n\n\n\n(path/'원하는 경로').ls()\n\n\n\ntorchvision.io.read_image()\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/1').ls()])\nx_tr = torch.concat([x0, x1])/255\n\ny_tr = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/1').ls()])\nx_test = torch.concat([x0, x1])/255\n\ny_test = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\nx_tr.shape, y_tr.shape, x_test.shape, y_test.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([2115, 1]))\n\n\n\n\n\n# conv\nc1 = torch.nn.Conv2d(1, 16 , 5) # 만약 color image였다면 입력 채널의 수를 3으로 지정해야 함\nx_tr.shape ,c1(x_tr).shape\n\n(torch.Size([12665, 1, 28, 28]), torch.Size([12665, 16, 24, 24]))\n\n\n\nsize계산 공식: 윈도우(커널)사이즈가 n이면 \\(size = height(width) - ( n - 1)\\)\n\n\n# ReLU\na1 = torch.nn.ReLU()\n\n\n# maxpooling\nm1 = torch.nn.MaxPool2d(2)\nprint(m1(a1(c1(x_tr))).shape)\n\ntorch.Size([12665, 16, 12, 12])\n\n\n\n행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n\n# flatten(이미지를 한줄로 펼치는 것)\nf1 = torch.nn.Flatten()\nprint(f1(a1(m1(c1(x_tr)))).shape) #16 * 12 * 12 = 2304\n\ntorch.Size([12665, 2304])\n\n\n\n# sigmoid에 올리기 위해서는 2304 디멘젼을 1로 만들어야함\nl1=torch.nn.Linear(in_features=2304,out_features=1) \n\n\n# sigmoid (값이 0에서 1사이의 값, 즉 확률로 출력되도록)\na2 = torch.nn.Sigmoid()\nprint(a2(f1(a1(m1(c1(x_tr))))).shape)\n\ntorch.Size([12665, 2304])\n\n\n\nprint('이미지 사이즈:                     ', x_tr.shape)\nprint('conv:                              ',c1(x_tr).shape)\nprint('(ReLU) -&gt; maxpooling:              ',m1(a1(c1(x_tr))).shape)\nprint('이미지 펼치기:                     ',f1(a1(m1(c1(x_tr)))).shape)\nprint('이미지를 하나의 스칼라로 선형변환: ',l1(f1(a1(m1(c1(x_tr))))).shape)\nprint('시그모이드:                        ',a2(l1(f1(a1(m1(c1(x_tr)))))).shape)\n\n이미지 사이즈:                      torch.Size([12665, 1, 28, 28])\nconv:                               torch.Size([12665, 16, 24, 24])\n(ReLU) -&gt; maxpooling:               torch.Size([12665, 16, 12, 12])\n이미지 펼치기:                      torch.Size([12665, 2304])\n이미지를 하나의 스칼라로 선형변환:  torch.Size([12665, 1])\n시그모이드:                         torch.Size([12665, 1])\n\n\n\n\n\n\n원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\\(\\to\\) overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n# 데이터 로더에 들어갈 데이터세트 준비\nds_tr = torch.utils.data.TensorDataset(x_tr, y_tr)\nds_test = torch.utils.data.TensorDataset(x_test, y_test)\n\n\n데이터 로더는 배치크기를 지정해줘야 함\n\n\nlen(x_tr), len(x_test)\n\n(12665, 2115)\n\n\n\n데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n\nstochastic gradient descent(구용어: mini-batch gradient descent)\n\n\n\n# 데이터 로더\ndl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=1266) \ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=2115) \n\n\ndls = DataLoaders(dl_tr,dl_test)\n\n\nnet = torch.nn.Sequential(\n    # conv layer \n    c1, \n    # ReLU\n    a1, \n    # maxpool\n    m1, \n    # flatten\n    f1, \n    # linear transform (n -&gt; 1)\n    l1,\n    # Sigmoid\n    a2\n)\n\nloss_fn=torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn) # fastai의 Learner는 오미타이저의 기본값이 adam이므로 따로 지정해주지 않아도 됨\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.979273\n0.639250\n00:05\n\n\n1\n0.703008\n0.402466\n00:00\n\n\n2\n0.547401\n0.256881\n00:00\n\n\n3\n0.434025\n0.142217\n00:00\n\n\n4\n0.340996\n0.079636\n00:00\n\n\n5\n0.267902\n0.048050\n00:00\n\n\n6\n0.211895\n0.031742\n00:00\n\n\n7\n0.169176\n0.022921\n00:00\n\n\n8\n0.136331\n0.017658\n00:00\n\n\n9\n0.110770\n0.014233\n00:00\n\n\n\n\n\n\nLearner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음 &gt; python    net[0].weight 위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = ’cuda:0’라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다.\n+ tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n\nnet = net.to('cpu')\nplt.plot(net(x_tr).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(x_test).data,'.')\nplt.title(\"Testing Set\",size=15)\n\nText(0.5, 1.0, 'Testing Set')\n\n\n\n\n\n\n\n\n\n\n\n\nBCEWithLogitsLoss = Sigmoid + BCELoss\n손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음\n\n\n\n\n\n\n다중(k개) 클래스를 분류 - LossFunction: CrossEntrophyLoss\n- ActivationFunction: SoftmaxFunction - 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n\n소프트맥스 함수가 계산하는 과정은 아래와 같음 (3개를 분류할 경우) \\(softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}\\)\n\n\n\n\n\nk개의 클래스를 분류하는 모델의 Loss 계산 방법\n\nsftmax(_netout) # -&gt; 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -&gt; 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -&gt; 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n+ 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임\n+ 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n\n\n\ntype 1) int형을 사용하는 방법 (vector)\ntype 2) float형을 사용하는 방법 (one-hot encoded vector)\n만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n\ntype 1)의 경우 &lt;사슴: 0, 강아지: 1, 고양이: 2&gt; (단, 데이터 형태는는 int(정수))\ntype 2)의 경우 &lt;사슴: [1, 0, 0], 강아지: [0, 1, 0], 고양이: [0, 0, 1] &gt; (단, 데이터의 형태는 float(실수))\n\n\n\n\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n\n\ntorch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n\n\n\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\ntorch.nn.functional.one_hot(yy).float()\n\ntensor([[1., 0.],\n        [1., 0.],\n        [1., 0.],\n        ...,\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])\n\n\n\n\n\n\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\nlen(X) # 18623\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번= 1862 꽉 채워서 10번하고 3개정도 남은 걸로 한 번\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # test는 전부다 넣어서\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\n\nloss_fn = torch.nn.CrossEntropyLoss() # 여기에는 softmax함수가 내장되어 있음 \n#-&gt; net마지막에 softmax를 넣으면 안됨\n# BCEWithLogitsLoss 느낌(시그모이드 + BCE)\n\n\nlrnr = Learner(dls,net,loss_fn) # 옵티마이저는 아답이 디폴트 값이어서 굳이 안넣어도 됨\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.797101\n1.103916\n00:00\n\n\n1\n1.267011\n0.823797\n00:00\n\n\n2\n1.055513\n0.667210\n00:00\n\n\n3\n0.910746\n0.435414\n00:00\n\n\n4\n0.762887\n0.284001\n00:00\n\n\n5\n0.625515\n0.199502\n00:00\n\n\n6\n0.515352\n0.152906\n00:00\n\n\n7\n0.429145\n0.123678\n00:00\n\n\n8\n0.359694\n0.105466\n00:00\n\n\n9\n0.303888\n0.092883\n00:00\n\n\n\n\n\n\nlrnr.model.to(\"cpu\")\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n3142\n-1.138366\n-5.435792\n0.370670\n2\n\n\n3143\n-4.458741\n-4.281343\n2.052410\n2\n\n\n3144\n-2.836508\n-3.204013\n0.012610\n2\n\n\n3145\n-1.704158\n-10.621873\n2.024313\n2\n\n\n3146\n-2.467648\n-4.612999\n0.656284\n2\n\n\n\n\n\n3147 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n975\n0.432969\n-5.653580\n-1.944451\n0\n\n\n976\n2.685695\n-10.254354\n-2.466680\n0\n\n\n977\n2.474842\n-9.650204\n-2.452746\n0\n\n\n978\n1.268743\n-6.928779\n-1.419695\n0\n\n\n979\n4.371464\n-8.959077\n-4.056257\n0\n\n\n\n\n\n980 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼\n\n\n\n\n\n이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n\n\n\n\nfastai에서 지원\nfastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\ny의 형태를 주의해서 사용해야 함\n\n앞서 말한 것처럼 두 가지 타입이 있음\n\nvector + int\none-hot encoded vector + float\n\n\n\ntype 1) y의 형태가 vector + int일 때 - metrics = accuracy를 사용해야 함\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\ny.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\n\n\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nyy.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n1.128365\n0.601474\n0.463357\n0.536643\n00:00\n\n\n1\n0.684630\n0.304262\n0.975414\n0.024586\n00:00\n\n\n2\n0.503124\n0.144147\n0.989598\n0.010402\n00:00\n\n\n3\n0.373899\n0.068306\n0.996217\n0.003783\n00:00\n\n\n4\n0.281332\n0.040790\n0.996217\n0.003783\n00:00\n\n\n5\n0.215743\n0.026980\n0.996690\n0.003310\n00:00\n\n\n6\n0.168349\n0.019467\n0.996690\n0.003310\n00:00\n\n\n7\n0.133313\n0.014856\n0.998109\n0.001891\n00:00\n\n\n8\n0.106776\n0.011745\n0.998109\n0.001891\n00:00\n\n\n9\n0.086275\n0.009553\n0.999054\n0.000946\n00:00\n\n\n\n\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때 - metrics = accuracy_multi를 사용해야 함 - error_rate는 사용못함\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n\n\ny_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\nyy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\ntorch.nn.functional.one_hot() 함수 조건\n\n기본적으로 크기가 n인 벡터가 들어오길 기대\n정수가 들어오는 것을 기대\n\n하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n1.213650\n0.647844\n0.463357\n00:00\n\n\n1\n0.744869\n0.384000\n0.933570\n00:00\n\n\n2\n0.571879\n0.187119\n0.986525\n00:00\n\n\n3\n0.432096\n0.090841\n0.994563\n00:00\n\n\n4\n0.326718\n0.048868\n0.995508\n00:00\n\n\n5\n0.250235\n0.028477\n0.996217\n00:00\n\n\n6\n0.194605\n0.018616\n0.996454\n00:00\n\n\n7\n0.153338\n0.013278\n0.996927\n00:00\n\n\n8\n0.122056\n0.010130\n0.997872\n00:00\n\n\n9\n0.097957\n0.008112\n0.998109\n00:00"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#cnn-구조",
    "href": "docs/rstat/posts/DL_CNN.html#cnn-구조",
    "title": "CNN",
    "section": "",
    "text": "간략하게 CNN의 구조를 설명하자면 - conv layer: 커널층(선형변환처럼 feature를 늘려주는 역할) - ReLU layer: 비선형을 추가해 표현력을 늘려주는 역할 - pooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n# (굳이 (2,2) 이런식으로 안해도 됨 어차피 윈도우 사이즈는 정사각행렬이므로 상수하나만 입력해도 됨됨)\n\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv.weight.data = torch.tensor([[[[0.25, 0.25],[0.25,0.25]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]), tensor([0.]))\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임) - 해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음 - 윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함 - 첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용 - 두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n\n\n\n\n사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.\n\\(ReLU(x) = \\max(0,x)\\)\n\n\n\n\n\npooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\nmaxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_maxpooling(_X) \n\ntensor([[[ 6.,  8.],\n         [16., 18.]]])\n\n\n\nmaxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\npooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n\n이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#cnn-구현",
    "href": "docs/rstat/posts/DL_CNN.html#cnn-구현",
    "title": "CNN",
    "section": "",
    "text": "path = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02&lt;00:00]"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#path-사용법-데이터-준비",
    "href": "docs/rstat/posts/DL_CNN.html#path-사용법-데이터-준비",
    "title": "CNN",
    "section": "",
    "text": "(path/'원하는 경로').ls()"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#위-코드를-사용하면-폴더-안에-있는-데이터들의-이름을-출력",
    "href": "docs/rstat/posts/DL_CNN.html#위-코드를-사용하면-폴더-안에-있는-데이터들의-이름을-출력",
    "title": "CNN",
    "section": "",
    "text": "torchvision.io.read_image()\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/1').ls()])\nx_tr = torch.concat([x0, x1])/255\n\ny_tr = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/1').ls()])\nx_test = torch.concat([x0, x1])/255\n\ny_test = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\nx_tr.shape, y_tr.shape, x_test.shape, y_test.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([2115, 1]))\n\n\n\n\n\n# conv\nc1 = torch.nn.Conv2d(1, 16 , 5) # 만약 color image였다면 입력 채널의 수를 3으로 지정해야 함\nx_tr.shape ,c1(x_tr).shape\n\n(torch.Size([12665, 1, 28, 28]), torch.Size([12665, 16, 24, 24]))\n\n\n\nsize계산 공식: 윈도우(커널)사이즈가 n이면 \\(size = height(width) - ( n - 1)\\)\n\n\n# ReLU\na1 = torch.nn.ReLU()\n\n\n# maxpooling\nm1 = torch.nn.MaxPool2d(2)\nprint(m1(a1(c1(x_tr))).shape)\n\ntorch.Size([12665, 16, 12, 12])\n\n\n\n행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n\n# flatten(이미지를 한줄로 펼치는 것)\nf1 = torch.nn.Flatten()\nprint(f1(a1(m1(c1(x_tr)))).shape) #16 * 12 * 12 = 2304\n\ntorch.Size([12665, 2304])\n\n\n\n# sigmoid에 올리기 위해서는 2304 디멘젼을 1로 만들어야함\nl1=torch.nn.Linear(in_features=2304,out_features=1) \n\n\n# sigmoid (값이 0에서 1사이의 값, 즉 확률로 출력되도록)\na2 = torch.nn.Sigmoid()\nprint(a2(f1(a1(m1(c1(x_tr))))).shape)\n\ntorch.Size([12665, 2304])\n\n\n\nprint('이미지 사이즈:                     ', x_tr.shape)\nprint('conv:                              ',c1(x_tr).shape)\nprint('(ReLU) -&gt; maxpooling:              ',m1(a1(c1(x_tr))).shape)\nprint('이미지 펼치기:                     ',f1(a1(m1(c1(x_tr)))).shape)\nprint('이미지를 하나의 스칼라로 선형변환: ',l1(f1(a1(m1(c1(x_tr))))).shape)\nprint('시그모이드:                        ',a2(l1(f1(a1(m1(c1(x_tr)))))).shape)\n\n이미지 사이즈:                      torch.Size([12665, 1, 28, 28])\nconv:                               torch.Size([12665, 16, 24, 24])\n(ReLU) -&gt; maxpooling:               torch.Size([12665, 16, 12, 12])\n이미지 펼치기:                      torch.Size([12665, 2304])\n이미지를 하나의 스칼라로 선형변환:  torch.Size([12665, 1])\n시그모이드:                         torch.Size([12665, 1])\n\n\n\n\n\n\n원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\\(\\to\\) overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n# 데이터 로더에 들어갈 데이터세트 준비\nds_tr = torch.utils.data.TensorDataset(x_tr, y_tr)\nds_test = torch.utils.data.TensorDataset(x_test, y_test)\n\n\n데이터 로더는 배치크기를 지정해줘야 함\n\n\nlen(x_tr), len(x_test)\n\n(12665, 2115)\n\n\n\n데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n\nstochastic gradient descent(구용어: mini-batch gradient descent)\n\n\n\n# 데이터 로더\ndl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=1266) \ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=2115) \n\n\ndls = DataLoaders(dl_tr,dl_test)\n\n\nnet = torch.nn.Sequential(\n    # conv layer \n    c1, \n    # ReLU\n    a1, \n    # maxpool\n    m1, \n    # flatten\n    f1, \n    # linear transform (n -&gt; 1)\n    l1,\n    # Sigmoid\n    a2\n)\n\nloss_fn=torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn) # fastai의 Learner는 오미타이저의 기본값이 adam이므로 따로 지정해주지 않아도 됨\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.979273\n0.639250\n00:05\n\n\n1\n0.703008\n0.402466\n00:00\n\n\n2\n0.547401\n0.256881\n00:00\n\n\n3\n0.434025\n0.142217\n00:00\n\n\n4\n0.340996\n0.079636\n00:00\n\n\n5\n0.267902\n0.048050\n00:00\n\n\n6\n0.211895\n0.031742\n00:00\n\n\n7\n0.169176\n0.022921\n00:00\n\n\n8\n0.136331\n0.017658\n00:00\n\n\n9\n0.110770\n0.014233\n00:00\n\n\n\n\n\n\nLearner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음 &gt; python    net[0].weight 위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = ’cuda:0’라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다.\n+ tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n\nnet = net.to('cpu')\nplt.plot(net(x_tr).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(x_test).data,'.')\nplt.title(\"Testing Set\",size=15)\n\nText(0.5, 1.0, 'Testing Set')"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#loss-function",
    "href": "docs/rstat/posts/DL_CNN.html#loss-function",
    "title": "CNN",
    "section": "",
    "text": "BCEWithLogitsLoss = Sigmoid + BCELoss\n손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#k개의-클래스를-분류하는-모델",
    "href": "docs/rstat/posts/DL_CNN.html#k개의-클래스를-분류하는-모델",
    "title": "CNN",
    "section": "",
    "text": "다중(k개) 클래스를 분류 - LossFunction: CrossEntrophyLoss\n- ActivationFunction: SoftmaxFunction - 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n\n소프트맥스 함수가 계산하는 과정은 아래와 같음 (3개를 분류할 경우) \\(softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}\\)\n\n\n\n\n\nk개의 클래스를 분류하는 모델의 Loss 계산 방법\n\nsftmax(_netout) # -&gt; 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -&gt; 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -&gt; 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n+ 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임\n+ 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n\n\n\ntype 1) int형을 사용하는 방법 (vector)\ntype 2) float형을 사용하는 방법 (one-hot encoded vector)\n만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n\ntype 1)의 경우 &lt;사슴: 0, 강아지: 1, 고양이: 2&gt; (단, 데이터 형태는는 int(정수))\ntype 2)의 경우 &lt;사슴: [1, 0, 0], 강아지: [0, 1, 0], 고양이: [0, 0, 1] &gt; (단, 데이터의 형태는 float(실수))\n\n\n\n\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n\n\ntorch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n\n\n\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\ntorch.nn.functional.one_hot(yy).float()\n\ntensor([[1., 0.],\n        [1., 0.],\n        [1., 0.],\n        ...,\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])\n\n\n\n\n\n\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\nlen(X) # 18623\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번= 1862 꽉 채워서 10번하고 3개정도 남은 걸로 한 번\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # test는 전부다 넣어서\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\n\nloss_fn = torch.nn.CrossEntropyLoss() # 여기에는 softmax함수가 내장되어 있음 \n#-&gt; net마지막에 softmax를 넣으면 안됨\n# BCEWithLogitsLoss 느낌(시그모이드 + BCE)\n\n\nlrnr = Learner(dls,net,loss_fn) # 옵티마이저는 아답이 디폴트 값이어서 굳이 안넣어도 됨\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.797101\n1.103916\n00:00\n\n\n1\n1.267011\n0.823797\n00:00\n\n\n2\n1.055513\n0.667210\n00:00\n\n\n3\n0.910746\n0.435414\n00:00\n\n\n4\n0.762887\n0.284001\n00:00\n\n\n5\n0.625515\n0.199502\n00:00\n\n\n6\n0.515352\n0.152906\n00:00\n\n\n7\n0.429145\n0.123678\n00:00\n\n\n8\n0.359694\n0.105466\n00:00\n\n\n9\n0.303888\n0.092883\n00:00\n\n\n\n\n\n\nlrnr.model.to(\"cpu\")\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n3142\n-1.138366\n-5.435792\n0.370670\n2\n\n\n3143\n-4.458741\n-4.281343\n2.052410\n2\n\n\n3144\n-2.836508\n-3.204013\n0.012610\n2\n\n\n3145\n-1.704158\n-10.621873\n2.024313\n2\n\n\n3146\n-2.467648\n-4.612999\n0.656284\n2\n\n\n\n\n\n3147 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n975\n0.432969\n-5.653580\n-1.944451\n0\n\n\n976\n2.685695\n-10.254354\n-2.466680\n0\n\n\n977\n2.474842\n-9.650204\n-2.452746\n0\n\n\n978\n1.268743\n-6.928779\n-1.419695\n0\n\n\n979\n4.371464\n-8.959077\n-4.056257\n0\n\n\n\n\n\n980 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼\n\n\n\n\n\n이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n\n\n\n\nfastai에서 지원\nfastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\ny의 형태를 주의해서 사용해야 함\n\n앞서 말한 것처럼 두 가지 타입이 있음\n\nvector + int\none-hot encoded vector + float\n\n\n\ntype 1) y의 형태가 vector + int일 때 - metrics = accuracy를 사용해야 함\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\ny.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\n\n\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nyy.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n1.128365\n0.601474\n0.463357\n0.536643\n00:00\n\n\n1\n0.684630\n0.304262\n0.975414\n0.024586\n00:00\n\n\n2\n0.503124\n0.144147\n0.989598\n0.010402\n00:00\n\n\n3\n0.373899\n0.068306\n0.996217\n0.003783\n00:00\n\n\n4\n0.281332\n0.040790\n0.996217\n0.003783\n00:00\n\n\n5\n0.215743\n0.026980\n0.996690\n0.003310\n00:00\n\n\n6\n0.168349\n0.019467\n0.996690\n0.003310\n00:00\n\n\n7\n0.133313\n0.014856\n0.998109\n0.001891\n00:00\n\n\n8\n0.106776\n0.011745\n0.998109\n0.001891\n00:00\n\n\n9\n0.086275\n0.009553\n0.999054\n0.000946\n00:00\n\n\n\n\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때 - metrics = accuracy_multi를 사용해야 함 - error_rate는 사용못함\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n\n\ny_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\nyy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\ntorch.nn.functional.one_hot() 함수 조건\n\n기본적으로 크기가 n인 벡터가 들어오길 기대\n정수가 들어오는 것을 기대\n\n하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n1.213650\n0.647844\n0.463357\n00:00\n\n\n1\n0.744869\n0.384000\n0.933570\n00:00\n\n\n2\n0.571879\n0.187119\n0.986525\n00:00\n\n\n3\n0.432096\n0.090841\n0.994563\n00:00\n\n\n4\n0.326718\n0.048868\n0.995508\n00:00\n\n\n5\n0.250235\n0.028477\n0.996217\n00:00\n\n\n6\n0.194605\n0.018616\n0.996454\n00:00\n\n\n7\n0.153338\n0.013278\n0.996927\n00:00\n\n\n8\n0.122056\n0.010130\n0.997872\n00:00\n\n\n9\n0.097957\n0.008112\n0.998109\n00:00"
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "JuWon Kwon",
    "section": "",
    "text": "This is the contents of the about page for my blog."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "JuWon Kwon",
    "section": "",
    "text": "바이오메디컬공학을 전공하고 있는 학부생 입니다.\n초음파와 딥러닝을 공부하고 있습니다."
  },
  {
    "objectID": "docs/DL/index.html",
    "href": "docs/DL/index.html",
    "title": "딥러닝 정리 페이지",
    "section": "",
    "text": "DCGAN paper review\n\n\nDeep generative model - DCGAN\n\n\n\n\nDL\n\n\nGenerative model\n\n\npaper review\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nU-net paper review\n\n\nsegmantation model - Unet\n\n\n\n\nDL\n\n\nsegmantation\n\n\npaper review\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nFCN paper review\n\n\nsegmantation\n\n\n\n\nDL\n\n\nsegmantation\n\n\npaper review\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nCNN part2\n\n\nCNN part2\n\n\n\n\nDL\n\n\nCNN\n\n\nXAI\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nCNN part1\n\n\nCNN part1\n\n\n\n\nDL\n\n\nCNN\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2022\n\n\nJuWon\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html",
    "href": "docs/DL/posts/DL_CNN.html",
    "title": "CNN",
    "section": "",
    "text": "toc:true\n\n\nimport torch \nimport torchvision\nfrom fastai.vision.all import * \nimport time\n\n\n\n간략하게 CNN의 구조를 설명하자면 - conv layer: 커널층(선형변환처럼 feature를 늘려주는 역할) - ReLU layer: 비선형을 추가해 표현력을 늘려주는 역할 - pooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n# (굳이 (2,2) 이런식으로 안해도 됨 어차피 윈도우 사이즈는 정사각행렬이므로 상수하나만 입력해도 됨됨)\n\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv.weight.data = torch.tensor([[[[0.25, 0.25],[0.25,0.25]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]), tensor([0.]))\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임) - 해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음 - 윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함 - 첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용 - 두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n\n\n\n\n사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.\n\\(ReLU(x) = \\max(0,x)\\)\n\n\n\n\n\npooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\nmaxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_maxpooling(_X) \n\ntensor([[[ 6.,  8.],\n         [16., 18.]]])\n\n\n\nmaxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\npooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n\n이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다\n\n\n\n\n\n\n\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02&lt;00:00]\n    \n    \n\n\n\n\n\n(path/'원하는 경로').ls()\n\n\n\ntorchvision.io.read_image()\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/1').ls()])\nx_tr = torch.concat([x0, x1])/255\n\ny_tr = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/1').ls()])\nx_test = torch.concat([x0, x1])/255\n\ny_test = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\nx_tr.shape, y_tr.shape, x_test.shape, y_test.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([2115, 1]))\n\n\n\n\n\n# conv\nc1 = torch.nn.Conv2d(1, 16 , 5) # 만약 color image였다면 입력 채널의 수를 3으로 지정해야 함\nx_tr.shape ,c1(x_tr).shape\n\n(torch.Size([12665, 1, 28, 28]), torch.Size([12665, 16, 24, 24]))\n\n\n\nsize계산 공식: 윈도우(커널)사이즈가 n이면 \\(size = height(width) - ( n - 1)\\)\n\n\n# ReLU\na1 = torch.nn.ReLU()\n\n\n# maxpooling\nm1 = torch.nn.MaxPool2d(2)\nprint(m1(a1(c1(x_tr))).shape)\n\ntorch.Size([12665, 16, 12, 12])\n\n\n\n행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n\n# flatten(이미지를 한줄로 펼치는 것)\nf1 = torch.nn.Flatten()\nprint(f1(a1(m1(c1(x_tr)))).shape) #16 * 12 * 12 = 2304\n\ntorch.Size([12665, 2304])\n\n\n\n# sigmoid에 올리기 위해서는 2304 디멘젼을 1로 만들어야함\nl1=torch.nn.Linear(in_features=2304,out_features=1) \n\n\n# sigmoid (값이 0에서 1사이의 값, 즉 확률로 출력되도록)\na2 = torch.nn.Sigmoid()\nprint(a2(f1(a1(m1(c1(x_tr))))).shape)\n\ntorch.Size([12665, 2304])\n\n\n\nprint('이미지 사이즈:                     ', x_tr.shape)\nprint('conv:                              ',c1(x_tr).shape)\nprint('(ReLU) -&gt; maxpooling:              ',m1(a1(c1(x_tr))).shape)\nprint('이미지 펼치기:                     ',f1(a1(m1(c1(x_tr)))).shape)\nprint('이미지를 하나의 스칼라로 선형변환: ',l1(f1(a1(m1(c1(x_tr))))).shape)\nprint('시그모이드:                        ',a2(l1(f1(a1(m1(c1(x_tr)))))).shape)\n\n이미지 사이즈:                      torch.Size([12665, 1, 28, 28])\nconv:                               torch.Size([12665, 16, 24, 24])\n(ReLU) -&gt; maxpooling:               torch.Size([12665, 16, 12, 12])\n이미지 펼치기:                      torch.Size([12665, 2304])\n이미지를 하나의 스칼라로 선형변환:  torch.Size([12665, 1])\n시그모이드:                         torch.Size([12665, 1])\n\n\n\n\n\n\n원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\\(\\to\\) overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n# 데이터 로더에 들어갈 데이터세트 준비\nds_tr = torch.utils.data.TensorDataset(x_tr, y_tr)\nds_test = torch.utils.data.TensorDataset(x_test, y_test)\n\n\n데이터 로더는 배치크기를 지정해줘야 함\n\n\nlen(x_tr), len(x_test)\n\n(12665, 2115)\n\n\n\n데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n\nstochastic gradient descent(구용어: mini-batch gradient descent)\n\n\n\n# 데이터 로더\ndl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=1266) \ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=2115) \n\n\ndls = DataLoaders(dl_tr,dl_test)\n\n\nnet = torch.nn.Sequential(\n    # conv layer \n    c1, \n    # ReLU\n    a1, \n    # maxpool\n    m1, \n    # flatten\n    f1, \n    # linear transform (n -&gt; 1)\n    l1,\n    # Sigmoid\n    a2\n)\n\nloss_fn=torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn) # fastai의 Learner는 오미타이저의 기본값이 adam이므로 따로 지정해주지 않아도 됨\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.979273\n0.639250\n00:05\n\n\n1\n0.703008\n0.402466\n00:00\n\n\n2\n0.547401\n0.256881\n00:00\n\n\n3\n0.434025\n0.142217\n00:00\n\n\n4\n0.340996\n0.079636\n00:00\n\n\n5\n0.267902\n0.048050\n00:00\n\n\n6\n0.211895\n0.031742\n00:00\n\n\n7\n0.169176\n0.022921\n00:00\n\n\n8\n0.136331\n0.017658\n00:00\n\n\n9\n0.110770\n0.014233\n00:00\n\n\n\n\n\n\nLearner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음 &gt; python    net[0].weight 위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = ’cuda:0’라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다.\n+ tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n\nnet = net.to('cpu')\nplt.plot(net(x_tr).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(x_test).data,'.')\nplt.title(\"Testing Set\",size=15)\n\nText(0.5, 1.0, 'Testing Set')\n\n\n\n\n\n\n\n\n\n\n\n\nBCEWithLogitsLoss = Sigmoid + BCELoss\n손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음\n\n\n\n\n\n\n다중(k개) 클래스를 분류 - LossFunction: CrossEntrophyLoss\n- ActivationFunction: SoftmaxFunction - 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n\n소프트맥스 함수가 계산하는 과정은 아래와 같음 (3개를 분류할 경우) \\(softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}\\)\n\n\n\n\n\nk개의 클래스를 분류하는 모델의 Loss 계산 방법\n\nsftmax(_netout) # -&gt; 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -&gt; 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -&gt; 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n+ 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임\n+ 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n\n\n\ntype 1) int형을 사용하는 방법 (vector)\ntype 2) float형을 사용하는 방법 (one-hot encoded vector)\n만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n\ntype 1)의 경우 &lt;사슴: 0, 강아지: 1, 고양이: 2&gt; (단, 데이터 형태는는 int(정수))\ntype 2)의 경우 &lt;사슴: [1, 0, 0], 강아지: [0, 1, 0], 고양이: [0, 0, 1] &gt; (단, 데이터의 형태는 float(실수))\n\n\n\n\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n\n\ntorch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n\n\n\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\ntorch.nn.functional.one_hot(yy).float()\n\ntensor([[1., 0.],\n        [1., 0.],\n        [1., 0.],\n        ...,\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])\n\n\n\n\n\n\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\nlen(X) # 18623\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번= 1862 꽉 채워서 10번하고 3개정도 남은 걸로 한 번\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # test는 전부다 넣어서\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\n\nloss_fn = torch.nn.CrossEntropyLoss() # 여기에는 softmax함수가 내장되어 있음 \n#-&gt; net마지막에 softmax를 넣으면 안됨\n# BCEWithLogitsLoss 느낌(시그모이드 + BCE)\n\n\nlrnr = Learner(dls,net,loss_fn) # 옵티마이저는 아답이 디폴트 값이어서 굳이 안넣어도 됨\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.797101\n1.103916\n00:00\n\n\n1\n1.267011\n0.823797\n00:00\n\n\n2\n1.055513\n0.667210\n00:00\n\n\n3\n0.910746\n0.435414\n00:00\n\n\n4\n0.762887\n0.284001\n00:00\n\n\n5\n0.625515\n0.199502\n00:00\n\n\n6\n0.515352\n0.152906\n00:00\n\n\n7\n0.429145\n0.123678\n00:00\n\n\n8\n0.359694\n0.105466\n00:00\n\n\n9\n0.303888\n0.092883\n00:00\n\n\n\n\n\n\nlrnr.model.to(\"cpu\")\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n3142\n-1.138366\n-5.435792\n0.370670\n2\n\n\n3143\n-4.458741\n-4.281343\n2.052410\n2\n\n\n3144\n-2.836508\n-3.204013\n0.012610\n2\n\n\n3145\n-1.704158\n-10.621873\n2.024313\n2\n\n\n3146\n-2.467648\n-4.612999\n0.656284\n2\n\n\n\n\n\n3147 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n975\n0.432969\n-5.653580\n-1.944451\n0\n\n\n976\n2.685695\n-10.254354\n-2.466680\n0\n\n\n977\n2.474842\n-9.650204\n-2.452746\n0\n\n\n978\n1.268743\n-6.928779\n-1.419695\n0\n\n\n979\n4.371464\n-8.959077\n-4.056257\n0\n\n\n\n\n\n980 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼\n\n\n\n\n\n이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n\n\n\n\nfastai에서 지원\nfastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\ny의 형태를 주의해서 사용해야 함\n\n앞서 말한 것처럼 두 가지 타입이 있음\n\nvector + int\none-hot encoded vector + float\n\n\n\ntype 1) y의 형태가 vector + int일 때 - metrics = accuracy를 사용해야 함\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\ny.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\n\n\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nyy.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n1.128365\n0.601474\n0.463357\n0.536643\n00:00\n\n\n1\n0.684630\n0.304262\n0.975414\n0.024586\n00:00\n\n\n2\n0.503124\n0.144147\n0.989598\n0.010402\n00:00\n\n\n3\n0.373899\n0.068306\n0.996217\n0.003783\n00:00\n\n\n4\n0.281332\n0.040790\n0.996217\n0.003783\n00:00\n\n\n5\n0.215743\n0.026980\n0.996690\n0.003310\n00:00\n\n\n6\n0.168349\n0.019467\n0.996690\n0.003310\n00:00\n\n\n7\n0.133313\n0.014856\n0.998109\n0.001891\n00:00\n\n\n8\n0.106776\n0.011745\n0.998109\n0.001891\n00:00\n\n\n9\n0.086275\n0.009553\n0.999054\n0.000946\n00:00\n\n\n\n\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때 - metrics = accuracy_multi를 사용해야 함 - error_rate는 사용못함\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n\n\ny_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\nyy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\ntorch.nn.functional.one_hot() 함수 조건\n\n기본적으로 크기가 n인 벡터가 들어오길 기대\n정수가 들어오는 것을 기대\n\n하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n1.213650\n0.647844\n0.463357\n00:00\n\n\n1\n0.744869\n0.384000\n0.933570\n00:00\n\n\n2\n0.571879\n0.187119\n0.986525\n00:00\n\n\n3\n0.432096\n0.090841\n0.994563\n00:00\n\n\n4\n0.326718\n0.048868\n0.995508\n00:00\n\n\n5\n0.250235\n0.028477\n0.996217\n00:00\n\n\n6\n0.194605\n0.018616\n0.996454\n00:00\n\n\n7\n0.153338\n0.013278\n0.996927\n00:00\n\n\n8\n0.122056\n0.010130\n0.997872\n00:00\n\n\n9\n0.097957\n0.008112\n0.998109\n00:00"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#cnn-구조",
    "href": "docs/DL/posts/DL_CNN.html#cnn-구조",
    "title": "CNN",
    "section": "",
    "text": "간략하게 CNN의 구조를 설명하자면 - conv layer: 커널층(선형변환처럼 feature를 늘려주는 역할) - ReLU layer: 비선형을 추가해 표현력을 늘려주는 역할 - pooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n# (굳이 (2,2) 이런식으로 안해도 됨 어차피 윈도우 사이즈는 정사각행렬이므로 상수하나만 입력해도 됨됨)\n\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv.weight.data = torch.tensor([[[[0.25, 0.25],[0.25,0.25]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]), tensor([0.]))\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임) - 해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음 - 윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함 - 첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용 - 두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n\n\n\n\n사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.\n\\(ReLU(x) = \\max(0,x)\\)\n\n\n\n\n\npooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\nmaxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_maxpooling(_X) \n\ntensor([[[ 6.,  8.],\n         [16., 18.]]])\n\n\n\nmaxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\npooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n\n이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#cnn-구현",
    "href": "docs/DL/posts/DL_CNN.html#cnn-구현",
    "title": "CNN",
    "section": "",
    "text": "path = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02&lt;00:00]"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#path-사용법-데이터-준비",
    "href": "docs/DL/posts/DL_CNN.html#path-사용법-데이터-준비",
    "title": "CNN",
    "section": "",
    "text": "(path/'원하는 경로').ls()"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#위-코드를-사용하면-폴더-안에-있는-데이터들의-이름을-출력",
    "href": "docs/DL/posts/DL_CNN.html#위-코드를-사용하면-폴더-안에-있는-데이터들의-이름을-출력",
    "title": "CNN",
    "section": "",
    "text": "torchvision.io.read_image()\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/1').ls()])\nx_tr = torch.concat([x0, x1])/255\n\ny_tr = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/1').ls()])\nx_test = torch.concat([x0, x1])/255\n\ny_test = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\nx_tr.shape, y_tr.shape, x_test.shape, y_test.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([2115, 1]))\n\n\n\n\n\n# conv\nc1 = torch.nn.Conv2d(1, 16 , 5) # 만약 color image였다면 입력 채널의 수를 3으로 지정해야 함\nx_tr.shape ,c1(x_tr).shape\n\n(torch.Size([12665, 1, 28, 28]), torch.Size([12665, 16, 24, 24]))\n\n\n\nsize계산 공식: 윈도우(커널)사이즈가 n이면 \\(size = height(width) - ( n - 1)\\)\n\n\n# ReLU\na1 = torch.nn.ReLU()\n\n\n# maxpooling\nm1 = torch.nn.MaxPool2d(2)\nprint(m1(a1(c1(x_tr))).shape)\n\ntorch.Size([12665, 16, 12, 12])\n\n\n\n행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n\n# flatten(이미지를 한줄로 펼치는 것)\nf1 = torch.nn.Flatten()\nprint(f1(a1(m1(c1(x_tr)))).shape) #16 * 12 * 12 = 2304\n\ntorch.Size([12665, 2304])\n\n\n\n# sigmoid에 올리기 위해서는 2304 디멘젼을 1로 만들어야함\nl1=torch.nn.Linear(in_features=2304,out_features=1) \n\n\n# sigmoid (값이 0에서 1사이의 값, 즉 확률로 출력되도록)\na2 = torch.nn.Sigmoid()\nprint(a2(f1(a1(m1(c1(x_tr))))).shape)\n\ntorch.Size([12665, 2304])\n\n\n\nprint('이미지 사이즈:                     ', x_tr.shape)\nprint('conv:                              ',c1(x_tr).shape)\nprint('(ReLU) -&gt; maxpooling:              ',m1(a1(c1(x_tr))).shape)\nprint('이미지 펼치기:                     ',f1(a1(m1(c1(x_tr)))).shape)\nprint('이미지를 하나의 스칼라로 선형변환: ',l1(f1(a1(m1(c1(x_tr))))).shape)\nprint('시그모이드:                        ',a2(l1(f1(a1(m1(c1(x_tr)))))).shape)\n\n이미지 사이즈:                      torch.Size([12665, 1, 28, 28])\nconv:                               torch.Size([12665, 16, 24, 24])\n(ReLU) -&gt; maxpooling:               torch.Size([12665, 16, 12, 12])\n이미지 펼치기:                      torch.Size([12665, 2304])\n이미지를 하나의 스칼라로 선형변환:  torch.Size([12665, 1])\n시그모이드:                         torch.Size([12665, 1])\n\n\n\n\n\n\n원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\\(\\to\\) overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n# 데이터 로더에 들어갈 데이터세트 준비\nds_tr = torch.utils.data.TensorDataset(x_tr, y_tr)\nds_test = torch.utils.data.TensorDataset(x_test, y_test)\n\n\n데이터 로더는 배치크기를 지정해줘야 함\n\n\nlen(x_tr), len(x_test)\n\n(12665, 2115)\n\n\n\n데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n\nstochastic gradient descent(구용어: mini-batch gradient descent)\n\n\n\n# 데이터 로더\ndl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=1266) \ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=2115) \n\n\ndls = DataLoaders(dl_tr,dl_test)\n\n\nnet = torch.nn.Sequential(\n    # conv layer \n    c1, \n    # ReLU\n    a1, \n    # maxpool\n    m1, \n    # flatten\n    f1, \n    # linear transform (n -&gt; 1)\n    l1,\n    # Sigmoid\n    a2\n)\n\nloss_fn=torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn) # fastai의 Learner는 오미타이저의 기본값이 adam이므로 따로 지정해주지 않아도 됨\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.979273\n0.639250\n00:05\n\n\n1\n0.703008\n0.402466\n00:00\n\n\n2\n0.547401\n0.256881\n00:00\n\n\n3\n0.434025\n0.142217\n00:00\n\n\n4\n0.340996\n0.079636\n00:00\n\n\n5\n0.267902\n0.048050\n00:00\n\n\n6\n0.211895\n0.031742\n00:00\n\n\n7\n0.169176\n0.022921\n00:00\n\n\n8\n0.136331\n0.017658\n00:00\n\n\n9\n0.110770\n0.014233\n00:00\n\n\n\n\n\n\nLearner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음 &gt; python    net[0].weight 위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = ’cuda:0’라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다.\n+ tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n\nnet = net.to('cpu')\nplt.plot(net(x_tr).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(x_test).data,'.')\nplt.title(\"Testing Set\",size=15)\n\nText(0.5, 1.0, 'Testing Set')"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#loss-function",
    "href": "docs/DL/posts/DL_CNN.html#loss-function",
    "title": "CNN",
    "section": "",
    "text": "BCEWithLogitsLoss = Sigmoid + BCELoss\n손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#k개의-클래스를-분류하는-모델",
    "href": "docs/DL/posts/DL_CNN.html#k개의-클래스를-분류하는-모델",
    "title": "CNN",
    "section": "",
    "text": "다중(k개) 클래스를 분류 - LossFunction: CrossEntrophyLoss\n- ActivationFunction: SoftmaxFunction - 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n\n소프트맥스 함수가 계산하는 과정은 아래와 같음 (3개를 분류할 경우) \\(softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}\\)\n\n\n\n\n\nk개의 클래스를 분류하는 모델의 Loss 계산 방법\n\nsftmax(_netout) # -&gt; 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -&gt; 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -&gt; 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n+ 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임\n+ 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n\n\n\ntype 1) int형을 사용하는 방법 (vector)\ntype 2) float형을 사용하는 방법 (one-hot encoded vector)\n만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n\ntype 1)의 경우 &lt;사슴: 0, 강아지: 1, 고양이: 2&gt; (단, 데이터 형태는는 int(정수))\ntype 2)의 경우 &lt;사슴: [1, 0, 0], 강아지: [0, 1, 0], 고양이: [0, 0, 1] &gt; (단, 데이터의 형태는 float(실수))\n\n\n\n\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n\n\ntorch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n\n\n\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\ntorch.nn.functional.one_hot(yy).float()\n\ntensor([[1., 0.],\n        [1., 0.],\n        [1., 0.],\n        ...,\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])\n\n\n\n\n\n\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\nlen(X) # 18623\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번= 1862 꽉 채워서 10번하고 3개정도 남은 걸로 한 번\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # test는 전부다 넣어서\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\n\nloss_fn = torch.nn.CrossEntropyLoss() # 여기에는 softmax함수가 내장되어 있음 \n#-&gt; net마지막에 softmax를 넣으면 안됨\n# BCEWithLogitsLoss 느낌(시그모이드 + BCE)\n\n\nlrnr = Learner(dls,net,loss_fn) # 옵티마이저는 아답이 디폴트 값이어서 굳이 안넣어도 됨\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.797101\n1.103916\n00:00\n\n\n1\n1.267011\n0.823797\n00:00\n\n\n2\n1.055513\n0.667210\n00:00\n\n\n3\n0.910746\n0.435414\n00:00\n\n\n4\n0.762887\n0.284001\n00:00\n\n\n5\n0.625515\n0.199502\n00:00\n\n\n6\n0.515352\n0.152906\n00:00\n\n\n7\n0.429145\n0.123678\n00:00\n\n\n8\n0.359694\n0.105466\n00:00\n\n\n9\n0.303888\n0.092883\n00:00\n\n\n\n\n\n\nlrnr.model.to(\"cpu\")\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n3142\n-1.138366\n-5.435792\n0.370670\n2\n\n\n3143\n-4.458741\n-4.281343\n2.052410\n2\n\n\n3144\n-2.836508\n-3.204013\n0.012610\n2\n\n\n3145\n-1.704158\n-10.621873\n2.024313\n2\n\n\n3146\n-2.467648\n-4.612999\n0.656284\n2\n\n\n\n\n\n3147 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n975\n0.432969\n-5.653580\n-1.944451\n0\n\n\n976\n2.685695\n-10.254354\n-2.466680\n0\n\n\n977\n2.474842\n-9.650204\n-2.452746\n0\n\n\n978\n1.268743\n-6.928779\n-1.419695\n0\n\n\n979\n4.371464\n-8.959077\n-4.056257\n0\n\n\n\n\n\n980 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼\n\n\n\n\n\n이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n\n\n\n\nfastai에서 지원\nfastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\ny의 형태를 주의해서 사용해야 함\n\n앞서 말한 것처럼 두 가지 타입이 있음\n\nvector + int\none-hot encoded vector + float\n\n\n\ntype 1) y의 형태가 vector + int일 때 - metrics = accuracy를 사용해야 함\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\ny.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\n\n\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nyy.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n1.128365\n0.601474\n0.463357\n0.536643\n00:00\n\n\n1\n0.684630\n0.304262\n0.975414\n0.024586\n00:00\n\n\n2\n0.503124\n0.144147\n0.989598\n0.010402\n00:00\n\n\n3\n0.373899\n0.068306\n0.996217\n0.003783\n00:00\n\n\n4\n0.281332\n0.040790\n0.996217\n0.003783\n00:00\n\n\n5\n0.215743\n0.026980\n0.996690\n0.003310\n00:00\n\n\n6\n0.168349\n0.019467\n0.996690\n0.003310\n00:00\n\n\n7\n0.133313\n0.014856\n0.998109\n0.001891\n00:00\n\n\n8\n0.106776\n0.011745\n0.998109\n0.001891\n00:00\n\n\n9\n0.086275\n0.009553\n0.999054\n0.000946\n00:00\n\n\n\n\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때 - metrics = accuracy_multi를 사용해야 함 - error_rate는 사용못함\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n\n\ny_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\nyy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\ntorch.nn.functional.one_hot() 함수 조건\n\n기본적으로 크기가 n인 벡터가 들어오길 기대\n정수가 들어오는 것을 기대\n\n하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n1.213650\n0.647844\n0.463357\n00:00\n\n\n1\n0.744869\n0.384000\n0.933570\n00:00\n\n\n2\n0.571879\n0.187119\n0.986525\n00:00\n\n\n3\n0.432096\n0.090841\n0.994563\n00:00\n\n\n4\n0.326718\n0.048868\n0.995508\n00:00\n\n\n5\n0.250235\n0.028477\n0.996217\n00:00\n\n\n6\n0.194605\n0.018616\n0.996454\n00:00\n\n\n7\n0.153338\n0.013278\n0.996927\n00:00\n\n\n8\n0.122056\n0.010130\n0.997872\n00:00\n\n\n9\n0.097957\n0.008112\n0.998109\n00:00"
  },
  {
    "objectID": "docs/DL/posts/rstat101.html",
    "href": "docs/DL/posts/rstat101.html",
    "title": "Tutorial",
    "section": "",
    "text": "test12414입니\n123123다"
  },
  {
    "objectID": "docs/DL/rstat_lecture.html",
    "href": "docs/DL/rstat_lecture.html",
    "title": "rstat lecture",
    "section": "",
    "text": "1강\n2강\n3강"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "JuWon Kwon",
    "section": "",
    "text": "바이오메디컬공학을 전공하고 있는 학부생 입니다.\n초음파와 딥러닝을 공부하고 있습니다."
  },
  {
    "objectID": "docs/DL/posts/DL_CNN_2(XAI_정리).html",
    "href": "docs/DL/posts/DL_CNN_2(XAI_정리).html",
    "title": "CNN",
    "section": "",
    "text": "toc:true\n\n\nimport torch \nimport torchvision\nfrom fastai.vision.all import * \n\n\n\n\n누군가가 만들어 놓은 모델의 구조를 사용하는 것\nfine_tune을 사용하면 전이학습이 되어 학습시간이 줄어들음\nlrnr.fit을 사용하면 그냥 학습을 시키는 것\n\n\n\n\npath = untar_data(URLs.CIFAR)\n\n\npath.ls() # path에는 labels라는 텍스트 파일과 test,train이라는 폴더가 있음\n\n(#3) [Path('/root/.fastai/data/cifar10/labels.txt'),Path('/root/.fastai/data/cifar10/test'),Path('/root/.fastai/data/cifar10/train')]\n\n\n\n!ls /root/.fastai/data/cifar10/train\n\nairplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n\n\n\ntrain 폴더 안에는 위위와 같은 폴더들이 있음\n각 폴더에는 이름과 맞는 이미지 파일이 있음\n10개의 클래스\n앞 장에서 만든 CNN Architecture를 사용하면 정확도가 매우 떨어짐\n\n학습과정은 패스했음\n\n더 복잡하고 정교한 모델을 만들어야 함 \\(\\to\\) transfer learning 사용\n\n\n\n\n\nResNet의 weights를 그대로 가져와 학습에 사용\n\n\n\n\ndls = ImageDataLoaders.from_folder(path,train='train',valid='test')\n\n\n_x, _y = dls.one_batch()\n_x.shape, _y.shape\n\n(torch.Size([64, 3, 32, 32]), torch.Size([64]))\n\n\n\nx의 배치크기는 64(= 하나의 배치에 64개의 데이터(이미지)가 있음을 의미), 채널은 3(컬러 이미지), size는 32 x 32이다.\ny는 x의 레이블로 64개의 이미지가 있으므로 64개의 값이 y에 저장됨\n\n\n_y\n\nTensorCategory([5, 2, 8, 6, 8, 5, 1, 2, 1, 0, 4, 8, 7, 7, 0, 4, 2, 6, 8, 6, 6,\n                2, 0, 7, 1, 9, 5, 4, 3, 6, 4, 9, 6, 4, 3, 7, 3, 9, 3, 6, 5, 3,\n                6, 6, 6, 8, 6, 3, 5, 3, 5, 9, 5, 3, 9, 0, 7, 3, 6, 6, 7, 7, 8,\n                9], device='cuda:0')\n\n\n\n#collapse_output\nnet = torchvision.models.resnet18(weights=torchvision.models.resnet.ResNet18_Weights.IMAGENET1K_V1)\nnet\n\n\n마지막 출력층을 확인해보면 1000개를 출력함\n하지만 우리는 10개의 클래스를 구분하는 모델을 만들 것이므로 1000을 10으로 바꿔야 함\n\n\nnet.fc = torch.nn.Linear(in_features=512, out_features=10) \n\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\nlrnr = Learner(dls, net, loss_fn, metrics = accuracy)\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.804860\n0.821773\n0.724900\n01:15\n\n\n1\n0.640089\n0.679617\n0.773700\n01:09\n\n\n2\n0.519974\n0.647182\n0.783100\n01:05\n\n\n3\n0.405207\n0.564490\n0.811400\n01:14\n\n\n4\n0.344672\n0.683868\n0.783300\n01:05\n\n\n5\n0.269288\n0.737170\n0.785200\n01:12\n\n\n6\n0.272949\n0.788109\n0.769800\n01:12\n\n\n7\n0.188042\n0.690548\n0.808600\n01:10\n\n\n8\n0.175680\n0.736700\n0.800500\n01:09\n\n\n9\n0.151409\n0.821169\n0.795500\n01:06\n\n\n\n\n\n\n\n\n\noverview에서 fastai를 사용한 방법 (fastai로만 모델을 구현)\n\n\npath = untar_data(URLs.PETS)/'images'\n\nfiles= get_image_files(path)\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:08&lt;00:00]\n    \n    \n\n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.175012\n0.026964\n0.989851\n01:54\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.047284\n0.022501\n0.994587\n02:29\n\n\n\n\n\n\n아래의 코드를 사용하면 network의 구조를 볼 수 있음\n\nlrnr.model()\n\n\n\n\n\n\n# 위의 코드(fastai로 transfer learning) 똑같음\npath = untar_data(URLs.PETS)/'images'\n\nfiles= get_image_files(path)\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:04&lt;00:00]\n    \n    \n\n\n\npath.ls()\n\n(#7393) [Path('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_7.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_97.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/chihuahua_135.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/keeshond_110.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_54.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/wheaten_terrier_11.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/British_Shorthair_62.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/german_shorthaired_171.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/havanese_84.jpg')...]\n\n\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg')\nximg\n\n\n\n\n\nx= first(dls.test_dl([ximg]))[0]\nx.shape # 위 이미지의 shape\n\ntorch.Size([1, 3, 512, 512])\n\n\n\ntorch.nn.AdaptiveAvgPool2d()\n\npooling을 평균으로 하는 것으로 output_size = 1이라면 채널당 하나의 값을 출력해줌\n위의 이미지는 채널이 3개이므로 이미지 하나당 3개의 값을 출력\n\n\n\nap = torch.nn.AdaptiveAvgPool2d(output_size=1) # output size가 1이라는 것은 (1x1)로 출력하겠다는 뜻\n# 만약 2이면 출력이 (2 x 2) 즉 채널당 4개의 값이 나오는 것임\nap(x)\n\nTensorImage([[[[0.6086]],\n\n              [[0.5648]],\n\n              [[0.5058]]]], device='cuda:0')\n\n\n\n\n\n데이터 로더를 사용해 불러온 이미지 데이트의 차원을 확인해보면 [1, 3, 512, 512]이다. 이는 배치사이즈, 채널, 높이, 너비 순으로 되어있는 것임\n이러한 데이터를 높이, 너비, 채널 순으로 바꾸고 싶음 \\(\\to\\) torch.einsum()사용\n\ntorch.einsum('ij,jk-&gt;ik',tensor1, tensor2) # 크기가 i x j인 tensor1과 j x k인 tensor2의 행렬 곱을 해줌\n\ntorch.Size([1, 3, 512, 512]) \\(\\to\\) torch.Size([512, 512, 3]) 형태로 만들고 싶음\n\n→ tensor의 차원을 변경하거나 계산을 할 때에는 torch.einsum()함수를 사용하면 좋음\n\nx_new = torch.einsum('ocij -&gt; ijc',x.to('cpu'))\n\n\nplt는 높이 x 너비 x 채널 순으로 데이터가 입력되길 기대\n만약 x 즉, 채널 x 높이 x 너비 순으로 되어있는 데이터를 입력하면 에러가 발생함\n\n\nplt.imshow(x_new)\n\n&lt;matplotlib.image.AxesImage at 0x7fb7e874cf10&gt;\n\n\n\n\n\n\n\n\n\n\nCAM(Class Activation Mapping): CNN의 판단근거를 시각화하는 기술\n\n\n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.208073\n0.010794\n0.995940\n01:55\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.047890\n0.002702\n0.998647\n02:25\n\n\n\n\n\n\nlrnr.model 함수를 사용하면 resnet34의 구조를 볼 수 있음\n구조를 확인해보면 resnet34는 두 개의 모델이 합쳐져 있음\n첫 번째 모델은 그대로 사용하고 두 번째 모델만 약간 수정하여 사용해보자\n\n\n\n\n\nnet1 = lrnr.model[0]\nnet2 = lrnr.model[1]\n\n\nnet1.to('cpu')\nnet2.to('cpu')\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): fastai.layers.Flatten(full=False)\n  (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=1024, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\nBatchNorm1d과 Dropout, ReLU 층은 파라미터 개수에 변화를 주지 않음\npooling층과 Linear층을 주목해야 함\n\n\n_X, _y = dls.one_batch() \n\n\n_X = _X.to('cpu')\n# net1(_X).shape #torch.Size([64, 512, 16, 16]) 이러한 형태로 출력됨\n\n\nnet2= torch.nn.Sequential(\n    torch.nn.AdaptiveAvgPool2d(output_size=1), # (64,512,16,16) -&gt; (64,512,1,1) \n    torch.nn.Flatten(), # (64,512,1,1) -&gt; (64,512) \n    torch.nn.Linear(512,2,bias=False) # (64,512) -&gt; (64,2) \n)\n\n\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\n\nlrnr2= Learner(dls,net,metrics=accuracy)\n\nlrnr2.fine_tune(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.232032\n1.522452\n0.836265\n02:25\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.134071\n0.816453\n0.830176\n02:24\n\n\n1\n0.138622\n0.150047\n0.936401\n02:25\n\n\n2\n0.095163\n0.100338\n0.960081\n02:24\n\n\n3\n0.051146\n0.039767\n0.984438\n02:28\n\n\n4\n0.024118\n0.044908\n0.982409\n02:30\n\n\n\n\n\n\n\n\n- net2의 순서 바꾸기 전 전체 네트워크:\n\\[\\underset{(1,3,512,512)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{ap}{\\to} \\underset{(1,512,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,512)}{{\\boldsymbol \\sharp}}\\overset{linear}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [-9.0358,  9.0926]\\]\n\n위의 수식을 아래의 수식으로 바꾸고 싶음\n\n순서만 바꾸는 것임 -&gt; 결과는 똑같음\n\n\n\\[\\underset{(1,3,224,224)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{linear}{\\to} \\underset{(1,2,16,16)}{{\\bf why}}\\overset{ap}{\\to} \\underset{(1,2,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [−9.0358,9.0926]\\]\n\n여기서 주목해야 하는 것은 \\(why\\)\n\\(why\\)의 값들을 평균을 내고 이 값을 선형변환하여 하나의 값을 만듦 [−9.0358 or 9.0926]\n\\(why\\)의 tensor를 확인해보자!\n바꾸기 전 net2의 구조 중 일부분 (배치를 넣는 것이 아니라 하나의 이미지를 net에 넣을때임)\n\nX = net1(x)\n\nX.shape = (1, 512, 16, 16)\n\nXX = torch.nn.AdaptiveAvgPool2d(X)\n\nXX.shape =(1, 512, 1, 1)\n\nXXX= torch.nn.Flatten(XX)\n\nXXX.shape -&gt; (1,512)\n\nXXX @ net2[2].weight \\(\\to\\) shape(1, 2)\n\nnet2[2].weight.shape -&gt; (512, 2)\n\n\n\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg')\nx = first(dls.test_dl([ximg]))[0]\n\n\n# 위의 정리된 것들을 보면\n# net2[2].weight의 크기는 (512, 2)\n# net1(x)의 크기는 (1, 512, 16, 16)\n# why의 크기를 (1, 2, 16, 16)으로 만들고 싶음\nwhy = torch.einsum('cb,abij-&gt;acij',\n                   net2[2].weight,#(512, 2)\n                   net1(x) # (1, 512, 16, 16)\n                   )\nwhy.shape\n\ntorch.Size([1, 2, 16, 16])\n\n\n\n# net2[0]은 avg pooling layer -&gt; 높이와 폭을 0으로 만듦\n# (1, 2, 16, 16) -&gt; (1, 2, 1, 1)\nnet2[0](why)\n\nTensorImage([[[[-7.2395]],\n\n              [[ 7.5647]]]], device='cuda:0', grad_fn=&lt;AliasBackward0&gt;)\n\n\n\n\n\n\n\ntensor의 평균 -9.0358\n해당 tensor의 평균이 크다면 고양이라고 판단을 함\n\n하지만 이번 예시에서는 평균이 음수가 나옴 -&gt; 해당 값이 작으면 작을수록 고양이가 아니라고 생각하는 근거가 됨\n\n그러므로 해당 tensor에서 엄청 작은 값들은 고양이가 아니라고 판단하는 근거가 되는 점들임\n\n\nwhy[0,0,:,:].to(torch.int64)\n\nTensorImage([[  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    1,    1,    0,    1,    1,    1,    1,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    1,    1,    0,    0,   -1,   -2,   -2,\n                -1,    0,    2,    2,    1],\n             [   0,    0,    0,    0,    0,    0,   -2,  -10,  -17,  -18,  -17,\n               -14,   -7,    2,    5,    1],\n             [  -1,    0,    0,    0,    0,    0,   -7,  -26,  -45,  -43,  -38,\n               -34,  -16,    0,    3,    0],\n             [  -1,    0,    0,   -1,   -2,   -3,  -10,  -32,  -56,  -58,  -50,\n               -43,  -22,   -2,    1,    1],\n             [   1,    2,    0,   -6,  -14,  -12,  -11,  -25,  -45,  -66,  -71,\n               -49,  -20,    0,    2,    2],\n             [   8,   10,    2,   -9,  -19,  -18,  -10,  -13,  -34,  -74, -100,\n               -65,  -21,    1,    1,    0],\n             [  11,   12,    2,   -6,  -13,  -13,   -7,   -5,  -23,  -60,  -82,\n               -63,  -21,    0,    0,    0],\n             [   7,    6,    1,    0,   -2,   -4,   -3,   -1,  -12,  -32,  -45,\n               -36,  -14,   -1,    0,   -2],\n             [   0,    1,    0,    1,    2,    0,    0,    0,   -1,   -7,  -12,\n               -11,   -5,   -1,   -1,   -2],\n             [  -2,   -1,   -1,   -1,   -1,   -1,    0,    1,    4,    1,   -1,\n                -1,   -2,   -2,   -2,   -3],\n             [  -4,   -4,   -4,   -5,   -4,   -4,   -4,   -2,   -2,   -3,   -4,\n                -5,   -5,   -5,   -5,   -4],\n             [  -5,   -6,   -6,   -6,   -6,   -6,   -5,   -5,   -5,   -5,   -6,\n                -6,   -6,   -6,   -5,   -5]], device='cuda:0')\n\n\n\n\n\n\ntensor의 평균 9.0926\n해당 tensor는 강아지라고 판단하는 tensor라고 생각하면 됨\n그러므로 해당 tensor에서 엄청 작은 값들은 고양이가 아니라고 판단하는 근거가 되는 점들임\n\n\nwhy[0,1,:,:].to(torch.int64)\n\nTensorImage([[ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 2,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  1,  2,  2,  2,  0, -1, -1,  0],\n             [ 0,  0,  0,  0,  0,  0,  2, 10, 16, 18, 17, 14,  7, -1, -3, -1],\n             [ 1,  1,  0,  0,  0,  0,  6, 25, 43, 42, 37, 32, 16,  1, -2,  0],\n             [ 1,  0,  0,  1,  2,  3, 10, 32, 55, 58, 50, 42, 21,  3,  0,  0],\n             [ 0, -1,  0,  6, 13, 12, 11, 24, 44, 65, 69, 49, 20,  1, -1, -1],\n             [-7, -8, -1,  9, 18, 17, 10, 13, 33, 72, 97, 65, 22,  0,  0,  0],\n             [-9, -9, -2,  5, 12, 13,  7,  5, 22, 58, 81, 62, 21,  1,  0,  1],\n             [-6, -6, -1,  1,  2,  4,  3,  1, 11, 31, 43, 34, 14,  1,  1,  2],\n             [ 0,  0,  0,  0,  0,  0,  1,  0,  1,  7, 12, 10,  5,  1,  1,  3],\n             [ 3,  2,  2,  2,  2,  1,  1,  0, -2,  0,  1,  2,  2,  3,  3,  4],\n             [ 5,  6,  6,  6,  6,  6,  5,  3,  4,  5,  5,  6,  6,  6,  6,  6],\n             [ 5,  6,  7,  7,  7,  7,  6,  6,  6,  6,  6,  7,  7,  7,  6,  6]],\n            device='cuda:0')\n\n\n\n\n\n\n\n\nwhy의 값과 이미지를 동시에 출력한다면 그림의 어떠한 부분을 보고 강아지라고 판단을 한 것인지 알 수 있음\n\n\nwhy_cat = why[0,0,:,:] # 해당 tensor의 값이 작으면 작을 수록 고양이가 아니라고 판단하는 근거가 됨\nwhy_dog = why[0,1,:,:] # 해당 tensor의 값이 크면 클수록 강아지라고 판단하는 근거가 됨\n\n\n밑에서 cmap을 magma로 지정하였는 이는 값이 작은 부분은 검정색 큰 부분은 노란색으로 표시함\n\n검정 -&gt; 보라 -&gt; 빨강 -&gt; 노랑\n\n이를 해석하면 why_cat에서 값이 가장 작은 부분, 즉 검정색이 고양이가 아니라고 판단함\nwhy_dog에서는 값이 가장 큰 부분, 즉 노란색이 강아지라고 판단하는 영역을 표시함\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma')\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc99c0810&gt;\n\n\n\n\n\n\nwhy의 크기 조절\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc90e9410&gt;\n\n\n\n\n\n\n크기조절 후 겹쳐그리기\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc8df78d0&gt;\n\n\n\n\n\n\n\n\n# why값을 확률로 출력하기\nsftmax=torch.nn.Softmax(dim=1)\nsftmax(net(x))\ncatprob, dogprob = sftmax(net(x))[0,0].item(), sftmax(net(x))[0,1].item()\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[0].set_title('catprob= %f' % catprob) \nax[1].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].set_title('dogprob=%f' % dogprob)\n\nText(0.5, 1.0, 'dogprob=1.000000')\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=0 \nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -&gt; acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob&gt;dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN_2(XAI_정리).html#transfer-learning",
    "href": "docs/DL/posts/DL_CNN_2(XAI_정리).html#transfer-learning",
    "title": "CNN",
    "section": "",
    "text": "누군가가 만들어 놓은 모델의 구조를 사용하는 것\nfine_tune을 사용하면 전이학습이 되어 학습시간이 줄어들음\nlrnr.fit을 사용하면 그냥 학습을 시키는 것\n\n\n\n\npath = untar_data(URLs.CIFAR)\n\n\npath.ls() # path에는 labels라는 텍스트 파일과 test,train이라는 폴더가 있음\n\n(#3) [Path('/root/.fastai/data/cifar10/labels.txt'),Path('/root/.fastai/data/cifar10/test'),Path('/root/.fastai/data/cifar10/train')]\n\n\n\n!ls /root/.fastai/data/cifar10/train\n\nairplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n\n\n\ntrain 폴더 안에는 위위와 같은 폴더들이 있음\n각 폴더에는 이름과 맞는 이미지 파일이 있음\n10개의 클래스\n앞 장에서 만든 CNN Architecture를 사용하면 정확도가 매우 떨어짐\n\n학습과정은 패스했음\n\n더 복잡하고 정교한 모델을 만들어야 함 \\(\\to\\) transfer learning 사용\n\n\n\n\n\nResNet의 weights를 그대로 가져와 학습에 사용\n\n\n\n\ndls = ImageDataLoaders.from_folder(path,train='train',valid='test')\n\n\n_x, _y = dls.one_batch()\n_x.shape, _y.shape\n\n(torch.Size([64, 3, 32, 32]), torch.Size([64]))\n\n\n\nx의 배치크기는 64(= 하나의 배치에 64개의 데이터(이미지)가 있음을 의미), 채널은 3(컬러 이미지), size는 32 x 32이다.\ny는 x의 레이블로 64개의 이미지가 있으므로 64개의 값이 y에 저장됨\n\n\n_y\n\nTensorCategory([5, 2, 8, 6, 8, 5, 1, 2, 1, 0, 4, 8, 7, 7, 0, 4, 2, 6, 8, 6, 6,\n                2, 0, 7, 1, 9, 5, 4, 3, 6, 4, 9, 6, 4, 3, 7, 3, 9, 3, 6, 5, 3,\n                6, 6, 6, 8, 6, 3, 5, 3, 5, 9, 5, 3, 9, 0, 7, 3, 6, 6, 7, 7, 8,\n                9], device='cuda:0')\n\n\n\n#collapse_output\nnet = torchvision.models.resnet18(weights=torchvision.models.resnet.ResNet18_Weights.IMAGENET1K_V1)\nnet\n\n\n마지막 출력층을 확인해보면 1000개를 출력함\n하지만 우리는 10개의 클래스를 구분하는 모델을 만들 것이므로 1000을 10으로 바꿔야 함\n\n\nnet.fc = torch.nn.Linear(in_features=512, out_features=10) \n\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\nlrnr = Learner(dls, net, loss_fn, metrics = accuracy)\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.804860\n0.821773\n0.724900\n01:15\n\n\n1\n0.640089\n0.679617\n0.773700\n01:09\n\n\n2\n0.519974\n0.647182\n0.783100\n01:05\n\n\n3\n0.405207\n0.564490\n0.811400\n01:14\n\n\n4\n0.344672\n0.683868\n0.783300\n01:05\n\n\n5\n0.269288\n0.737170\n0.785200\n01:12\n\n\n6\n0.272949\n0.788109\n0.769800\n01:12\n\n\n7\n0.188042\n0.690548\n0.808600\n01:10\n\n\n8\n0.175680\n0.736700\n0.800500\n01:09\n\n\n9\n0.151409\n0.821169\n0.795500\n01:06\n\n\n\n\n\n\n\n\n\noverview에서 fastai를 사용한 방법 (fastai로만 모델을 구현)\n\n\npath = untar_data(URLs.PETS)/'images'\n\nfiles= get_image_files(path)\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:08&lt;00:00]\n    \n    \n\n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.175012\n0.026964\n0.989851\n01:54\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.047284\n0.022501\n0.994587\n02:29\n\n\n\n\n\n\n아래의 코드를 사용하면 network의 구조를 볼 수 있음\n\nlrnr.model()"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN_2(XAI_정리).html#데이터-확인",
    "href": "docs/DL/posts/DL_CNN_2(XAI_정리).html#데이터-확인",
    "title": "CNN",
    "section": "",
    "text": "# 위의 코드(fastai로 transfer learning) 똑같음\npath = untar_data(URLs.PETS)/'images'\n\nfiles= get_image_files(path)\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:04&lt;00:00]\n    \n    \n\n\n\npath.ls()\n\n(#7393) [Path('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_7.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_97.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/chihuahua_135.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/keeshond_110.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_54.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/wheaten_terrier_11.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/British_Shorthair_62.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/german_shorthaired_171.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/havanese_84.jpg')...]\n\n\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg')\nximg\n\n\n\n\n\nx= first(dls.test_dl([ximg]))[0]\nx.shape # 위 이미지의 shape\n\ntorch.Size([1, 3, 512, 512])\n\n\n\ntorch.nn.AdaptiveAvgPool2d()\n\npooling을 평균으로 하는 것으로 output_size = 1이라면 채널당 하나의 값을 출력해줌\n위의 이미지는 채널이 3개이므로 이미지 하나당 3개의 값을 출력\n\n\n\nap = torch.nn.AdaptiveAvgPool2d(output_size=1) # output size가 1이라는 것은 (1x1)로 출력하겠다는 뜻\n# 만약 2이면 출력이 (2 x 2) 즉 채널당 4개의 값이 나오는 것임\nap(x)\n\nTensorImage([[[[0.6086]],\n\n              [[0.5648]],\n\n              [[0.5058]]]], device='cuda:0')\n\n\n\n\n\n데이터 로더를 사용해 불러온 이미지 데이트의 차원을 확인해보면 [1, 3, 512, 512]이다. 이는 배치사이즈, 채널, 높이, 너비 순으로 되어있는 것임\n이러한 데이터를 높이, 너비, 채널 순으로 바꾸고 싶음 \\(\\to\\) torch.einsum()사용\n\ntorch.einsum('ij,jk-&gt;ik',tensor1, tensor2) # 크기가 i x j인 tensor1과 j x k인 tensor2의 행렬 곱을 해줌\n\ntorch.Size([1, 3, 512, 512]) \\(\\to\\) torch.Size([512, 512, 3]) 형태로 만들고 싶음\n\n→ tensor의 차원을 변경하거나 계산을 할 때에는 torch.einsum()함수를 사용하면 좋음\n\nx_new = torch.einsum('ocij -&gt; ijc',x.to('cpu'))\n\n\nplt는 높이 x 너비 x 채널 순으로 데이터가 입력되길 기대\n만약 x 즉, 채널 x 높이 x 너비 순으로 되어있는 데이터를 입력하면 에러가 발생함\n\n\nplt.imshow(x_new)\n\n&lt;matplotlib.image.AxesImage at 0x7fb7e874cf10&gt;"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN_2(XAI_정리).html#cam-구현",
    "href": "docs/DL/posts/DL_CNN_2(XAI_정리).html#cam-구현",
    "title": "CNN",
    "section": "",
    "text": "CAM(Class Activation Mapping): CNN의 판단근거를 시각화하는 기술\n\n\n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.208073\n0.010794\n0.995940\n01:55\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.047890\n0.002702\n0.998647\n02:25\n\n\n\n\n\n\nlrnr.model 함수를 사용하면 resnet34의 구조를 볼 수 있음\n구조를 확인해보면 resnet34는 두 개의 모델이 합쳐져 있음\n첫 번째 모델은 그대로 사용하고 두 번째 모델만 약간 수정하여 사용해보자\n\n\n\n\n\nnet1 = lrnr.model[0]\nnet2 = lrnr.model[1]\n\n\nnet1.to('cpu')\nnet2.to('cpu')\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): fastai.layers.Flatten(full=False)\n  (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=1024, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\nBatchNorm1d과 Dropout, ReLU 층은 파라미터 개수에 변화를 주지 않음\npooling층과 Linear층을 주목해야 함\n\n\n_X, _y = dls.one_batch() \n\n\n_X = _X.to('cpu')\n# net1(_X).shape #torch.Size([64, 512, 16, 16]) 이러한 형태로 출력됨\n\n\nnet2= torch.nn.Sequential(\n    torch.nn.AdaptiveAvgPool2d(output_size=1), # (64,512,16,16) -&gt; (64,512,1,1) \n    torch.nn.Flatten(), # (64,512,1,1) -&gt; (64,512) \n    torch.nn.Linear(512,2,bias=False) # (64,512) -&gt; (64,2) \n)\n\n\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\n\nlrnr2= Learner(dls,net,metrics=accuracy)\n\nlrnr2.fine_tune(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.232032\n1.522452\n0.836265\n02:25\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.134071\n0.816453\n0.830176\n02:24\n\n\n1\n0.138622\n0.150047\n0.936401\n02:25\n\n\n2\n0.095163\n0.100338\n0.960081\n02:24\n\n\n3\n0.051146\n0.039767\n0.984438\n02:28\n\n\n4\n0.024118\n0.044908\n0.982409\n02:30\n\n\n\n\n\n\n\n\n- net2의 순서 바꾸기 전 전체 네트워크:\n\\[\\underset{(1,3,512,512)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{ap}{\\to} \\underset{(1,512,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,512)}{{\\boldsymbol \\sharp}}\\overset{linear}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [-9.0358,  9.0926]\\]\n\n위의 수식을 아래의 수식으로 바꾸고 싶음\n\n순서만 바꾸는 것임 -&gt; 결과는 똑같음\n\n\n\\[\\underset{(1,3,224,224)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{linear}{\\to} \\underset{(1,2,16,16)}{{\\bf why}}\\overset{ap}{\\to} \\underset{(1,2,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [−9.0358,9.0926]\\]\n\n여기서 주목해야 하는 것은 \\(why\\)\n\\(why\\)의 값들을 평균을 내고 이 값을 선형변환하여 하나의 값을 만듦 [−9.0358 or 9.0926]\n\\(why\\)의 tensor를 확인해보자!\n바꾸기 전 net2의 구조 중 일부분 (배치를 넣는 것이 아니라 하나의 이미지를 net에 넣을때임)\n\nX = net1(x)\n\nX.shape = (1, 512, 16, 16)\n\nXX = torch.nn.AdaptiveAvgPool2d(X)\n\nXX.shape =(1, 512, 1, 1)\n\nXXX= torch.nn.Flatten(XX)\n\nXXX.shape -&gt; (1,512)\n\nXXX @ net2[2].weight \\(\\to\\) shape(1, 2)\n\nnet2[2].weight.shape -&gt; (512, 2)\n\n\n\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg')\nx = first(dls.test_dl([ximg]))[0]\n\n\n# 위의 정리된 것들을 보면\n# net2[2].weight의 크기는 (512, 2)\n# net1(x)의 크기는 (1, 512, 16, 16)\n# why의 크기를 (1, 2, 16, 16)으로 만들고 싶음\nwhy = torch.einsum('cb,abij-&gt;acij',\n                   net2[2].weight,#(512, 2)\n                   net1(x) # (1, 512, 16, 16)\n                   )\nwhy.shape\n\ntorch.Size([1, 2, 16, 16])\n\n\n\n# net2[0]은 avg pooling layer -&gt; 높이와 폭을 0으로 만듦\n# (1, 2, 16, 16) -&gt; (1, 2, 1, 1)\nnet2[0](why)\n\nTensorImage([[[[-7.2395]],\n\n              [[ 7.5647]]]], device='cuda:0', grad_fn=&lt;AliasBackward0&gt;)\n\n\n\n\n\n\n\ntensor의 평균 -9.0358\n해당 tensor의 평균이 크다면 고양이라고 판단을 함\n\n하지만 이번 예시에서는 평균이 음수가 나옴 -&gt; 해당 값이 작으면 작을수록 고양이가 아니라고 생각하는 근거가 됨\n\n그러므로 해당 tensor에서 엄청 작은 값들은 고양이가 아니라고 판단하는 근거가 되는 점들임\n\n\nwhy[0,0,:,:].to(torch.int64)\n\nTensorImage([[  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    1,    1,    0,    1,    1,    1,    1,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    1,    1,    0,    0,   -1,   -2,   -2,\n                -1,    0,    2,    2,    1],\n             [   0,    0,    0,    0,    0,    0,   -2,  -10,  -17,  -18,  -17,\n               -14,   -7,    2,    5,    1],\n             [  -1,    0,    0,    0,    0,    0,   -7,  -26,  -45,  -43,  -38,\n               -34,  -16,    0,    3,    0],\n             [  -1,    0,    0,   -1,   -2,   -3,  -10,  -32,  -56,  -58,  -50,\n               -43,  -22,   -2,    1,    1],\n             [   1,    2,    0,   -6,  -14,  -12,  -11,  -25,  -45,  -66,  -71,\n               -49,  -20,    0,    2,    2],\n             [   8,   10,    2,   -9,  -19,  -18,  -10,  -13,  -34,  -74, -100,\n               -65,  -21,    1,    1,    0],\n             [  11,   12,    2,   -6,  -13,  -13,   -7,   -5,  -23,  -60,  -82,\n               -63,  -21,    0,    0,    0],\n             [   7,    6,    1,    0,   -2,   -4,   -3,   -1,  -12,  -32,  -45,\n               -36,  -14,   -1,    0,   -2],\n             [   0,    1,    0,    1,    2,    0,    0,    0,   -1,   -7,  -12,\n               -11,   -5,   -1,   -1,   -2],\n             [  -2,   -1,   -1,   -1,   -1,   -1,    0,    1,    4,    1,   -1,\n                -1,   -2,   -2,   -2,   -3],\n             [  -4,   -4,   -4,   -5,   -4,   -4,   -4,   -2,   -2,   -3,   -4,\n                -5,   -5,   -5,   -5,   -4],\n             [  -5,   -6,   -6,   -6,   -6,   -6,   -5,   -5,   -5,   -5,   -6,\n                -6,   -6,   -6,   -5,   -5]], device='cuda:0')\n\n\n\n\n\n\ntensor의 평균 9.0926\n해당 tensor는 강아지라고 판단하는 tensor라고 생각하면 됨\n그러므로 해당 tensor에서 엄청 작은 값들은 고양이가 아니라고 판단하는 근거가 되는 점들임\n\n\nwhy[0,1,:,:].to(torch.int64)\n\nTensorImage([[ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 2,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  1,  2,  2,  2,  0, -1, -1,  0],\n             [ 0,  0,  0,  0,  0,  0,  2, 10, 16, 18, 17, 14,  7, -1, -3, -1],\n             [ 1,  1,  0,  0,  0,  0,  6, 25, 43, 42, 37, 32, 16,  1, -2,  0],\n             [ 1,  0,  0,  1,  2,  3, 10, 32, 55, 58, 50, 42, 21,  3,  0,  0],\n             [ 0, -1,  0,  6, 13, 12, 11, 24, 44, 65, 69, 49, 20,  1, -1, -1],\n             [-7, -8, -1,  9, 18, 17, 10, 13, 33, 72, 97, 65, 22,  0,  0,  0],\n             [-9, -9, -2,  5, 12, 13,  7,  5, 22, 58, 81, 62, 21,  1,  0,  1],\n             [-6, -6, -1,  1,  2,  4,  3,  1, 11, 31, 43, 34, 14,  1,  1,  2],\n             [ 0,  0,  0,  0,  0,  0,  1,  0,  1,  7, 12, 10,  5,  1,  1,  3],\n             [ 3,  2,  2,  2,  2,  1,  1,  0, -2,  0,  1,  2,  2,  3,  3,  4],\n             [ 5,  6,  6,  6,  6,  6,  5,  3,  4,  5,  5,  6,  6,  6,  6,  6],\n             [ 5,  6,  7,  7,  7,  7,  6,  6,  6,  6,  6,  7,  7,  7,  6,  6]],\n            device='cuda:0')\n\n\n\n\n\n\n\n\nwhy의 값과 이미지를 동시에 출력한다면 그림의 어떠한 부분을 보고 강아지라고 판단을 한 것인지 알 수 있음\n\n\nwhy_cat = why[0,0,:,:] # 해당 tensor의 값이 작으면 작을 수록 고양이가 아니라고 판단하는 근거가 됨\nwhy_dog = why[0,1,:,:] # 해당 tensor의 값이 크면 클수록 강아지라고 판단하는 근거가 됨\n\n\n밑에서 cmap을 magma로 지정하였는 이는 값이 작은 부분은 검정색 큰 부분은 노란색으로 표시함\n\n검정 -&gt; 보라 -&gt; 빨강 -&gt; 노랑\n\n이를 해석하면 why_cat에서 값이 가장 작은 부분, 즉 검정색이 고양이가 아니라고 판단함\nwhy_dog에서는 값이 가장 큰 부분, 즉 노란색이 강아지라고 판단하는 영역을 표시함\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma')\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc99c0810&gt;\n\n\n\n\n\n\nwhy의 크기 조절\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc90e9410&gt;\n\n\n\n\n\n\n크기조절 후 겹쳐그리기\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc8df78d0&gt;\n\n\n\n\n\n\n\n\n# why값을 확률로 출력하기\nsftmax=torch.nn.Softmax(dim=1)\nsftmax(net(x))\ncatprob, dogprob = sftmax(net(x))[0,0].item(), sftmax(net(x))[0,1].item()\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[0].set_title('catprob= %f' % catprob) \nax[1].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].set_title('dogprob=%f' % dogprob)\n\nText(0.5, 1.0, 'dogprob=1.000000')\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=0 \nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -&gt; acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob&gt;dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()"
  },
  {
    "objectID": "docs/DL/posts/FCN_summary.html",
    "href": "docs/DL/posts/FCN_summary.html",
    "title": "FCN paper review",
    "section": "",
    "text": "semantic segmantation의 초석을 닦은 연구\nsegmantation = (1) semantic segmantaion (2) instance segmantation\n\n\n\n\nimage.png\n\n\n\nbiomedical image에서는 semantic segmentation을 사용 ex) 의료 영상\n\n\n\n\n\n\nimage.png\n\n\n\nend-to-end방식\npixel 단위로 클래스 예측 (from supervised pre-trained)\nencoder - decoder 형태\n유명한 CNN 아키텍쳐의 특징을 갖고 있음\n\n\n\n\n\n\nimage.png\n\n\n1 encoder 부분(Downsampling)에서는 3 * 3 Conv-&gt; 3 * 3 Conv -&gt; pooling을 진행하는 구조\n2 image segmantation은 기본적으로 CNN의 구조를 따라감. 단, CNN 구조의 마지막에 있는 FC layer는 갖고 오지 않음\n\n\n\nimage.png\n\n\n\nFC layer를 사용하게 되면 위치정보가 소실됨 -&gt; Conv layer 사용\nConv Layer는 1x1 Conv layer(Google Net)를 사용하여 채널의 수를 조절\n채널의 수는 객체의 수와 동일하게 맞춤\n\n3 [\\(H\\), \\(W\\)] 이미지가 [conv - conv - pooling] (VGG의 구조)를 n번 통과한다면 이미지의 크기는 [\\(H/2^n\\), \\(W/2^n\\)]가 되게 됨\n\n\n\nimage.png\n\n\n\n이를 다시 말하면 픽셀 하나가 \\(2^n\\)개의 픽셀의 정보를 갖고 있다는 뜻 -&gt; 위치정보를 애매하게 알게 됨\nupsampling 과정에서 대략적인(?) segmantation이 진행됨\n이를 보완하기 위해서 skip connection(ResNet)을 이용\n\n\n\n\n\n\n\nimage.png\n\n\n\n1x1 Conv를 사용해 채널수를 맞추어 Upsampling한 Feature map과 합침\n위 과정을 지난 Feature map을 다시 한번 Upsampling한 후 Feature map과 합침\n마지막에 몇 배로 Upsampling을 했는지에 따라서 이름이 바뀜\n\nex) 위의 그림은 마지막에 8배로 Upsampling을 했으므로 FCN-8s"
  },
  {
    "objectID": "docs/DL/posts/FCN_summary.html#fcn-structure",
    "href": "docs/DL/posts/FCN_summary.html#fcn-structure",
    "title": "FCN paper review",
    "section": "",
    "text": "image.png\n\n\n\nend-to-end방식\npixel 단위로 클래스 예측 (from supervised pre-trained)\nencoder - decoder 형태\n유명한 CNN 아키텍쳐의 특징을 갖고 있음\n\n\n\n\n\n\nimage.png\n\n\n1 encoder 부분(Downsampling)에서는 3 * 3 Conv-&gt; 3 * 3 Conv -&gt; pooling을 진행하는 구조\n2 image segmantation은 기본적으로 CNN의 구조를 따라감. 단, CNN 구조의 마지막에 있는 FC layer는 갖고 오지 않음\n\n\n\nimage.png\n\n\n\nFC layer를 사용하게 되면 위치정보가 소실됨 -&gt; Conv layer 사용\nConv Layer는 1x1 Conv layer(Google Net)를 사용하여 채널의 수를 조절\n채널의 수는 객체의 수와 동일하게 맞춤\n\n3 [\\(H\\), \\(W\\)] 이미지가 [conv - conv - pooling] (VGG의 구조)를 n번 통과한다면 이미지의 크기는 [\\(H/2^n\\), \\(W/2^n\\)]가 되게 됨\n\n\n\nimage.png\n\n\n\n이를 다시 말하면 픽셀 하나가 \\(2^n\\)개의 픽셀의 정보를 갖고 있다는 뜻 -&gt; 위치정보를 애매하게 알게 됨\nupsampling 과정에서 대략적인(?) segmantation이 진행됨\n이를 보완하기 위해서 skip connection(ResNet)을 이용\n\n\n\n\n\n\n\nimage.png\n\n\n\n1x1 Conv를 사용해 채널수를 맞추어 Upsampling한 Feature map과 합침\n위 과정을 지난 Feature map을 다시 한번 Upsampling한 후 Feature map과 합침\n마지막에 몇 배로 Upsampling을 했는지에 따라서 이름이 바뀜\n\nex) 위의 그림은 마지막에 8배로 Upsampling을 했으므로 FCN-8s"
  },
  {
    "objectID": "docs/DL/posts/U-net_summary.html",
    "href": "docs/DL/posts/U-net_summary.html",
    "title": "U-net paper review",
    "section": "",
    "text": "image.png\n\n\n\n\n\n\nimport argparse\n\nimport os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom torchvision import transforms, datasets\n\n+ CNN output size 계산\n$ outputsize = + 1$\n\n아래에서 정의하는 CBR2d를 위 식에 적용해보면 outputsize = inputsize인 것을 알 수 있음\n이미지의 사이즈는 maxpooling을 사용하여 Width와 Height를 절반씩 줄여주는 것을 확인할 수 있음\n\n\nclass Unet(nn.Module):\n    def __init__(self):\n        super(Unet, self).__init__()\n\n        def CBR2d(in_channels, out_channels, kernel_size =3, stride = 1, padding = 1, bias = True): #convolution + bathnormalization + Relu\n            layers = []\n            layers = layers + [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n                                          kernel_size=kernel_size, stride=stride, padding= padding,\n                                          bias=bias)]\n            layers = layers + [nn.BatchNorm2d(num_features = out_channels)]\n            layers = layers + [nn.ReLU()]\n\n            cbr = nn.Sequential(*layers)\n\n            return cbr\n        \n        # contracting(encoder)\n        self.enc1_1 = CBR2d(in_channels=1, out_channels=64)\n        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n\n        self.pool3 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n\n        self.pool4 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n\n        ## Expansive (decoder)\n        self.dec5_2 = CBR2d(in_channels=1024, out_channels=512)\n        \n        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec4_2 = CBR2d(in_channels= 2 * 512, out_channels=512)\n        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n        \n        self.unpool3 = nn.ConvTranspose2d(in_channels= 256, out_channels=256, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec3_2 = CBR2d(in_channels= 2 * 256, out_channels=256)\n        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n\n        self.unpool2 = nn.ConvTranspose2d(in_channels= 128, out_channels=128, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec2_2 = CBR2d(in_channels= 2 * 128, out_channels=128)\n        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n        \n        self.unpool1 = nn.ConvTranspose2d(in_channels= 64, out_channels=64, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec1_2 = CBR2d(in_channels= 2 * 64, out_channels=64)\n        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n\n        self.fc = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1,stride=1, padding = 0, bias = True)\n\n    def forward(self, x):\n        enc1_1 = self.enc1_1(x)\n        enc1_2 = self.enc1_2(enc1_1)\n        pool1 = self.pool1(enc1_2)\n\n        enc2_1 = self.enc2_1(pool1)\n        enc2_2 = self.enc2_2(enc2_1)\n        pool2 = self.pool2(enc2_2)\n\n        enc3_1 = self.enc3_1(pool2)\n        enc3_2 = self.enc3_2(enc3_1)\n        pool3 = self.pool3(enc3_2)\n\n        enc4_1 = self.enc4_1(pool3)\n        enc4_2 = self.enc4_2(enc4_1)\n        pool4 = self.pool4(enc4_2)\n\n        enc5_1 = self.enc5_1(pool4)\n\n        dec5_1 = self.dec5_1(enc5_1)\n\n        unpool4 = self.unpool4(dec5_1)\n        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n        dec4_2 = self.dec4_2(cat4)\n        dec4_1 = self.dec4_1(dec4_2)\n\n        unpool3 = self.unpool3(dec4_1)\n        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n        dec3_2 = self.dec3_2(cat3)\n        dec3_1 = self.dec3_1(dec3_2)\n\n        unpool2 = self.unpool2(dec3_1)\n        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n        dec2_2 = self.dec2_2(cat2)\n        dec2_1 = self.dec2_1(dec2_2)\n\n        unpool1 = self.unpool1(dec2_1)\n        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n        dec1_2 = self.dec1_2(cat1)\n        dec1_1 = self.dec1_1(dec1_2)\n\n        x = self.fc(dec1_1)\n\n        return x\n\n\n\n\n\n\n사용 예시\n\nself.unpool1 = nn.ConvTranspose2d(in_channels= 64, out_channels=64, kernel_size=2, stride=2, padding= 0,bias=True)\nhttps://cumulu-s.tistory.com/29\n\n\n\n- dim = 1로 지정하면 채널을 기준으로 concat  \n- dim = 0일 때 -&gt; batch를 기준으로\n- dim = 2일 때 -&gt; height를 기준으로\n- dim = 3일 떄 -&gt; width를 기준으로\n\na = torch.tensor([[[[1,2,3,4],[5,6,7,8],[9,10,11,12]]]])\nb = torch.tensor([[[[1,2,3,4],[5,6,7,8],[9,10,11,12]]]])\nprint(a)\n\ntensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]])\n\n\n\ntest_dim0 = torch.cat((a,b),dim = 0)\nprint(\"dim = 0일 때: \\n\",test_dim0, '\\n', test_dim0.shape)\n\ndim = 0일 때: \n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]],\n\n\n        [[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]]) \n torch.Size([2, 1, 3, 4])\n\n\n\ntest_dim1 = torch.cat((a,b),dim = 1)\nprint(\"dim = 1일 때: \\n\",test_dim1, '\\n', test_dim1.shape)\n\ndim = 1일 때: \n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]],\n\n         [[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]]) \n torch.Size([1, 2, 3, 4])\n\n\n\ntest_dim2 = torch.cat((a,b),dim = 2)\nprint(\"dim = 2일 때: \\n\",test_dim2, '\\n', test_dim2.shape)\n\ndim = 2일 때: \n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12],\n          [ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]]) \n torch.Size([1, 1, 6, 4])\n\n\n\ntest_dim3 = torch.cat((a,b),dim = 3)\nprint(\"dim = 3일 때: \\n\",test_dim3, '\\n', test_dim3.shape)\n\ndim = 3일 때: \n tensor([[[[ 1,  2,  3,  4,  1,  2,  3,  4],\n          [ 5,  6,  7,  8,  5,  6,  7,  8],\n          [ 9, 10, 11, 12,  9, 10, 11, 12]]]]) \n torch.Size([1, 1, 3, 8])\n\n\n\n\n\n\n\n- loss 곡선\n\n\n\nimage.png\n\n\n- 만들어진 annotation\n\n\n\n\n\n\n\nimport os\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\n## 트레이닝 파라미터 설정\nlr = 1e-3\nbatch_size = 2\nnum_epoch = 50\n\ndata_dir = './datasets'\nckpt_dir = './checkpoint'\nlog_dir = './log'\n\n\n##\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n\n        lst_data = os.listdir(self.data_dir)\n\n        lst_label = [f for f in lst_data if f.startswith('label')]\n        lst_input = [f for f in lst_data if f.startswith('input')]\n\n        lst_label.sort()\n        lst_input.sort()\n\n        self.lst_label = lst_label\n        self.lst_input = lst_input\n\n    def __len__(self):\n        return len(self.lst_label)\n\n    def __getitem__(self, index):\n        label = np.load(os.path.join(self.data_dir, self.lst_label[index]))\n        input = np.load(os.path.join(self.data_dir, self.lst_input[index]))\n\n        label = label/255.0\n        input = input/255.0\n\n        if label.ndim == 2:\n            label = label[:, :, np.newaxis]\n        if input.ndim == 2:\n            input = input[:, :, np.newaxis]\n\n        data = {'input': input, 'label': label}\n\n        if self.transform:\n            data = self.transform(data)\n\n        return data\n##\nclass ToTensor(object):\n    def __call__(self, data):\n        label, input = data['label'], data['input']\n\n        label = label.transpose((2, 0, 1)).astype(np.float32)\n        input = input.transpose((2, 0, 1)).astype(np.float32)\n\n        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n        return data\n\nclass Normalization(object):\n    def __init__(self, mean = 0.5, std = 0.5):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, data):\n        label, input = data['label'], data['input']\n\n        input = (input - self.mean) / self.std\n        data = {'label': label, 'input': input}\n\n        return data\n\n\nclass RandomFlip(object):\n    def __call__(self, data):\n        label, input = data['label'], data['input']\n\n        if np.random.rand() &gt; 0.5:\n            label = np.fliplr(label)\n            input = np.fliplr(input)\n\n        if np.random.rand() &gt; 0.5:\n            label = np.flipud(label)\n            input = np.flipud(input)\n\n        data = {'label': label, 'input': input}\n\n        return data\n\n## 네트워크 구축하기\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n            layers = []\n            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n                                 kernel_size=kernel_size, stride=stride, padding=padding,\n                                 bias=bias)]\n            layers += [nn.BatchNorm2d(num_features=out_channels)]\n            layers += [nn.ReLU()]\n\n            cbr = nn.Sequential(*layers)\n\n            return cbr\n\n        # Contracting path\n        self.enc1_1 = CBR2d(in_channels=1, out_channels=64)\n        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n\n        self.pool3 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n\n        self.pool4 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n\n        # Expansive path\n        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n\n        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec4_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n\n        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec3_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n\n        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec2_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n\n        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec1_2 = CBR2d(in_channels=2 * 64, out_channels=64)\n        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n\n        self.fc = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n\n    def forward(self, x):\n        enc1_1 = self.enc1_1(x)\n        enc1_2 = self.enc1_2(enc1_1)\n        pool1 = self.pool1(enc1_2)\n\n        enc2_1 = self.enc2_1(pool1)\n        enc2_2 = self.enc2_2(enc2_1)\n        pool2 = self.pool2(enc2_2)\n\n        enc3_1 = self.enc3_1(pool2)\n        enc3_2 = self.enc3_2(enc3_1)\n        pool3 = self.pool3(enc3_2)\n\n        enc4_1 = self.enc4_1(pool3)\n        enc4_2 = self.enc4_2(enc4_1)\n        pool4 = self.pool4(enc4_2)\n\n        enc5_1 = self.enc5_1(pool4)\n\n        dec5_1 = self.dec5_1(enc5_1)\n\n        unpool4 = self.unpool4(dec5_1)\n        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n        dec4_2 = self.dec4_2(cat4)\n        dec4_1 = self.dec4_1(dec4_2)\n\n        unpool3 = self.unpool3(dec4_1)\n        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n        dec3_2 = self.dec3_2(cat3)\n        dec3_1 = self.dec3_1(dec3_2)\n\n        unpool2 = self.unpool2(dec3_1)\n        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n        dec2_2 = self.dec2_2(cat2)\n        dec2_1 = self.dec2_1(dec2_2)\n\n        unpool1 = self.unpool1(dec2_1)\n        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n        dec1_2 = self.dec1_2(cat1)\n        dec1_1 = self.dec1_1(dec1_2)\n\n        x = self.fc(dec1_1)\n\n        return x\n\n## 네트워크 학습하기\ntransform = transforms.Compose([Normalization(mean=0.5, std=0.5), RandomFlip(), ToTensor()])\n\ndataset_train = Dataset(data_dir=os.path.join(data_dir, 'train'), transform=transform)\nloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n\ndataset_val = Dataset(data_dir=os.path.join(data_dir, 'val'), transform=transform)\nloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True, num_workers=0)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n## 네트워크 생성하기\nnet = UNet().to(device)\n\nfn_loss = nn.BCEWithLogitsLoss().to(device)\noptim = torch.optim.Adam(net.parameters(),lr=lr)\n\nnum_data_train = len(dataset_train)\nnum_data_val = len(dataset_val)\n\nnum_batch_train = np.ceil(num_data_train / batch_size)\nnum_batch_val = np.ceil(num_data_val / batch_size)\n\n## function\nfn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\nfn_denorm = lambda x, mean, std: (x * std) + mean\nfn_clss = lambda x: 1.0*(x&gt;0.5)\n\n## tensorboard를 사용하기 위한 SummaryWriter 설정\nwriter_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\nwriter_val = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))\n\n## 네트워크 학습시키기\nst_epoch = 0\n\n## 네트워크 저장하기\ndef save(ckpt_dir, net, optim, epoch):\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n\n    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n               \"%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n\n\n## 네트워크 불러오기\ndef load(ckpt_dir, net, optim):\n    if not os.path.exists(ckpt_dir):\n        epoch = 0\n        return net, optim, epoch\n\n    ckpt_lst = os.listdir(ckpt_dir)\n    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n\n    dict_model = torch.load('%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n\n    net.load_state_dict(dict_model['net'])\n    optim.load_state_dict(dict_model['optim'])\n    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n\n    return net, optim, epoch\n\n#net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n\nfor epoch in range(st_epoch + 1, num_epoch + 1):\n    net.train()\n    loss_arr = []\n\n    for batch, data in enumerate(loader_train, 1):\n        # forward pass\n        label = data['label'].to(device)\n        input = data['input'].to(device)\n\n        output = net(input)\n\n        # backward pass\n        optim.zero_grad()\n\n        loss = fn_loss(output, label)\n        loss.backward()\n\n        optim.step()\n\n        # 손실함수 계산\n        loss_arr += [loss.item()]\n\n        print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f\" %\n              (epoch, num_epoch, batch, num_batch_train, np.mean(loss_arr)))\n\n        # Tensorboard 저장하기\n        label = fn_tonumpy(label)\n        input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n        output = fn_tonumpy(fn_clss(output))\n\n        writer_train.add_image('label', label, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n        writer_train.add_image('input', input, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n        writer_train.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n\n    writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n\n    with torch.no_grad():\n        net.eval()\n        loss_arr = []\n\n        for batch, data in enumerate(loader_val, 1):\n            # forward pass\n            label = data['label'].to(device)\n            input = data['input'].to(device)\n\n            output = net(input)\n\n            # 손실함수 계산하기\n            loss = fn_loss(output, label)\n\n            loss_arr += [loss.item()]\n\n            print(\"VALID: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f\" %\n                  (epoch, num_epoch, batch, num_batch_val, np.mean(loss_arr)))\n\n            # Tensorboard 저장하기\n            label = fn_tonumpy(label)\n            input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n            output = fn_tonumpy(fn_clss(output))\n\n            writer_val.add_image('label', label, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n            writer_val.add_image('input', input, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n            writer_val.add_image('output', output, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n\n    writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n\n    if epoch % 50 == 0:\n        save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch)\n\nwriter_train.close()\nwriter_val.close()"
  },
  {
    "objectID": "docs/DL/posts/U-net_summary.html#논문에-나오는-구조",
    "href": "docs/DL/posts/U-net_summary.html#논문에-나오는-구조",
    "title": "U-net paper review",
    "section": "",
    "text": "image.png"
  },
  {
    "objectID": "docs/DL/posts/U-net_summary.html#논문을-기반으로-네트워크-구현",
    "href": "docs/DL/posts/U-net_summary.html#논문을-기반으로-네트워크-구현",
    "title": "U-net paper review",
    "section": "",
    "text": "import argparse\n\nimport os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom torchvision import transforms, datasets\n\n+ CNN output size 계산\n$ outputsize = + 1$\n\n아래에서 정의하는 CBR2d를 위 식에 적용해보면 outputsize = inputsize인 것을 알 수 있음\n이미지의 사이즈는 maxpooling을 사용하여 Width와 Height를 절반씩 줄여주는 것을 확인할 수 있음\n\n\nclass Unet(nn.Module):\n    def __init__(self):\n        super(Unet, self).__init__()\n\n        def CBR2d(in_channels, out_channels, kernel_size =3, stride = 1, padding = 1, bias = True): #convolution + bathnormalization + Relu\n            layers = []\n            layers = layers + [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n                                          kernel_size=kernel_size, stride=stride, padding= padding,\n                                          bias=bias)]\n            layers = layers + [nn.BatchNorm2d(num_features = out_channels)]\n            layers = layers + [nn.ReLU()]\n\n            cbr = nn.Sequential(*layers)\n\n            return cbr\n        \n        # contracting(encoder)\n        self.enc1_1 = CBR2d(in_channels=1, out_channels=64)\n        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n\n        self.pool3 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n\n        self.pool4 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n\n        ## Expansive (decoder)\n        self.dec5_2 = CBR2d(in_channels=1024, out_channels=512)\n        \n        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec4_2 = CBR2d(in_channels= 2 * 512, out_channels=512)\n        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n        \n        self.unpool3 = nn.ConvTranspose2d(in_channels= 256, out_channels=256, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec3_2 = CBR2d(in_channels= 2 * 256, out_channels=256)\n        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n\n        self.unpool2 = nn.ConvTranspose2d(in_channels= 128, out_channels=128, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec2_2 = CBR2d(in_channels= 2 * 128, out_channels=128)\n        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n        \n        self.unpool1 = nn.ConvTranspose2d(in_channels= 64, out_channels=64, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec1_2 = CBR2d(in_channels= 2 * 64, out_channels=64)\n        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n\n        self.fc = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1,stride=1, padding = 0, bias = True)\n\n    def forward(self, x):\n        enc1_1 = self.enc1_1(x)\n        enc1_2 = self.enc1_2(enc1_1)\n        pool1 = self.pool1(enc1_2)\n\n        enc2_1 = self.enc2_1(pool1)\n        enc2_2 = self.enc2_2(enc2_1)\n        pool2 = self.pool2(enc2_2)\n\n        enc3_1 = self.enc3_1(pool2)\n        enc3_2 = self.enc3_2(enc3_1)\n        pool3 = self.pool3(enc3_2)\n\n        enc4_1 = self.enc4_1(pool3)\n        enc4_2 = self.enc4_2(enc4_1)\n        pool4 = self.pool4(enc4_2)\n\n        enc5_1 = self.enc5_1(pool4)\n\n        dec5_1 = self.dec5_1(enc5_1)\n\n        unpool4 = self.unpool4(dec5_1)\n        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n        dec4_2 = self.dec4_2(cat4)\n        dec4_1 = self.dec4_1(dec4_2)\n\n        unpool3 = self.unpool3(dec4_1)\n        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n        dec3_2 = self.dec3_2(cat3)\n        dec3_1 = self.dec3_1(dec3_2)\n\n        unpool2 = self.unpool2(dec3_1)\n        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n        dec2_2 = self.dec2_2(cat2)\n        dec2_1 = self.dec2_1(dec2_2)\n\n        unpool1 = self.unpool1(dec2_1)\n        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n        dec1_2 = self.dec1_2(cat1)\n        dec1_1 = self.dec1_1(dec1_2)\n\n        x = self.fc(dec1_1)\n\n        return x\n\n\n\n\n\n\n사용 예시\n\nself.unpool1 = nn.ConvTranspose2d(in_channels= 64, out_channels=64, kernel_size=2, stride=2, padding= 0,bias=True)\nhttps://cumulu-s.tistory.com/29\n\n\n\n- dim = 1로 지정하면 채널을 기준으로 concat  \n- dim = 0일 때 -&gt; batch를 기준으로\n- dim = 2일 때 -&gt; height를 기준으로\n- dim = 3일 떄 -&gt; width를 기준으로\n\na = torch.tensor([[[[1,2,3,4],[5,6,7,8],[9,10,11,12]]]])\nb = torch.tensor([[[[1,2,3,4],[5,6,7,8],[9,10,11,12]]]])\nprint(a)\n\ntensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]])\n\n\n\ntest_dim0 = torch.cat((a,b),dim = 0)\nprint(\"dim = 0일 때: \\n\",test_dim0, '\\n', test_dim0.shape)\n\ndim = 0일 때: \n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]],\n\n\n        [[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]]) \n torch.Size([2, 1, 3, 4])\n\n\n\ntest_dim1 = torch.cat((a,b),dim = 1)\nprint(\"dim = 1일 때: \\n\",test_dim1, '\\n', test_dim1.shape)\n\ndim = 1일 때: \n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]],\n\n         [[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]]) \n torch.Size([1, 2, 3, 4])\n\n\n\ntest_dim2 = torch.cat((a,b),dim = 2)\nprint(\"dim = 2일 때: \\n\",test_dim2, '\\n', test_dim2.shape)\n\ndim = 2일 때: \n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12],\n          [ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]]) \n torch.Size([1, 1, 6, 4])\n\n\n\ntest_dim3 = torch.cat((a,b),dim = 3)\nprint(\"dim = 3일 때: \\n\",test_dim3, '\\n', test_dim3.shape)\n\ndim = 3일 때: \n tensor([[[[ 1,  2,  3,  4,  1,  2,  3,  4],\n          [ 5,  6,  7,  8,  5,  6,  7,  8],\n          [ 9, 10, 11, 12,  9, 10, 11, 12]]]]) \n torch.Size([1, 1, 3, 8])"
  },
  {
    "objectID": "docs/DL/posts/U-net_summary.html#결과",
    "href": "docs/DL/posts/U-net_summary.html#결과",
    "title": "U-net paper review",
    "section": "",
    "text": "- loss 곡선\n\n\n\nimage.png\n\n\n- 만들어진 annotation"
  },
  {
    "objectID": "docs/DL/posts/U-net_summary.html#참고-u-net-훈련을-위한-.py-파일",
    "href": "docs/DL/posts/U-net_summary.html#참고-u-net-훈련을-위한-.py-파일",
    "title": "U-net paper review",
    "section": "",
    "text": "import os\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\n## 트레이닝 파라미터 설정\nlr = 1e-3\nbatch_size = 2\nnum_epoch = 50\n\ndata_dir = './datasets'\nckpt_dir = './checkpoint'\nlog_dir = './log'\n\n\n##\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n\n        lst_data = os.listdir(self.data_dir)\n\n        lst_label = [f for f in lst_data if f.startswith('label')]\n        lst_input = [f for f in lst_data if f.startswith('input')]\n\n        lst_label.sort()\n        lst_input.sort()\n\n        self.lst_label = lst_label\n        self.lst_input = lst_input\n\n    def __len__(self):\n        return len(self.lst_label)\n\n    def __getitem__(self, index):\n        label = np.load(os.path.join(self.data_dir, self.lst_label[index]))\n        input = np.load(os.path.join(self.data_dir, self.lst_input[index]))\n\n        label = label/255.0\n        input = input/255.0\n\n        if label.ndim == 2:\n            label = label[:, :, np.newaxis]\n        if input.ndim == 2:\n            input = input[:, :, np.newaxis]\n\n        data = {'input': input, 'label': label}\n\n        if self.transform:\n            data = self.transform(data)\n\n        return data\n##\nclass ToTensor(object):\n    def __call__(self, data):\n        label, input = data['label'], data['input']\n\n        label = label.transpose((2, 0, 1)).astype(np.float32)\n        input = input.transpose((2, 0, 1)).astype(np.float32)\n\n        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n        return data\n\nclass Normalization(object):\n    def __init__(self, mean = 0.5, std = 0.5):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, data):\n        label, input = data['label'], data['input']\n\n        input = (input - self.mean) / self.std\n        data = {'label': label, 'input': input}\n\n        return data\n\n\nclass RandomFlip(object):\n    def __call__(self, data):\n        label, input = data['label'], data['input']\n\n        if np.random.rand() &gt; 0.5:\n            label = np.fliplr(label)\n            input = np.fliplr(input)\n\n        if np.random.rand() &gt; 0.5:\n            label = np.flipud(label)\n            input = np.flipud(input)\n\n        data = {'label': label, 'input': input}\n\n        return data\n\n## 네트워크 구축하기\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n            layers = []\n            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n                                 kernel_size=kernel_size, stride=stride, padding=padding,\n                                 bias=bias)]\n            layers += [nn.BatchNorm2d(num_features=out_channels)]\n            layers += [nn.ReLU()]\n\n            cbr = nn.Sequential(*layers)\n\n            return cbr\n\n        # Contracting path\n        self.enc1_1 = CBR2d(in_channels=1, out_channels=64)\n        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n\n        self.pool3 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n\n        self.pool4 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n\n        # Expansive path\n        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n\n        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec4_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n\n        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec3_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n\n        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec2_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n\n        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec1_2 = CBR2d(in_channels=2 * 64, out_channels=64)\n        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n\n        self.fc = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n\n    def forward(self, x):\n        enc1_1 = self.enc1_1(x)\n        enc1_2 = self.enc1_2(enc1_1)\n        pool1 = self.pool1(enc1_2)\n\n        enc2_1 = self.enc2_1(pool1)\n        enc2_2 = self.enc2_2(enc2_1)\n        pool2 = self.pool2(enc2_2)\n\n        enc3_1 = self.enc3_1(pool2)\n        enc3_2 = self.enc3_2(enc3_1)\n        pool3 = self.pool3(enc3_2)\n\n        enc4_1 = self.enc4_1(pool3)\n        enc4_2 = self.enc4_2(enc4_1)\n        pool4 = self.pool4(enc4_2)\n\n        enc5_1 = self.enc5_1(pool4)\n\n        dec5_1 = self.dec5_1(enc5_1)\n\n        unpool4 = self.unpool4(dec5_1)\n        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n        dec4_2 = self.dec4_2(cat4)\n        dec4_1 = self.dec4_1(dec4_2)\n\n        unpool3 = self.unpool3(dec4_1)\n        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n        dec3_2 = self.dec3_2(cat3)\n        dec3_1 = self.dec3_1(dec3_2)\n\n        unpool2 = self.unpool2(dec3_1)\n        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n        dec2_2 = self.dec2_2(cat2)\n        dec2_1 = self.dec2_1(dec2_2)\n\n        unpool1 = self.unpool1(dec2_1)\n        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n        dec1_2 = self.dec1_2(cat1)\n        dec1_1 = self.dec1_1(dec1_2)\n\n        x = self.fc(dec1_1)\n\n        return x\n\n## 네트워크 학습하기\ntransform = transforms.Compose([Normalization(mean=0.5, std=0.5), RandomFlip(), ToTensor()])\n\ndataset_train = Dataset(data_dir=os.path.join(data_dir, 'train'), transform=transform)\nloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n\ndataset_val = Dataset(data_dir=os.path.join(data_dir, 'val'), transform=transform)\nloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True, num_workers=0)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n## 네트워크 생성하기\nnet = UNet().to(device)\n\nfn_loss = nn.BCEWithLogitsLoss().to(device)\noptim = torch.optim.Adam(net.parameters(),lr=lr)\n\nnum_data_train = len(dataset_train)\nnum_data_val = len(dataset_val)\n\nnum_batch_train = np.ceil(num_data_train / batch_size)\nnum_batch_val = np.ceil(num_data_val / batch_size)\n\n## function\nfn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\nfn_denorm = lambda x, mean, std: (x * std) + mean\nfn_clss = lambda x: 1.0*(x&gt;0.5)\n\n## tensorboard를 사용하기 위한 SummaryWriter 설정\nwriter_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\nwriter_val = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))\n\n## 네트워크 학습시키기\nst_epoch = 0\n\n## 네트워크 저장하기\ndef save(ckpt_dir, net, optim, epoch):\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n\n    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n               \"%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n\n\n## 네트워크 불러오기\ndef load(ckpt_dir, net, optim):\n    if not os.path.exists(ckpt_dir):\n        epoch = 0\n        return net, optim, epoch\n\n    ckpt_lst = os.listdir(ckpt_dir)\n    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n\n    dict_model = torch.load('%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n\n    net.load_state_dict(dict_model['net'])\n    optim.load_state_dict(dict_model['optim'])\n    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n\n    return net, optim, epoch\n\n#net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n\nfor epoch in range(st_epoch + 1, num_epoch + 1):\n    net.train()\n    loss_arr = []\n\n    for batch, data in enumerate(loader_train, 1):\n        # forward pass\n        label = data['label'].to(device)\n        input = data['input'].to(device)\n\n        output = net(input)\n\n        # backward pass\n        optim.zero_grad()\n\n        loss = fn_loss(output, label)\n        loss.backward()\n\n        optim.step()\n\n        # 손실함수 계산\n        loss_arr += [loss.item()]\n\n        print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f\" %\n              (epoch, num_epoch, batch, num_batch_train, np.mean(loss_arr)))\n\n        # Tensorboard 저장하기\n        label = fn_tonumpy(label)\n        input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n        output = fn_tonumpy(fn_clss(output))\n\n        writer_train.add_image('label', label, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n        writer_train.add_image('input', input, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n        writer_train.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n\n    writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n\n    with torch.no_grad():\n        net.eval()\n        loss_arr = []\n\n        for batch, data in enumerate(loader_val, 1):\n            # forward pass\n            label = data['label'].to(device)\n            input = data['input'].to(device)\n\n            output = net(input)\n\n            # 손실함수 계산하기\n            loss = fn_loss(output, label)\n\n            loss_arr += [loss.item()]\n\n            print(\"VALID: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f\" %\n                  (epoch, num_epoch, batch, num_batch_val, np.mean(loss_arr)))\n\n            # Tensorboard 저장하기\n            label = fn_tonumpy(label)\n            input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n            output = fn_tonumpy(fn_clss(output))\n\n            writer_val.add_image('label', label, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n            writer_val.add_image('input', input, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n            writer_val.add_image('output', output, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n\n    writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n\n    if epoch % 50 == 0:\n        save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch)\n\nwriter_train.close()\nwriter_val.close()"
  },
  {
    "objectID": "docs/DL/posts/DCGAN.html",
    "href": "docs/DL/posts/DCGAN.html",
    "title": "DCGAN paper review",
    "section": "",
    "text": "[DCGAN 장점] - 대부분의 상황에서 안정적으로 학습이 됨 - word2vec과 같이 DCGAN으로 학습된 Generator가 벡터 산술 연산이 가능한 성질을 갖음\n - DCGAN이 학습한 필터를 시각화하여 보여줌 -&gt; 특정 필터들이 이미지의 특정 물체를 학습했음을 보여줌\n\n\n성능면에서 비지도 학습 알고리즘에 비해 우수함\n\n[APPROACH AND MODEL ARCHITECTURE]\nDCGAN은\n- Max Pooling To Strided Convolution - Fully-Connected Layer 삭제 - BatchNormalization을 추가함 -&gt; deep한 모델이더라도 gradient의 흐름이 잘 전달됨 - ReLU와 Leaky ReLU를 사용\n[DETAILS OF ADVERSARIAL TRAINING]\n\n모델 및 옵티마이저\n\n\nmini-batch Stochastic Gradient Descent(SGD) a with batch size of 128\nAll weight: zero-centered Normal distribution with std 0.02\nLeaky ReLU: slope 0.2\nOptimizer: Adam (GAN에서는 momentum 사용)\nlearning late: 0.0002(0.001은 너무 커서..)\nD’s criterion= \\(\\log(D(x))\\)(real data) + \\(\\log(1 - D(G(x)))\\)(fake data)\nG’s criterion = \\(\\log(D(G(z)))\\)\n\n데이터\n\nLSUN\nFACES\nIMAGENET-1K\n\n\nDCGAN에서 중요한 기준 - NOT MIMICKING TRAIN DATA -&gt; 단순히 학습 데이터를 모방하면 안됨! - “Walking in the latent Space” -&gt; G의 input z의 공간인 latent Space에서 z1에서 z2로 살짝 이동한다 하더라도 급작스러운 변화가 일어나지 않고 물흐르듯 부드러운 변화를 보여줘야 한다.\n\n\n\n\n\n\n\nimport random\nimport math\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom PIL import Image\n\nimport torch\nimport torch.utils.data as data\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision.datasets as dsets\nfrom torchvision import transforms\n\ntorch.manual_seed(1234)\nnp.random.seed(1234)\nrandom.seed(1234)\n\n\n\n\ndef conv_dim(i,k,s,p):\n    '''\n    nn.Conv2d 사용할 때 사이즈 계산\n    i: input image size\n    k: kernel size\n    s: stride\n    p: padding\n    '''\n    out = (i - k + 2*p)/s + 1\n    return out  \n\n\ndef convt_dim(i,k,s,p):\n    '''\n    nn.ConvTranspose2d 사용할 때 사이즈 계산\n    i: input image size\n    k: kernel size\n    s: stride\n    p: padding\n    '''\n    out = (i-1) * s - 2 * p + k\n    return out  \n\n\n\n\n\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim = 20, image_size = 64):\n        super(Generator, self).__init__()\n\n        # layer1 -&gt; W(H) * 4\n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, image_size * 8, kernel_size = 4, stride = 1),\n            nn.BatchNorm2d(image_size * 8),\n            nn.ReLU(inplace=True))\n        \n        # layer2 -&gt; W(H) * 2\n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 8, image_size * 4, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size * 4),\n            nn.ReLU(inplace=True))\n        \n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 4, image_size * 2, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size * 2),\n            nn.ReLU(inplace=True))\n        \n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 2, image_size, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size),\n            nn.ReLU(inplace=True))\n        \n        self.last = nn.Sequential(\n            nn.ConvTranspose2d(image_size, 1, kernel_size= 4, stride=2, padding=1), # 흑백 이미지이므로 출력 차원을 1으로 지정한 것\n            nn.Tanh())\n        \n    def forward(self, z):\n        out = self.layer1(z)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        out = out.type(torch.FloatTensor)\n        return out\n\n\n\n\nG = Generator(z_dim=20, image_size=64)\ninput_z = torch.randn(1, 20)\n\n# tensor size -&gt; (1, 20, 1, 1) \n# pytorch: (batch_size, channel, height, width)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n\nfake_img = G(input_z)\n\n\n# fake_img[0][0].size() -&gt; (64, 64)\nimg_transformed = fake_img[0][0].detach().numpy()\nplt.imshow(img_transformed, 'gray')\nplt.show()\n\n\n\n\n\n\n\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, z_dim=20, image_size=64):\n        super(Discriminator, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, image_size, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(image_size, image_size*2, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(image_size*2, image_size*4, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(image_size*4, image_size*8, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.last = nn.Conv2d(image_size*8, 1, kernel_size=4, stride=1)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        out = out.type(torch.FloatTensor)\n        return out\n\n\nD = Discriminator(z_dim=20, image_size=64)\n\nd_out = D(fake_img)\n\n# 출력 d_out에 Sigmoid를 곱해 0에서 1로 변환\nprint(nn.Sigmoid()(d_out))\n\ntensor([[[[0.4980]]]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\n\n\n\nmini_batch_size = 2\n\n# 정답 라벨 생성 -&gt; torch.tensor([1,1])\nlabel_real = torch.full((mini_batch_size,), 1)\n\n# 가짜 라벨 생성 -&gt; torch.tensor([0,0])\nlabel_fake = torch.full((mini_batch_size,), 0)\n\n\n# 가짜 이미지 생성\ninput_z = torch.randn(mini_batch_size, 20)\n# tensor size -&gt; (2,20,1,1)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1) \n# G에 의해 tensor size -&gt; (2,1,64,64)\nfake_images = G(input_z)\n# D에 의해 tensor size -&gt; (2,1,1,1)\nd_out_fake = D(fake_images)\n\n\n\n\\(maximize\\) \\(log(D(x))\\)+\\(log(1 - D(G(z)))\\)\n# loss function 정의\nloss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n\n# 진짜 이미지 판정\nd_out_real = D(x)\n\n# 가짜 이미지 생성\ninput_z = torch.randn(mini_batch_size, 20)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1) \nfake_images = G(input_z)\nd_out_fake = D(fake_images)\n\n# 오차를 계산\nd_loss_real = loss_fn(d_out_real.view(-1), label_real)\nd_loss_fake = loss_fn(d_out_fake.view(-1), label_fake)\nd_loss = d_loss_real + d_loss_fake\n\n\n\n\\(maximize\\) \\(log\\)\\((D(G(z)))\\)\n# 가짜 화상을 생성해 판정\ninput_z = torch.randn(mini_batch_size, 20)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\nfake_images = G(input_z)\nd_out_fake = D(fake_images)\n\n# 오차를 계산\ng_loss = criterion(d_out_fake.view(-1), label_real)\n\n\n\n\n\n\n\nimport os\nimport urllib.request\nimport zipfile\nimport tarfile\n\n\nfrom sklearn.datasets import fetch_openml\n\nmnist_F = fetch_openml('mnist_784', version=1, data_home=\"./data/\")  \n\n\n\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', version=1, data_home=\"./data/\")  \n\n\nX = mnist.data.to_numpy()\ny = mnist.target.to_numpy()\n\n\ndata_dir_path = \"./data/img_78/\"\nif not os.path.exists(data_dir_path):\n    os.mkdir(data_dir_path)\n\n\n# MNIST에서 숫자7, 8의 화상만 \"img_78\" 폴더에 화상으로 저장해 나간다\ncount7=0\ncount8=0\nmax_num=200  # 화상은 200장씩 작성한다\n\nfor i in range(len(X)):\n    \n    # 화상7 작성\n    if (y[i] is \"7\") and (count7&lt;max_num):\n        file_path=\"./data/img_78/img_7_\"+str(count7)+\".jpg\"\n        im_f=(X[i].reshape(28, 28))  # 화상을 28×28의 형태로 변경\n        pil_img_f = Image.fromarray(im_f.astype(np.uint8))  # 화상을 PIL으로\n        pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\n        pil_img_f.save(file_path)  # 저장\n        count7+=1 \n    \n    # 화상8 작성\n    if (y[i] is \"8\") and (count8&lt;max_num):\n        file_path=\"./data/img_78/img_8_\"+str(count8)+\".jpg\"\n        im_f=(X[i].reshape(28, 28))  # 화상을 28×28의 형태로 변경\n        pil_img_f = Image.fromarray(im_f.astype(np.uint8))  # 화상을 PIL으로\n        pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\n        pil_img_f.save(file_path)  # 저장\n        count8+=1 \n\n&lt;&gt;:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n&lt;&gt;:18: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n&lt;&gt;:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n&lt;&gt;:18: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if (y[i] is \"7\") and (count7&lt;max_num):\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:18: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if (y[i] is \"8\") and (count8&lt;max_num):\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:13: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n  pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:22: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n  pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\n\n\n\ndef make_datapath_list():\n    train_img_list = list() \n    for img_idx in range(200):\n        img_path = \"./data/img_78/img_7_\" + str(img_idx)+'.jpg'\n        train_img_list.append(img_path)\n\n        img_path = \"./data/img_78/img_8_\" + str(img_idx)+'.jpg'\n        train_img_list.append(img_path)\n\n    return train_img_list\n\n\n\n\n\n\nclass ImageTransform():\n    \"\"\"이미지 전처리 클래스\"\"\"\n\n    def __init__(self, mean, std):\n        self.data_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n\n    def __call__(self, img):\n        return self.data_transform(img)\n\n\nclass GAN_Img_Dataset(data.Dataset):\n    \"\"\"Dataset 클래스. PyTorch의 Dataset 클래스를 상속\"\"\"\n\n    def __init__(self, file_list, transform):\n        self.file_list = file_list\n        self.transform = transform\n\n    def __len__(self):\n        '''이미지 개수 반환'''\n        return len(self.file_list)\n\n    def __getitem__(self, index):\n        '''전처리된 이미지를 Tensor 형식 데이터로 변환'''\n\n        img_path = self.file_list[index]\n        img = Image.open(img_path)  # [높이][폭]흑백\n\n        # 이미지 전처리\n        img_transformed = self.transform(img)\n        img_transformed = img_transformed.type(torch.FloatTensor)\n        return img_transformed\n\n\n# DataLoader 작성과 동작 확인\n\n# 파일 리스트를 작성\ntrain_img_list=make_datapath_list()\n\n# Dataset 작성\nmean = (0.5,)\nstd = (0.5,)\ntrain_dataset = GAN_Img_Dataset(file_list=train_img_list, transform=ImageTransform(mean, std))\n\n# DataLoader 작성\nbatch_size = 64\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True)\n\n# 동작 확인\nbatch_iterator = iter(train_dataloader)  # 반복자로 변환\nimges = next(batch_iterator)  # 1번째 요소를 꺼낸다\nprint(imges.size())  # torch.Size([64, 1, 64, 64])\n\ntorch.Size([64, 1, 64, 64])\n\n\n\n\n\n\n\n# 네트워크 초기화\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        # Conv2dとConvTranspose2d 초기화\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n    elif classname.find('BatchNorm') != -1:\n        # BatchNorm2d 초기화\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# 초기화 실시\nG.apply(weights_init)\nD.apply(weights_init)\n\nprint(\"네트워크 초기화 완료\")\n\n네트워크 초기화 완료\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");device\n\ndevice(type='cuda', index=0)\n\n\n\n# 모델을 학습시키는 함수를 작성\ndef train_model(G, D, dataloader, num_epochs):\n\n    # GPU가 사용 가능한지 확인\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"사용 장치: \", device)\n\n    # 최적화 기법 설정\n    g_lr, d_lr = 0.0001, 0.0004\n    beta1, beta2 = 0.0, 0.9\n    g_optimizer = torch.optim.Adam(G.parameters(), g_lr, [beta1, beta2])\n    d_optimizer = torch.optim.Adam(D.parameters(), d_lr, [beta1, beta2])\n\n    # 오차함수 정의\n    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n\n    # 파라미터를 하드코딩\n    z_dim = 20\n    mini_batch_size = 64\n\n    # 네트워크를 GPU로\n    G.to(device)\n    D.to(device)\n\n    G.train()  # 모델을 훈련 모드로\n    D.train()  # 모델을 훈련 모드로\n\n    # 네트워크가 어느 정도 고정되면, 고속화시킨다\n    torch.backends.cudnn.benchmark = True\n\n    num_train_imgs = len(dataloader.dataset)\n    batch_size = dataloader.batch_size\n\n    # 반복 카운터 설정\n    iteration = 1\n    logs = []\n\n    # epoch 루프\n    for epoch in range(num_epochs):\n\n        # 개시 시간을 저장\n        t_epoch_start = time.time()\n        epoch_g_loss = 0.0  # epoch의 손실합\n        epoch_d_loss = 0.0  # epoch의 손실합\n\n        print('-------------')\n        print('Epoch {}/{}'.format(epoch, num_epochs))\n        print('-------------')\n        print('(train)')\n\n        # 데이터 로더에서 minibatch씩 꺼내는 루프\n        for imges in dataloader:\n\n            # --------------------\n            # 1. Discriminator 학습\n            # --------------------\n            # 미니 배치 크기가 1이면, 배치 노멀라이제이션에서 에러가 발생하므로 피한다\n            if imges.size()[0] == 1:\n                continue\n\n            # GPU가 사용 가능하면 GPU로 데이터를 보낸다\n            imges = imges.to(device)\n\n            # 정답 라벨과 가짜 라벨 작성\n            # epoch의 마지막 반복은 미니 배치 수가 줄어든다\n            mini_batch_size = imges.size()[0]\n            label_real = torch.full((mini_batch_size,), 1).to(device)\n            label_fake = torch.full((mini_batch_size,), 0).to(device)\n\n            # 진짜 이미지 판정\n            d_out_real = D(imges)\n\n            # 가짜 이미지 생성해 판정\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1).to(device)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images.to(device))\n\n            # 오차를 계산\n            d_loss_real = criterion(d_out_real.view(-1).to(device), label_real.float())\n            d_loss_fake = criterion(d_out_fake.view(-1).to(device), label_fake.float())\n            d_loss = d_loss_real + d_loss_fake\n\n            # 역전파\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n\n            d_loss.backward()\n            d_optimizer.step()\n\n            # 2. Generator 학습\n            # --------------------\n            # 가짜 이미지 생성해 판정\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1).to(device)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images.to(device))\n\n            # 오차를 계산\n            g_loss = criterion(d_out_fake.view(-1).to(device), label_real.float().to(device))\n\n            # 역전파\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n            g_loss.backward()\n            g_optimizer.step()\n\n            # --------------------\n            # 3. 기록\n            # --------------------\n            epoch_d_loss += d_loss.item()\n            epoch_g_loss += g_loss.item()\n            iteration += 1\n\n        # epoch의 phase별 loss와 정답률\n        t_epoch_finish = time.time()\n        print('-------------')\n        print('epoch {} || Epoch_D_Loss:{:.4f} ||Epoch_G_Loss:{:.4f}'.format(\n            epoch, epoch_d_loss/batch_size, epoch_g_loss/batch_size))\n        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n        t_epoch_start = time.time()\n\n    return G, D\n\n\nnum_epochs = 200\nG_update, D_update = train_model(G, D, dataloader=train_dataloader, num_epochs=num_epochs)\n\n사용 장치:  cuda:0\n-------------\nEpoch 0/200\n-------------\n(train)\n-------------\nepoch 0 || Epoch_D_Loss:0.0147 ||Epoch_G_Loss:0.7212\ntimer:  0.9429 sec.\n-------------\nEpoch 1/200\n-------------\n(train)\n-------------\nepoch 1 || Epoch_D_Loss:0.0108 ||Epoch_G_Loss:0.8667\ntimer:  0.7360 sec.\n-------------\nEpoch 2/200\n-------------\n(train)\n-------------\nepoch 2 || Epoch_D_Loss:0.0266 ||Epoch_G_Loss:0.6338\ntimer:  0.7420 sec.\n-------------\nEpoch 3/200\n-------------\n(train)\n-------------\nepoch 3 || Epoch_D_Loss:0.0029 ||Epoch_G_Loss:0.7435\ntimer:  0.7325 sec.\n-------------\nEpoch 4/200\n-------------\n(train)\n-------------\nepoch 4 || Epoch_D_Loss:0.0038 ||Epoch_G_Loss:0.7857\ntimer:  0.7349 sec.\n-------------\nEpoch 5/200\n-------------\n(train)\n-------------\nepoch 5 || Epoch_D_Loss:0.0128 ||Epoch_G_Loss:0.7397\ntimer:  0.7323 sec.\n-------------\nEpoch 6/200\n-------------\n(train)\n-------------\nepoch 6 || Epoch_D_Loss:0.0058 ||Epoch_G_Loss:0.9182\ntimer:  0.7353 sec.\n-------------\nEpoch 7/200\n-------------\n(train)\n-------------\nepoch 7 || Epoch_D_Loss:0.0561 ||Epoch_G_Loss:0.6317\ntimer:  0.7301 sec.\n-------------\nEpoch 8/200\n-------------\n(train)\n-------------\nepoch 8 || Epoch_D_Loss:0.0046 ||Epoch_G_Loss:0.7038\ntimer:  0.7350 sec.\n-------------\nEpoch 9/200\n-------------\n(train)\n-------------\nepoch 9 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.7608\ntimer:  0.7320 sec.\n-------------\nEpoch 10/200\n-------------\n(train)\n-------------\nepoch 10 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.7988\ntimer:  0.7388 sec.\n-------------\nEpoch 11/200\n-------------\n(train)\n-------------\nepoch 11 || Epoch_D_Loss:0.0035 ||Epoch_G_Loss:0.8410\ntimer:  0.7402 sec.\n-------------\nEpoch 12/200\n-------------\n(train)\n-------------\nepoch 12 || Epoch_D_Loss:0.0554 ||Epoch_G_Loss:0.9072\ntimer:  0.7309 sec.\n-------------\nEpoch 13/200\n-------------\n(train)\n-------------\nepoch 13 || Epoch_D_Loss:0.0054 ||Epoch_G_Loss:0.6871\ntimer:  0.7408 sec.\n-------------\nEpoch 14/200\n-------------\n(train)\n-------------\nepoch 14 || Epoch_D_Loss:0.0027 ||Epoch_G_Loss:0.8104\ntimer:  0.7420 sec.\n-------------\nEpoch 15/200\n-------------\n(train)\n-------------\nepoch 15 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8349\ntimer:  0.7335 sec.\n-------------\nEpoch 16/200\n-------------\n(train)\n-------------\nepoch 16 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8269\ntimer:  0.7387 sec.\n-------------\nEpoch 17/200\n-------------\n(train)\n-------------\nepoch 17 || Epoch_D_Loss:0.0113 ||Epoch_G_Loss:0.9406\ntimer:  0.7346 sec.\n-------------\nEpoch 18/200\n-------------\n(train)\n-------------\nepoch 18 || Epoch_D_Loss:0.0484 ||Epoch_G_Loss:0.7943\ntimer:  0.7341 sec.\n-------------\nEpoch 19/200\n-------------\n(train)\n-------------\nepoch 19 || Epoch_D_Loss:0.0038 ||Epoch_G_Loss:0.7086\ntimer:  0.7367 sec.\n-------------\nEpoch 20/200\n-------------\n(train)\n-------------\nepoch 20 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8174\ntimer:  0.7461 sec.\n-------------\nEpoch 21/200\n-------------\n(train)\n-------------\nepoch 21 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.9420\ntimer:  0.7323 sec.\n-------------\nEpoch 22/200\n-------------\n(train)\n-------------\nepoch 22 || Epoch_D_Loss:0.0253 ||Epoch_G_Loss:0.7721\ntimer:  0.7278 sec.\n-------------\nEpoch 23/200\n-------------\n(train)\n-------------\nepoch 23 || Epoch_D_Loss:0.0093 ||Epoch_G_Loss:0.7429\ntimer:  0.7256 sec.\n-------------\nEpoch 24/200\n-------------\n(train)\n-------------\nepoch 24 || Epoch_D_Loss:0.0123 ||Epoch_G_Loss:0.8130\ntimer:  0.7300 sec.\n-------------\nEpoch 25/200\n-------------\n(train)\n-------------\nepoch 25 || Epoch_D_Loss:0.0103 ||Epoch_G_Loss:0.8479\ntimer:  0.7365 sec.\n-------------\nEpoch 26/200\n-------------\n(train)\n-------------\nepoch 26 || Epoch_D_Loss:0.0015 ||Epoch_G_Loss:0.8154\ntimer:  0.7331 sec.\n-------------\nEpoch 27/200\n-------------\n(train)\n-------------\nepoch 27 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8887\ntimer:  0.7366 sec.\n-------------\nEpoch 28/200\n-------------\n(train)\n-------------\nepoch 28 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:1.0104\ntimer:  0.7301 sec.\n-------------\nEpoch 29/200\n-------------\n(train)\n-------------\nepoch 29 || Epoch_D_Loss:0.0665 ||Epoch_G_Loss:0.6415\ntimer:  0.7291 sec.\n-------------\nEpoch 30/200\n-------------\n(train)\n-------------\nepoch 30 || Epoch_D_Loss:0.0060 ||Epoch_G_Loss:0.7702\ntimer:  0.7330 sec.\n-------------\nEpoch 31/200\n-------------\n(train)\n-------------\nepoch 31 || Epoch_D_Loss:0.0018 ||Epoch_G_Loss:0.8099\ntimer:  0.7369 sec.\n-------------\nEpoch 32/200\n-------------\n(train)\n-------------\nepoch 32 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8387\ntimer:  0.7320 sec.\n-------------\nEpoch 33/200\n-------------\n(train)\n-------------\nepoch 33 || Epoch_D_Loss:0.0116 ||Epoch_G_Loss:0.9780\ntimer:  0.7371 sec.\n-------------\nEpoch 34/200\n-------------\n(train)\n-------------\nepoch 34 || Epoch_D_Loss:0.0895 ||Epoch_G_Loss:0.6259\ntimer:  0.7390 sec.\n-------------\nEpoch 35/200\n-------------\n(train)\n-------------\nepoch 35 || Epoch_D_Loss:0.0040 ||Epoch_G_Loss:0.7130\ntimer:  0.7300 sec.\n-------------\nEpoch 36/200\n-------------\n(train)\n-------------\nepoch 36 || Epoch_D_Loss:0.0033 ||Epoch_G_Loss:0.7479\ntimer:  0.7291 sec.\n-------------\nEpoch 37/200\n-------------\n(train)\n-------------\nepoch 37 || Epoch_D_Loss:0.0019 ||Epoch_G_Loss:0.7853\ntimer:  0.7266 sec.\n-------------\nEpoch 38/200\n-------------\n(train)\n-------------\nepoch 38 || Epoch_D_Loss:0.0034 ||Epoch_G_Loss:0.7967\ntimer:  0.7288 sec.\n-------------\nEpoch 39/200\n-------------\n(train)\n-------------\nepoch 39 || Epoch_D_Loss:0.0376 ||Epoch_G_Loss:0.9572\ntimer:  0.7312 sec.\n-------------\nEpoch 40/200\n-------------\n(train)\n-------------\nepoch 40 || Epoch_D_Loss:0.0112 ||Epoch_G_Loss:0.7412\ntimer:  0.7272 sec.\n-------------\nEpoch 41/200\n-------------\n(train)\n-------------\nepoch 41 || Epoch_D_Loss:0.0022 ||Epoch_G_Loss:0.8254\ntimer:  0.7251 sec.\n-------------\nEpoch 42/200\n-------------\n(train)\n-------------\nepoch 42 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8413\ntimer:  0.7251 sec.\n-------------\nEpoch 43/200\n-------------\n(train)\n-------------\nepoch 43 || Epoch_D_Loss:0.0007 ||Epoch_G_Loss:0.8888\ntimer:  0.7370 sec.\n-------------\nEpoch 44/200\n-------------\n(train)\n-------------\nepoch 44 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.9397\ntimer:  0.7321 sec.\n-------------\nEpoch 45/200\n-------------\n(train)\n-------------\nepoch 45 || Epoch_D_Loss:0.0589 ||Epoch_G_Loss:0.7371\ntimer:  0.7245 sec.\n-------------\nEpoch 46/200\n-------------\n(train)\n-------------\nepoch 46 || Epoch_D_Loss:0.0032 ||Epoch_G_Loss:0.7395\ntimer:  0.7276 sec.\n-------------\nEpoch 47/200\n-------------\n(train)\n-------------\nepoch 47 || Epoch_D_Loss:0.0037 ||Epoch_G_Loss:0.8705\ntimer:  0.7277 sec.\n-------------\nEpoch 48/200\n-------------\n(train)\n-------------\nepoch 48 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.8553\ntimer:  0.7288 sec.\n-------------\nEpoch 49/200\n-------------\n(train)\n-------------\nepoch 49 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.8959\ntimer:  0.7374 sec.\n-------------\nEpoch 50/200\n-------------\n(train)\n-------------\nepoch 50 || Epoch_D_Loss:0.0429 ||Epoch_G_Loss:0.9181\ntimer:  0.7258 sec.\n-------------\nEpoch 51/200\n-------------\n(train)\n-------------\nepoch 51 || Epoch_D_Loss:0.0030 ||Epoch_G_Loss:0.7838\ntimer:  0.7300 sec.\n-------------\nEpoch 52/200\n-------------\n(train)\n-------------\nepoch 52 || Epoch_D_Loss:0.0032 ||Epoch_G_Loss:0.8647\ntimer:  0.7330 sec.\n-------------\nEpoch 53/200\n-------------\n(train)\n-------------\nepoch 53 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.8791\ntimer:  0.7306 sec.\n-------------\nEpoch 54/200\n-------------\n(train)\n-------------\nepoch 54 || Epoch_D_Loss:0.0362 ||Epoch_G_Loss:0.9293\ntimer:  0.7291 sec.\n-------------\nEpoch 55/200\n-------------\n(train)\n-------------\nepoch 55 || Epoch_D_Loss:0.0123 ||Epoch_G_Loss:0.7459\ntimer:  0.7313 sec.\n-------------\nEpoch 56/200\n-------------\n(train)\n-------------\nepoch 56 || Epoch_D_Loss:0.0033 ||Epoch_G_Loss:0.8016\ntimer:  0.7408 sec.\n-------------\nEpoch 57/200\n-------------\n(train)\n-------------\nepoch 57 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8647\ntimer:  0.7294 sec.\n-------------\nEpoch 58/200\n-------------\n(train)\n-------------\nepoch 58 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8626\ntimer:  0.7275 sec.\n-------------\nEpoch 59/200\n-------------\n(train)\n-------------\nepoch 59 || Epoch_D_Loss:0.0625 ||Epoch_G_Loss:0.9511\ntimer:  0.7284 sec.\n-------------\nEpoch 60/200\n-------------\n(train)\n-------------\nepoch 60 || Epoch_D_Loss:0.0097 ||Epoch_G_Loss:0.7070\ntimer:  0.7263 sec.\n-------------\nEpoch 61/200\n-------------\n(train)\n-------------\nepoch 61 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.7738\ntimer:  0.7272 sec.\n-------------\nEpoch 62/200\n-------------\n(train)\n-------------\nepoch 62 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.7819\ntimer:  0.7280 sec.\n-------------\nEpoch 63/200\n-------------\n(train)\n-------------\nepoch 63 || Epoch_D_Loss:0.0015 ||Epoch_G_Loss:0.8568\ntimer:  0.7297 sec.\n-------------\nEpoch 64/200\n-------------\n(train)\n-------------\nepoch 64 || Epoch_D_Loss:0.0375 ||Epoch_G_Loss:0.8728\ntimer:  0.7267 sec.\n-------------\nEpoch 65/200\n-------------\n(train)\n-------------\nepoch 65 || Epoch_D_Loss:0.0063 ||Epoch_G_Loss:0.6889\ntimer:  0.7381 sec.\n-------------\nEpoch 66/200\n-------------\n(train)\n-------------\nepoch 66 || Epoch_D_Loss:0.0038 ||Epoch_G_Loss:0.8209\ntimer:  0.7297 sec.\n-------------\nEpoch 67/200\n-------------\n(train)\n-------------\nepoch 67 || Epoch_D_Loss:0.0022 ||Epoch_G_Loss:0.8709\ntimer:  0.7559 sec.\n-------------\nEpoch 68/200\n-------------\n(train)\n-------------\nepoch 68 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.8871\ntimer:  0.7298 sec.\n-------------\nEpoch 69/200\n-------------\n(train)\n-------------\nepoch 69 || Epoch_D_Loss:0.0414 ||Epoch_G_Loss:0.9295\ntimer:  0.7275 sec.\n-------------\nEpoch 70/200\n-------------\n(train)\n-------------\nepoch 70 || Epoch_D_Loss:0.0088 ||Epoch_G_Loss:0.7318\ntimer:  0.7226 sec.\n-------------\nEpoch 71/200\n-------------\n(train)\n-------------\nepoch 71 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8082\ntimer:  0.7209 sec.\n-------------\nEpoch 72/200\n-------------\n(train)\n-------------\nepoch 72 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.8387\ntimer:  0.7196 sec.\n-------------\nEpoch 73/200\n-------------\n(train)\n-------------\nepoch 73 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.9041\ntimer:  0.7261 sec.\n-------------\nEpoch 74/200\n-------------\n(train)\n-------------\nepoch 74 || Epoch_D_Loss:0.0522 ||Epoch_G_Loss:0.9280\ntimer:  0.7300 sec.\n-------------\nEpoch 75/200\n-------------\n(train)\n-------------\nepoch 75 || Epoch_D_Loss:0.0056 ||Epoch_G_Loss:0.6952\ntimer:  0.7258 sec.\n-------------\nEpoch 76/200\n-------------\n(train)\n-------------\nepoch 76 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.8027\ntimer:  0.7219 sec.\n-------------\nEpoch 77/200\n-------------\n(train)\n-------------\nepoch 77 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.8502\ntimer:  0.7210 sec.\n-------------\nEpoch 78/200\n-------------\n(train)\n-------------\nepoch 78 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.8654\ntimer:  0.7208 sec.\n-------------\nEpoch 79/200\n-------------\n(train)\n-------------\nepoch 79 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.9063\ntimer:  0.7214 sec.\n-------------\nEpoch 80/200\n-------------\n(train)\n-------------\nepoch 80 || Epoch_D_Loss:0.0458 ||Epoch_G_Loss:0.9730\ntimer:  0.7253 sec.\n-------------\nEpoch 81/200\n-------------\n(train)\n-------------\nepoch 81 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.7860\ntimer:  0.7200 sec.\n-------------\nEpoch 82/200\n-------------\n(train)\n-------------\nepoch 82 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8159\ntimer:  0.7201 sec.\n-------------\nEpoch 83/200\n-------------\n(train)\n-------------\nepoch 83 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.8737\ntimer:  0.7207 sec.\n-------------\nEpoch 84/200\n-------------\n(train)\n-------------\nepoch 84 || Epoch_D_Loss:0.0444 ||Epoch_G_Loss:0.8085\ntimer:  0.7164 sec.\n-------------\nEpoch 85/200\n-------------\n(train)\n-------------\nepoch 85 || Epoch_D_Loss:0.0100 ||Epoch_G_Loss:0.7245\ntimer:  0.7192 sec.\n-------------\nEpoch 86/200\n-------------\n(train)\n-------------\nepoch 86 || Epoch_D_Loss:0.0019 ||Epoch_G_Loss:0.7648\ntimer:  0.7202 sec.\n-------------\nEpoch 87/200\n-------------\n(train)\n-------------\nepoch 87 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8417\ntimer:  0.7172 sec.\n-------------\nEpoch 88/200\n-------------\n(train)\n-------------\nepoch 88 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8620\ntimer:  0.7171 sec.\n-------------\nEpoch 89/200\n-------------\n(train)\n-------------\nepoch 89 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.9353\ntimer:  0.7189 sec.\n-------------\nEpoch 90/200\n-------------\n(train)\n-------------\nepoch 90 || Epoch_D_Loss:0.0538 ||Epoch_G_Loss:0.9575\ntimer:  0.7199 sec.\n-------------\nEpoch 91/200\n-------------\n(train)\n-------------\nepoch 91 || Epoch_D_Loss:0.0036 ||Epoch_G_Loss:0.7752\ntimer:  0.7231 sec.\n-------------\nEpoch 92/200\n-------------\n(train)\n-------------\nepoch 92 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.8001\ntimer:  0.7212 sec.\n-------------\nEpoch 93/200\n-------------\n(train)\n-------------\nepoch 93 || Epoch_D_Loss:0.0029 ||Epoch_G_Loss:0.9208\ntimer:  0.7198 sec.\n-------------\nEpoch 94/200\n-------------\n(train)\n-------------\nepoch 94 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.9002\ntimer:  0.7233 sec.\n-------------\nEpoch 95/200\n-------------\n(train)\n-------------\nepoch 95 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.8902\ntimer:  0.7227 sec.\n-------------\nEpoch 96/200\n-------------\n(train)\n-------------\nepoch 96 || Epoch_D_Loss:0.0467 ||Epoch_G_Loss:0.8899\ntimer:  0.7380 sec.\n-------------\nEpoch 97/200\n-------------\n(train)\n-------------\nepoch 97 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.7825\ntimer:  0.7333 sec.\n-------------\nEpoch 98/200\n-------------\n(train)\n-------------\nepoch 98 || Epoch_D_Loss:0.0037 ||Epoch_G_Loss:0.8136\ntimer:  0.7191 sec.\n-------------\nEpoch 99/200\n-------------\n(train)\n-------------\nepoch 99 || Epoch_D_Loss:0.0096 ||Epoch_G_Loss:0.9419\ntimer:  0.7208 sec.\n-------------\nEpoch 100/200\n-------------\n(train)\n-------------\nepoch 100 || Epoch_D_Loss:0.0014 ||Epoch_G_Loss:0.8698\ntimer:  0.7468 sec.\n-------------\nEpoch 101/200\n-------------\n(train)\n-------------\nepoch 101 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.9751\ntimer:  0.7229 sec.\n-------------\nEpoch 102/200\n-------------\n(train)\n-------------\nepoch 102 || Epoch_D_Loss:0.0005 ||Epoch_G_Loss:0.9452\ntimer:  0.7198 sec.\n-------------\nEpoch 103/200\n-------------\n(train)\n-------------\nepoch 103 || Epoch_D_Loss:0.0007 ||Epoch_G_Loss:0.9833\ntimer:  0.7279 sec.\n-------------\nEpoch 104/200\n-------------\n(train)\n-------------\nepoch 104 || Epoch_D_Loss:0.1555 ||Epoch_G_Loss:1.0839\ntimer:  0.7222 sec.\n-------------\nEpoch 105/200\n-------------\n(train)\n-------------\nepoch 105 || Epoch_D_Loss:0.0103 ||Epoch_G_Loss:0.6298\ntimer:  0.7233 sec.\n-------------\nEpoch 106/200\n-------------\n(train)\n-------------\nepoch 106 || Epoch_D_Loss:0.0061 ||Epoch_G_Loss:0.7351\ntimer:  0.7223 sec.\n-------------\nEpoch 107/200\n-------------\n(train)\n-------------\nepoch 107 || Epoch_D_Loss:0.0036 ||Epoch_G_Loss:0.7819\ntimer:  0.7340 sec.\n-------------\nEpoch 108/200\n-------------\n(train)\n-------------\nepoch 108 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.8024\ntimer:  0.7279 sec.\n-------------\nEpoch 109/200\n-------------\n(train)\n-------------\nepoch 109 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.9204\ntimer:  0.7284 sec.\n-------------\nEpoch 110/200\n-------------\n(train)\n-------------\nepoch 110 || Epoch_D_Loss:0.0069 ||Epoch_G_Loss:1.1190\ntimer:  0.7254 sec.\n-------------\nEpoch 111/200\n-------------\n(train)\n-------------\nepoch 111 || Epoch_D_Loss:0.0006 ||Epoch_G_Loss:1.0324\ntimer:  0.7267 sec.\n-------------\nEpoch 112/200\n-------------\n(train)\n-------------\nepoch 112 || Epoch_D_Loss:0.0014 ||Epoch_G_Loss:0.9086\ntimer:  0.7268 sec.\n-------------\nEpoch 113/200\n-------------\n(train)\n-------------\nepoch 113 || Epoch_D_Loss:0.0014 ||Epoch_G_Loss:0.9236\ntimer:  0.7270 sec.\n-------------\nEpoch 114/200\n-------------\n(train)\n-------------\nepoch 114 || Epoch_D_Loss:0.0561 ||Epoch_G_Loss:0.9581\ntimer:  0.7281 sec.\n-------------\nEpoch 115/200\n-------------\n(train)\n-------------\nepoch 115 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.7752\ntimer:  0.7249 sec.\n-------------\nEpoch 116/200\n-------------\n(train)\n-------------\nepoch 116 || Epoch_D_Loss:0.0021 ||Epoch_G_Loss:0.8565\ntimer:  0.7257 sec.\n-------------\nEpoch 117/200\n-------------\n(train)\n-------------\nepoch 117 || Epoch_D_Loss:0.0317 ||Epoch_G_Loss:0.8358\ntimer:  0.7263 sec.\n-------------\nEpoch 118/200\n-------------\n(train)\n-------------\nepoch 118 || Epoch_D_Loss:0.0104 ||Epoch_G_Loss:0.7355\ntimer:  0.7370 sec.\n-------------\nEpoch 119/200\n-------------\n(train)\n-------------\nepoch 119 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.8858\ntimer:  0.7332 sec.\n-------------\nEpoch 120/200\n-------------\n(train)\n-------------\nepoch 120 || Epoch_D_Loss:0.0337 ||Epoch_G_Loss:0.7324\ntimer:  0.7300 sec.\n-------------\nEpoch 121/200\n-------------\n(train)\n-------------\nepoch 121 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.7945\ntimer:  0.7221 sec.\n-------------\nEpoch 122/200\n-------------\n(train)\n-------------\nepoch 122 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.8590\ntimer:  0.7202 sec.\n-------------\nEpoch 123/200\n-------------\n(train)\n-------------\nepoch 123 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8890\ntimer:  0.7228 sec.\n-------------\nEpoch 124/200\n-------------\n(train)\n-------------\nepoch 124 || Epoch_D_Loss:0.0006 ||Epoch_G_Loss:0.8837\ntimer:  0.7254 sec.\n-------------\nEpoch 125/200\n-------------\n(train)\n-------------\nepoch 125 || Epoch_D_Loss:0.0068 ||Epoch_G_Loss:0.9388\ntimer:  0.7226 sec.\n-------------\nEpoch 126/200\n-------------\n(train)\n-------------\nepoch 126 || Epoch_D_Loss:0.0567 ||Epoch_G_Loss:0.6943\ntimer:  0.7223 sec.\n-------------\nEpoch 127/200\n-------------\n(train)\n-------------\nepoch 127 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.7448\ntimer:  0.7190 sec.\n-------------\nEpoch 128/200\n-------------\n(train)\n-------------\nepoch 128 || Epoch_D_Loss:0.0019 ||Epoch_G_Loss:0.8421\ntimer:  0.7241 sec.\n-------------\nEpoch 129/200\n-------------\n(train)\n-------------\nepoch 129 || Epoch_D_Loss:0.0044 ||Epoch_G_Loss:0.9485\ntimer:  0.7198 sec.\n-------------\nEpoch 130/200\n-------------\n(train)\n-------------\nepoch 130 || Epoch_D_Loss:0.0442 ||Epoch_G_Loss:1.0138\ntimer:  0.7290 sec.\n-------------\nEpoch 131/200\n-------------\n(train)\n-------------\nepoch 131 || Epoch_D_Loss:0.0076 ||Epoch_G_Loss:0.7135\ntimer:  0.7240 sec.\n-------------\nEpoch 132/200\n-------------\n(train)\n-------------\nepoch 132 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.7633\ntimer:  0.7216 sec.\n-------------\nEpoch 133/200\n-------------\n(train)\n-------------\nepoch 133 || Epoch_D_Loss:0.0015 ||Epoch_G_Loss:0.8278\ntimer:  0.7209 sec.\n-------------\nEpoch 134/200\n-------------\n(train)\n-------------\nepoch 134 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.9835\ntimer:  0.7225 sec.\n-------------\nEpoch 135/200\n-------------\n(train)\n-------------\nepoch 135 || Epoch_D_Loss:0.0004 ||Epoch_G_Loss:0.9521\ntimer:  0.7191 sec.\n-------------\nEpoch 136/200\n-------------\n(train)\n-------------\nepoch 136 || Epoch_D_Loss:0.0004 ||Epoch_G_Loss:1.0310\ntimer:  0.7201 sec.\n-------------\nEpoch 137/200\n-------------\n(train)\n-------------\nepoch 137 || Epoch_D_Loss:0.0633 ||Epoch_G_Loss:1.0482\ntimer:  0.7257 sec.\n-------------\nEpoch 138/200\n-------------\n(train)\n-------------\nepoch 138 || Epoch_D_Loss:0.0215 ||Epoch_G_Loss:0.9015\ntimer:  0.7233 sec.\n-------------\nEpoch 139/200\n-------------\n(train)\n-------------\nepoch 139 || Epoch_D_Loss:0.0041 ||Epoch_G_Loss:0.8207\ntimer:  0.7242 sec.\n-------------\nEpoch 140/200\n-------------\n(train)\n-------------\nepoch 140 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8500\ntimer:  0.7291 sec.\n-------------\nEpoch 141/200\n-------------\n(train)\n-------------\nepoch 141 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.8946\ntimer:  0.7330 sec.\n-------------\nEpoch 142/200\n-------------\n(train)\n-------------\nepoch 142 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.8823\ntimer:  0.7250 sec.\n-------------\nEpoch 143/200\n-------------\n(train)\n-------------\nepoch 143 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:1.0055\ntimer:  0.7228 sec.\n-------------\nEpoch 144/200\n-------------\n(train)\n-------------\nepoch 144 || Epoch_D_Loss:0.0616 ||Epoch_G_Loss:0.9888\ntimer:  0.7235 sec.\n-------------\nEpoch 145/200\n-------------\n(train)\n-------------\nepoch 145 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8180\ntimer:  0.7214 sec.\n-------------\nEpoch 146/200\n-------------\n(train)\n-------------\nepoch 146 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8650\ntimer:  0.7242 sec.\n-------------\nEpoch 147/200\n-------------\n(train)\n-------------\nepoch 147 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8336\ntimer:  0.7255 sec.\n-------------\nEpoch 148/200\n-------------\n(train)\n-------------\nepoch 148 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.9114\ntimer:  0.7221 sec.\n-------------\nEpoch 149/200\n-------------\n(train)\n-------------\nepoch 149 || Epoch_D_Loss:0.0314 ||Epoch_G_Loss:0.9916\ntimer:  0.7217 sec.\n-------------\nEpoch 150/200\n-------------\n(train)\n-------------\nepoch 150 || Epoch_D_Loss:0.0159 ||Epoch_G_Loss:0.7072\ntimer:  0.7261 sec.\n-------------\nEpoch 151/200\n-------------\n(train)\n-------------\nepoch 151 || Epoch_D_Loss:0.0018 ||Epoch_G_Loss:0.8146\ntimer:  0.7238 sec.\n-------------\nEpoch 152/200\n-------------\n(train)\n-------------\nepoch 152 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8835\ntimer:  0.7259 sec.\n-------------\nEpoch 153/200\n-------------\n(train)\n-------------\nepoch 153 || Epoch_D_Loss:0.0078 ||Epoch_G_Loss:1.0363\ntimer:  0.7235 sec.\n-------------\nEpoch 154/200\n-------------\n(train)\n-------------\nepoch 154 || Epoch_D_Loss:0.0333 ||Epoch_G_Loss:0.8053\ntimer:  0.7234 sec.\n-------------\nEpoch 155/200\n-------------\n(train)\n-------------\nepoch 155 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.8146\ntimer:  0.7232 sec.\n-------------\nEpoch 156/200\n-------------\n(train)\n-------------\nepoch 156 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.8763\ntimer:  0.7245 sec.\n-------------\nEpoch 157/200\n-------------\n(train)\n-------------\nepoch 157 || Epoch_D_Loss:0.0037 ||Epoch_G_Loss:0.8991\ntimer:  0.7251 sec.\n-------------\nEpoch 158/200\n-------------\n(train)\n-------------\nepoch 158 || Epoch_D_Loss:0.0352 ||Epoch_G_Loss:0.9261\ntimer:  0.7250 sec.\n-------------\nEpoch 159/200\n-------------\n(train)\n-------------\nepoch 159 || Epoch_D_Loss:0.0104 ||Epoch_G_Loss:0.7463\ntimer:  0.7217 sec.\n-------------\nEpoch 160/200\n-------------\n(train)\n-------------\nepoch 160 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.8617\ntimer:  0.7248 sec.\n-------------\nEpoch 161/200\n-------------\n(train)\n-------------\nepoch 161 || Epoch_D_Loss:0.0021 ||Epoch_G_Loss:0.9127\ntimer:  0.7300 sec.\n-------------\nEpoch 162/200\n-------------\n(train)\n-------------\nepoch 162 || Epoch_D_Loss:0.0187 ||Epoch_G_Loss:0.8745\ntimer:  0.7322 sec.\n-------------\nEpoch 163/200\n-------------\n(train)\n-------------\nepoch 163 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.8871\ntimer:  0.7360 sec.\n-------------\nEpoch 164/200\n-------------\n(train)\n-------------\nepoch 164 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.8840\ntimer:  0.7259 sec.\n-------------\nEpoch 165/200\n-------------\n(train)\n-------------\nepoch 165 || Epoch_D_Loss:0.0290 ||Epoch_G_Loss:0.9592\ntimer:  0.7324 sec.\n-------------\nEpoch 166/200\n-------------\n(train)\n-------------\nepoch 166 || Epoch_D_Loss:0.0095 ||Epoch_G_Loss:0.8147\ntimer:  0.7241 sec.\n-------------\nEpoch 167/200\n-------------\n(train)\n-------------\nepoch 167 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8852\ntimer:  0.7227 sec.\n-------------\nEpoch 168/200\n-------------\n(train)\n-------------\nepoch 168 || Epoch_D_Loss:0.0124 ||Epoch_G_Loss:0.8975\ntimer:  0.7256 sec.\n-------------\nEpoch 169/200\n-------------\n(train)\n-------------\nepoch 169 || Epoch_D_Loss:0.0127 ||Epoch_G_Loss:0.8617\ntimer:  0.7270 sec.\n-------------\nEpoch 170/200\n-------------\n(train)\n-------------\nepoch 170 || Epoch_D_Loss:0.0043 ||Epoch_G_Loss:0.8565\ntimer:  0.7238 sec.\n-------------\nEpoch 171/200\n-------------\n(train)\n-------------\nepoch 171 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.9055\ntimer:  0.7263 sec.\n-------------\nEpoch 172/200\n-------------\n(train)\n-------------\nepoch 172 || Epoch_D_Loss:0.0300 ||Epoch_G_Loss:0.9339\ntimer:  0.7229 sec.\n-------------\nEpoch 173/200\n-------------\n(train)\n-------------\nepoch 173 || Epoch_D_Loss:0.0323 ||Epoch_G_Loss:0.7781\ntimer:  0.7257 sec.\n-------------\nEpoch 174/200\n-------------\n(train)\n-------------\nepoch 174 || Epoch_D_Loss:0.0057 ||Epoch_G_Loss:0.7429\ntimer:  0.7237 sec.\n-------------\nEpoch 175/200\n-------------\n(train)\n-------------\nepoch 175 || Epoch_D_Loss:0.0027 ||Epoch_G_Loss:0.8056\ntimer:  0.7228 sec.\n-------------\nEpoch 176/200\n-------------\n(train)\n-------------\nepoch 176 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.7961\ntimer:  0.7236 sec.\n-------------\nEpoch 177/200\n-------------\n(train)\n-------------\nepoch 177 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8577\ntimer:  0.7254 sec.\n-------------\nEpoch 178/200\n-------------\n(train)\n-------------\nepoch 178 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:1.0004\ntimer:  0.7239 sec.\n-------------\nEpoch 179/200\n-------------\n(train)\n-------------\nepoch 179 || Epoch_D_Loss:0.0479 ||Epoch_G_Loss:0.8469\ntimer:  0.7232 sec.\n-------------\nEpoch 180/200\n-------------\n(train)\n-------------\nepoch 180 || Epoch_D_Loss:0.0031 ||Epoch_G_Loss:0.6928\ntimer:  0.7236 sec.\n-------------\nEpoch 181/200\n-------------\n(train)\n-------------\nepoch 181 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.8195\ntimer:  0.7271 sec.\n-------------\nEpoch 182/200\n-------------\n(train)\n-------------\nepoch 182 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.8560\ntimer:  0.7243 sec.\n-------------\nEpoch 183/200\n-------------\n(train)\n-------------\nepoch 183 || Epoch_D_Loss:0.0191 ||Epoch_G_Loss:1.0310\ntimer:  0.7217 sec.\n-------------\nEpoch 184/200\n-------------\n(train)\n-------------\nepoch 184 || Epoch_D_Loss:0.0085 ||Epoch_G_Loss:0.7468\ntimer:  0.7303 sec.\n-------------\nEpoch 185/200\n-------------\n(train)\n-------------\nepoch 185 || Epoch_D_Loss:0.0032 ||Epoch_G_Loss:0.9024\ntimer:  0.7321 sec.\n-------------\nEpoch 186/200\n-------------\n(train)\n-------------\nepoch 186 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8672\ntimer:  0.7283 sec.\n-------------\nEpoch 187/200\n-------------\n(train)\n-------------\nepoch 187 || Epoch_D_Loss:0.0476 ||Epoch_G_Loss:0.9672\ntimer:  0.7208 sec.\n-------------\nEpoch 188/200\n-------------\n(train)\n-------------\nepoch 188 || Epoch_D_Loss:0.0077 ||Epoch_G_Loss:0.7749\ntimer:  0.7255 sec.\n-------------\nEpoch 189/200\n-------------\n(train)\n-------------\nepoch 189 || Epoch_D_Loss:0.0045 ||Epoch_G_Loss:0.8037\ntimer:  0.7221 sec.\n-------------\nEpoch 190/200\n-------------\n(train)\n-------------\nepoch 190 || Epoch_D_Loss:0.0031 ||Epoch_G_Loss:0.8678\ntimer:  0.7244 sec.\n-------------\nEpoch 191/200\n-------------\n(train)\n-------------\nepoch 191 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.9078\ntimer:  0.7223 sec.\n-------------\nEpoch 192/200\n-------------\n(train)\n-------------\nepoch 192 || Epoch_D_Loss:0.0018 ||Epoch_G_Loss:1.0057\ntimer:  0.7271 sec.\n-------------\nEpoch 193/200\n-------------\n(train)\n-------------\nepoch 193 || Epoch_D_Loss:0.0322 ||Epoch_G_Loss:0.8226\ntimer:  0.7226 sec.\n-------------\nEpoch 194/200\n-------------\n(train)\n-------------\nepoch 194 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8305\ntimer:  0.7231 sec.\n-------------\nEpoch 195/200\n-------------\n(train)\n-------------\nepoch 195 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.9072\ntimer:  0.7225 sec.\n-------------\nEpoch 196/200\n-------------\n(train)\n-------------\nepoch 196 || Epoch_D_Loss:0.0049 ||Epoch_G_Loss:1.0098\ntimer:  0.7306 sec.\n-------------\nEpoch 197/200\n-------------\n(train)\n-------------\nepoch 197 || Epoch_D_Loss:0.0216 ||Epoch_G_Loss:0.9132\ntimer:  0.7397 sec.\n-------------\nEpoch 198/200\n-------------\n(train)\n-------------\nepoch 198 || Epoch_D_Loss:0.0172 ||Epoch_G_Loss:0.8279\ntimer:  0.7386 sec.\n-------------\nEpoch 199/200\n-------------\n(train)\n-------------\nepoch 199 || Epoch_D_Loss:0.0021 ||Epoch_G_Loss:0.8379\ntimer:  0.7345 sec.\n\n\n\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# 입력 난수\nbatch_size = 8\nz_dim = 20\nfixed_z = torch.randn(batch_size, z_dim)\nfixed_z = fixed_z.view(fixed_z.size(0), fixed_z.size(1), 1, 1)\n\n# 화상 생성\nG_update.eval()\nfake_images = G_update(fixed_z.to(device))\n\n# 훈련 데이터\nbatch_iterator = iter(train_dataloader)  # 반복자로 변환\nimges = next(batch_iterator)  # 1번째 요소를 꺼낸다\n\n\n# 출력\nfig = plt.figure(figsize=(15, 6))\nfor i in range(0, 5):\n    # 상단에 훈련 데이터를,\n    plt.subplot(2, 5, i+1)\n    plt.imshow(imges[i][0].cpu().detach().numpy(), 'gray')\n\n    # 하단에 생성 데이터를 표시한다\n    plt.subplot(2, 5, 5+i+1)\n    plt.imshow(fake_images[i][0].cpu().detach().numpy(), 'gray')"
  },
  {
    "objectID": "docs/DL/posts/DCGAN.html#논문-리뷰",
    "href": "docs/DL/posts/DCGAN.html#논문-리뷰",
    "title": "DCGAN paper review",
    "section": "",
    "text": "[DCGAN 장점] - 대부분의 상황에서 안정적으로 학습이 됨 - word2vec과 같이 DCGAN으로 학습된 Generator가 벡터 산술 연산이 가능한 성질을 갖음\n - DCGAN이 학습한 필터를 시각화하여 보여줌 -&gt; 특정 필터들이 이미지의 특정 물체를 학습했음을 보여줌\n\n\n성능면에서 비지도 학습 알고리즘에 비해 우수함\n\n[APPROACH AND MODEL ARCHITECTURE]\nDCGAN은\n- Max Pooling To Strided Convolution - Fully-Connected Layer 삭제 - BatchNormalization을 추가함 -&gt; deep한 모델이더라도 gradient의 흐름이 잘 전달됨 - ReLU와 Leaky ReLU를 사용\n[DETAILS OF ADVERSARIAL TRAINING]\n\n모델 및 옵티마이저\n\n\nmini-batch Stochastic Gradient Descent(SGD) a with batch size of 128\nAll weight: zero-centered Normal distribution with std 0.02\nLeaky ReLU: slope 0.2\nOptimizer: Adam (GAN에서는 momentum 사용)\nlearning late: 0.0002(0.001은 너무 커서..)\nD’s criterion= \\(\\log(D(x))\\)(real data) + \\(\\log(1 - D(G(x)))\\)(fake data)\nG’s criterion = \\(\\log(D(G(z)))\\)\n\n데이터\n\nLSUN\nFACES\nIMAGENET-1K\n\n\nDCGAN에서 중요한 기준 - NOT MIMICKING TRAIN DATA -&gt; 단순히 학습 데이터를 모방하면 안됨! - “Walking in the latent Space” -&gt; G의 input z의 공간인 latent Space에서 z1에서 z2로 살짝 이동한다 하더라도 급작스러운 변화가 일어나지 않고 물흐르듯 부드러운 변화를 보여줘야 한다."
  },
  {
    "objectID": "docs/DL/posts/DCGAN.html#논문-구현",
    "href": "docs/DL/posts/DCGAN.html#논문-구현",
    "title": "DCGAN paper review",
    "section": "",
    "text": "import random\nimport math\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom PIL import Image\n\nimport torch\nimport torch.utils.data as data\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision.datasets as dsets\nfrom torchvision import transforms\n\ntorch.manual_seed(1234)\nnp.random.seed(1234)\nrandom.seed(1234)\n\n\n\n\ndef conv_dim(i,k,s,p):\n    '''\n    nn.Conv2d 사용할 때 사이즈 계산\n    i: input image size\n    k: kernel size\n    s: stride\n    p: padding\n    '''\n    out = (i - k + 2*p)/s + 1\n    return out  \n\n\ndef convt_dim(i,k,s,p):\n    '''\n    nn.ConvTranspose2d 사용할 때 사이즈 계산\n    i: input image size\n    k: kernel size\n    s: stride\n    p: padding\n    '''\n    out = (i-1) * s - 2 * p + k\n    return out  \n\n\n\n\n\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim = 20, image_size = 64):\n        super(Generator, self).__init__()\n\n        # layer1 -&gt; W(H) * 4\n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, image_size * 8, kernel_size = 4, stride = 1),\n            nn.BatchNorm2d(image_size * 8),\n            nn.ReLU(inplace=True))\n        \n        # layer2 -&gt; W(H) * 2\n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 8, image_size * 4, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size * 4),\n            nn.ReLU(inplace=True))\n        \n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 4, image_size * 2, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size * 2),\n            nn.ReLU(inplace=True))\n        \n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 2, image_size, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size),\n            nn.ReLU(inplace=True))\n        \n        self.last = nn.Sequential(\n            nn.ConvTranspose2d(image_size, 1, kernel_size= 4, stride=2, padding=1), # 흑백 이미지이므로 출력 차원을 1으로 지정한 것\n            nn.Tanh())\n        \n    def forward(self, z):\n        out = self.layer1(z)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        out = out.type(torch.FloatTensor)\n        return out\n\n\n\n\nG = Generator(z_dim=20, image_size=64)\ninput_z = torch.randn(1, 20)\n\n# tensor size -&gt; (1, 20, 1, 1) \n# pytorch: (batch_size, channel, height, width)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n\nfake_img = G(input_z)\n\n\n# fake_img[0][0].size() -&gt; (64, 64)\nimg_transformed = fake_img[0][0].detach().numpy()\nplt.imshow(img_transformed, 'gray')\nplt.show()\n\n\n\n\n\n\n\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, z_dim=20, image_size=64):\n        super(Discriminator, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, image_size, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(image_size, image_size*2, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(image_size*2, image_size*4, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(image_size*4, image_size*8, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.last = nn.Conv2d(image_size*8, 1, kernel_size=4, stride=1)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        out = out.type(torch.FloatTensor)\n        return out\n\n\nD = Discriminator(z_dim=20, image_size=64)\n\nd_out = D(fake_img)\n\n# 출력 d_out에 Sigmoid를 곱해 0에서 1로 변환\nprint(nn.Sigmoid()(d_out))\n\ntensor([[[[0.4980]]]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\n\n\n\nmini_batch_size = 2\n\n# 정답 라벨 생성 -&gt; torch.tensor([1,1])\nlabel_real = torch.full((mini_batch_size,), 1)\n\n# 가짜 라벨 생성 -&gt; torch.tensor([0,0])\nlabel_fake = torch.full((mini_batch_size,), 0)\n\n\n# 가짜 이미지 생성\ninput_z = torch.randn(mini_batch_size, 20)\n# tensor size -&gt; (2,20,1,1)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1) \n# G에 의해 tensor size -&gt; (2,1,64,64)\nfake_images = G(input_z)\n# D에 의해 tensor size -&gt; (2,1,1,1)\nd_out_fake = D(fake_images)\n\n\n\n\\(maximize\\) \\(log(D(x))\\)+\\(log(1 - D(G(z)))\\)\n# loss function 정의\nloss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n\n# 진짜 이미지 판정\nd_out_real = D(x)\n\n# 가짜 이미지 생성\ninput_z = torch.randn(mini_batch_size, 20)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1) \nfake_images = G(input_z)\nd_out_fake = D(fake_images)\n\n# 오차를 계산\nd_loss_real = loss_fn(d_out_real.view(-1), label_real)\nd_loss_fake = loss_fn(d_out_fake.view(-1), label_fake)\nd_loss = d_loss_real + d_loss_fake\n\n\n\n\\(maximize\\) \\(log\\)\\((D(G(z)))\\)\n# 가짜 화상을 생성해 판정\ninput_z = torch.randn(mini_batch_size, 20)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\nfake_images = G(input_z)\nd_out_fake = D(fake_images)\n\n# 오차를 계산\ng_loss = criterion(d_out_fake.view(-1), label_real)\n\n\n\n\n\n\n\nimport os\nimport urllib.request\nimport zipfile\nimport tarfile\n\n\nfrom sklearn.datasets import fetch_openml\n\nmnist_F = fetch_openml('mnist_784', version=1, data_home=\"./data/\")  \n\n\n\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', version=1, data_home=\"./data/\")  \n\n\nX = mnist.data.to_numpy()\ny = mnist.target.to_numpy()\n\n\ndata_dir_path = \"./data/img_78/\"\nif not os.path.exists(data_dir_path):\n    os.mkdir(data_dir_path)\n\n\n# MNIST에서 숫자7, 8의 화상만 \"img_78\" 폴더에 화상으로 저장해 나간다\ncount7=0\ncount8=0\nmax_num=200  # 화상은 200장씩 작성한다\n\nfor i in range(len(X)):\n    \n    # 화상7 작성\n    if (y[i] is \"7\") and (count7&lt;max_num):\n        file_path=\"./data/img_78/img_7_\"+str(count7)+\".jpg\"\n        im_f=(X[i].reshape(28, 28))  # 화상을 28×28의 형태로 변경\n        pil_img_f = Image.fromarray(im_f.astype(np.uint8))  # 화상을 PIL으로\n        pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\n        pil_img_f.save(file_path)  # 저장\n        count7+=1 \n    \n    # 화상8 작성\n    if (y[i] is \"8\") and (count8&lt;max_num):\n        file_path=\"./data/img_78/img_8_\"+str(count8)+\".jpg\"\n        im_f=(X[i].reshape(28, 28))  # 화상을 28×28의 형태로 변경\n        pil_img_f = Image.fromarray(im_f.astype(np.uint8))  # 화상을 PIL으로\n        pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\n        pil_img_f.save(file_path)  # 저장\n        count8+=1 \n\n&lt;&gt;:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n&lt;&gt;:18: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n&lt;&gt;:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n&lt;&gt;:18: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if (y[i] is \"7\") and (count7&lt;max_num):\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:18: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if (y[i] is \"8\") and (count8&lt;max_num):\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:13: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n  pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:22: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n  pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\n\n\n\ndef make_datapath_list():\n    train_img_list = list() \n    for img_idx in range(200):\n        img_path = \"./data/img_78/img_7_\" + str(img_idx)+'.jpg'\n        train_img_list.append(img_path)\n\n        img_path = \"./data/img_78/img_8_\" + str(img_idx)+'.jpg'\n        train_img_list.append(img_path)\n\n    return train_img_list\n\n\n\n\n\n\nclass ImageTransform():\n    \"\"\"이미지 전처리 클래스\"\"\"\n\n    def __init__(self, mean, std):\n        self.data_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n\n    def __call__(self, img):\n        return self.data_transform(img)\n\n\nclass GAN_Img_Dataset(data.Dataset):\n    \"\"\"Dataset 클래스. PyTorch의 Dataset 클래스를 상속\"\"\"\n\n    def __init__(self, file_list, transform):\n        self.file_list = file_list\n        self.transform = transform\n\n    def __len__(self):\n        '''이미지 개수 반환'''\n        return len(self.file_list)\n\n    def __getitem__(self, index):\n        '''전처리된 이미지를 Tensor 형식 데이터로 변환'''\n\n        img_path = self.file_list[index]\n        img = Image.open(img_path)  # [높이][폭]흑백\n\n        # 이미지 전처리\n        img_transformed = self.transform(img)\n        img_transformed = img_transformed.type(torch.FloatTensor)\n        return img_transformed\n\n\n# DataLoader 작성과 동작 확인\n\n# 파일 리스트를 작성\ntrain_img_list=make_datapath_list()\n\n# Dataset 작성\nmean = (0.5,)\nstd = (0.5,)\ntrain_dataset = GAN_Img_Dataset(file_list=train_img_list, transform=ImageTransform(mean, std))\n\n# DataLoader 작성\nbatch_size = 64\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True)\n\n# 동작 확인\nbatch_iterator = iter(train_dataloader)  # 반복자로 변환\nimges = next(batch_iterator)  # 1번째 요소를 꺼낸다\nprint(imges.size())  # torch.Size([64, 1, 64, 64])\n\ntorch.Size([64, 1, 64, 64])\n\n\n\n\n\n\n\n# 네트워크 초기화\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        # Conv2dとConvTranspose2d 초기화\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n    elif classname.find('BatchNorm') != -1:\n        # BatchNorm2d 초기화\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# 초기화 실시\nG.apply(weights_init)\nD.apply(weights_init)\n\nprint(\"네트워크 초기화 완료\")\n\n네트워크 초기화 완료\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");device\n\ndevice(type='cuda', index=0)\n\n\n\n# 모델을 학습시키는 함수를 작성\ndef train_model(G, D, dataloader, num_epochs):\n\n    # GPU가 사용 가능한지 확인\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"사용 장치: \", device)\n\n    # 최적화 기법 설정\n    g_lr, d_lr = 0.0001, 0.0004\n    beta1, beta2 = 0.0, 0.9\n    g_optimizer = torch.optim.Adam(G.parameters(), g_lr, [beta1, beta2])\n    d_optimizer = torch.optim.Adam(D.parameters(), d_lr, [beta1, beta2])\n\n    # 오차함수 정의\n    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n\n    # 파라미터를 하드코딩\n    z_dim = 20\n    mini_batch_size = 64\n\n    # 네트워크를 GPU로\n    G.to(device)\n    D.to(device)\n\n    G.train()  # 모델을 훈련 모드로\n    D.train()  # 모델을 훈련 모드로\n\n    # 네트워크가 어느 정도 고정되면, 고속화시킨다\n    torch.backends.cudnn.benchmark = True\n\n    num_train_imgs = len(dataloader.dataset)\n    batch_size = dataloader.batch_size\n\n    # 반복 카운터 설정\n    iteration = 1\n    logs = []\n\n    # epoch 루프\n    for epoch in range(num_epochs):\n\n        # 개시 시간을 저장\n        t_epoch_start = time.time()\n        epoch_g_loss = 0.0  # epoch의 손실합\n        epoch_d_loss = 0.0  # epoch의 손실합\n\n        print('-------------')\n        print('Epoch {}/{}'.format(epoch, num_epochs))\n        print('-------------')\n        print('(train)')\n\n        # 데이터 로더에서 minibatch씩 꺼내는 루프\n        for imges in dataloader:\n\n            # --------------------\n            # 1. Discriminator 학습\n            # --------------------\n            # 미니 배치 크기가 1이면, 배치 노멀라이제이션에서 에러가 발생하므로 피한다\n            if imges.size()[0] == 1:\n                continue\n\n            # GPU가 사용 가능하면 GPU로 데이터를 보낸다\n            imges = imges.to(device)\n\n            # 정답 라벨과 가짜 라벨 작성\n            # epoch의 마지막 반복은 미니 배치 수가 줄어든다\n            mini_batch_size = imges.size()[0]\n            label_real = torch.full((mini_batch_size,), 1).to(device)\n            label_fake = torch.full((mini_batch_size,), 0).to(device)\n\n            # 진짜 이미지 판정\n            d_out_real = D(imges)\n\n            # 가짜 이미지 생성해 판정\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1).to(device)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images.to(device))\n\n            # 오차를 계산\n            d_loss_real = criterion(d_out_real.view(-1).to(device), label_real.float())\n            d_loss_fake = criterion(d_out_fake.view(-1).to(device), label_fake.float())\n            d_loss = d_loss_real + d_loss_fake\n\n            # 역전파\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n\n            d_loss.backward()\n            d_optimizer.step()\n\n            # 2. Generator 학습\n            # --------------------\n            # 가짜 이미지 생성해 판정\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1).to(device)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images.to(device))\n\n            # 오차를 계산\n            g_loss = criterion(d_out_fake.view(-1).to(device), label_real.float().to(device))\n\n            # 역전파\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n            g_loss.backward()\n            g_optimizer.step()\n\n            # --------------------\n            # 3. 기록\n            # --------------------\n            epoch_d_loss += d_loss.item()\n            epoch_g_loss += g_loss.item()\n            iteration += 1\n\n        # epoch의 phase별 loss와 정답률\n        t_epoch_finish = time.time()\n        print('-------------')\n        print('epoch {} || Epoch_D_Loss:{:.4f} ||Epoch_G_Loss:{:.4f}'.format(\n            epoch, epoch_d_loss/batch_size, epoch_g_loss/batch_size))\n        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n        t_epoch_start = time.time()\n\n    return G, D\n\n\nnum_epochs = 200\nG_update, D_update = train_model(G, D, dataloader=train_dataloader, num_epochs=num_epochs)\n\n사용 장치:  cuda:0\n-------------\nEpoch 0/200\n-------------\n(train)\n-------------\nepoch 0 || Epoch_D_Loss:0.0147 ||Epoch_G_Loss:0.7212\ntimer:  0.9429 sec.\n-------------\nEpoch 1/200\n-------------\n(train)\n-------------\nepoch 1 || Epoch_D_Loss:0.0108 ||Epoch_G_Loss:0.8667\ntimer:  0.7360 sec.\n-------------\nEpoch 2/200\n-------------\n(train)\n-------------\nepoch 2 || Epoch_D_Loss:0.0266 ||Epoch_G_Loss:0.6338\ntimer:  0.7420 sec.\n-------------\nEpoch 3/200\n-------------\n(train)\n-------------\nepoch 3 || Epoch_D_Loss:0.0029 ||Epoch_G_Loss:0.7435\ntimer:  0.7325 sec.\n-------------\nEpoch 4/200\n-------------\n(train)\n-------------\nepoch 4 || Epoch_D_Loss:0.0038 ||Epoch_G_Loss:0.7857\ntimer:  0.7349 sec.\n-------------\nEpoch 5/200\n-------------\n(train)\n-------------\nepoch 5 || Epoch_D_Loss:0.0128 ||Epoch_G_Loss:0.7397\ntimer:  0.7323 sec.\n-------------\nEpoch 6/200\n-------------\n(train)\n-------------\nepoch 6 || Epoch_D_Loss:0.0058 ||Epoch_G_Loss:0.9182\ntimer:  0.7353 sec.\n-------------\nEpoch 7/200\n-------------\n(train)\n-------------\nepoch 7 || Epoch_D_Loss:0.0561 ||Epoch_G_Loss:0.6317\ntimer:  0.7301 sec.\n-------------\nEpoch 8/200\n-------------\n(train)\n-------------\nepoch 8 || Epoch_D_Loss:0.0046 ||Epoch_G_Loss:0.7038\ntimer:  0.7350 sec.\n-------------\nEpoch 9/200\n-------------\n(train)\n-------------\nepoch 9 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.7608\ntimer:  0.7320 sec.\n-------------\nEpoch 10/200\n-------------\n(train)\n-------------\nepoch 10 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.7988\ntimer:  0.7388 sec.\n-------------\nEpoch 11/200\n-------------\n(train)\n-------------\nepoch 11 || Epoch_D_Loss:0.0035 ||Epoch_G_Loss:0.8410\ntimer:  0.7402 sec.\n-------------\nEpoch 12/200\n-------------\n(train)\n-------------\nepoch 12 || Epoch_D_Loss:0.0554 ||Epoch_G_Loss:0.9072\ntimer:  0.7309 sec.\n-------------\nEpoch 13/200\n-------------\n(train)\n-------------\nepoch 13 || Epoch_D_Loss:0.0054 ||Epoch_G_Loss:0.6871\ntimer:  0.7408 sec.\n-------------\nEpoch 14/200\n-------------\n(train)\n-------------\nepoch 14 || Epoch_D_Loss:0.0027 ||Epoch_G_Loss:0.8104\ntimer:  0.7420 sec.\n-------------\nEpoch 15/200\n-------------\n(train)\n-------------\nepoch 15 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8349\ntimer:  0.7335 sec.\n-------------\nEpoch 16/200\n-------------\n(train)\n-------------\nepoch 16 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8269\ntimer:  0.7387 sec.\n-------------\nEpoch 17/200\n-------------\n(train)\n-------------\nepoch 17 || Epoch_D_Loss:0.0113 ||Epoch_G_Loss:0.9406\ntimer:  0.7346 sec.\n-------------\nEpoch 18/200\n-------------\n(train)\n-------------\nepoch 18 || Epoch_D_Loss:0.0484 ||Epoch_G_Loss:0.7943\ntimer:  0.7341 sec.\n-------------\nEpoch 19/200\n-------------\n(train)\n-------------\nepoch 19 || Epoch_D_Loss:0.0038 ||Epoch_G_Loss:0.7086\ntimer:  0.7367 sec.\n-------------\nEpoch 20/200\n-------------\n(train)\n-------------\nepoch 20 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8174\ntimer:  0.7461 sec.\n-------------\nEpoch 21/200\n-------------\n(train)\n-------------\nepoch 21 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.9420\ntimer:  0.7323 sec.\n-------------\nEpoch 22/200\n-------------\n(train)\n-------------\nepoch 22 || Epoch_D_Loss:0.0253 ||Epoch_G_Loss:0.7721\ntimer:  0.7278 sec.\n-------------\nEpoch 23/200\n-------------\n(train)\n-------------\nepoch 23 || Epoch_D_Loss:0.0093 ||Epoch_G_Loss:0.7429\ntimer:  0.7256 sec.\n-------------\nEpoch 24/200\n-------------\n(train)\n-------------\nepoch 24 || Epoch_D_Loss:0.0123 ||Epoch_G_Loss:0.8130\ntimer:  0.7300 sec.\n-------------\nEpoch 25/200\n-------------\n(train)\n-------------\nepoch 25 || Epoch_D_Loss:0.0103 ||Epoch_G_Loss:0.8479\ntimer:  0.7365 sec.\n-------------\nEpoch 26/200\n-------------\n(train)\n-------------\nepoch 26 || Epoch_D_Loss:0.0015 ||Epoch_G_Loss:0.8154\ntimer:  0.7331 sec.\n-------------\nEpoch 27/200\n-------------\n(train)\n-------------\nepoch 27 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8887\ntimer:  0.7366 sec.\n-------------\nEpoch 28/200\n-------------\n(train)\n-------------\nepoch 28 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:1.0104\ntimer:  0.7301 sec.\n-------------\nEpoch 29/200\n-------------\n(train)\n-------------\nepoch 29 || Epoch_D_Loss:0.0665 ||Epoch_G_Loss:0.6415\ntimer:  0.7291 sec.\n-------------\nEpoch 30/200\n-------------\n(train)\n-------------\nepoch 30 || Epoch_D_Loss:0.0060 ||Epoch_G_Loss:0.7702\ntimer:  0.7330 sec.\n-------------\nEpoch 31/200\n-------------\n(train)\n-------------\nepoch 31 || Epoch_D_Loss:0.0018 ||Epoch_G_Loss:0.8099\ntimer:  0.7369 sec.\n-------------\nEpoch 32/200\n-------------\n(train)\n-------------\nepoch 32 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8387\ntimer:  0.7320 sec.\n-------------\nEpoch 33/200\n-------------\n(train)\n-------------\nepoch 33 || Epoch_D_Loss:0.0116 ||Epoch_G_Loss:0.9780\ntimer:  0.7371 sec.\n-------------\nEpoch 34/200\n-------------\n(train)\n-------------\nepoch 34 || Epoch_D_Loss:0.0895 ||Epoch_G_Loss:0.6259\ntimer:  0.7390 sec.\n-------------\nEpoch 35/200\n-------------\n(train)\n-------------\nepoch 35 || Epoch_D_Loss:0.0040 ||Epoch_G_Loss:0.7130\ntimer:  0.7300 sec.\n-------------\nEpoch 36/200\n-------------\n(train)\n-------------\nepoch 36 || Epoch_D_Loss:0.0033 ||Epoch_G_Loss:0.7479\ntimer:  0.7291 sec.\n-------------\nEpoch 37/200\n-------------\n(train)\n-------------\nepoch 37 || Epoch_D_Loss:0.0019 ||Epoch_G_Loss:0.7853\ntimer:  0.7266 sec.\n-------------\nEpoch 38/200\n-------------\n(train)\n-------------\nepoch 38 || Epoch_D_Loss:0.0034 ||Epoch_G_Loss:0.7967\ntimer:  0.7288 sec.\n-------------\nEpoch 39/200\n-------------\n(train)\n-------------\nepoch 39 || Epoch_D_Loss:0.0376 ||Epoch_G_Loss:0.9572\ntimer:  0.7312 sec.\n-------------\nEpoch 40/200\n-------------\n(train)\n-------------\nepoch 40 || Epoch_D_Loss:0.0112 ||Epoch_G_Loss:0.7412\ntimer:  0.7272 sec.\n-------------\nEpoch 41/200\n-------------\n(train)\n-------------\nepoch 41 || Epoch_D_Loss:0.0022 ||Epoch_G_Loss:0.8254\ntimer:  0.7251 sec.\n-------------\nEpoch 42/200\n-------------\n(train)\n-------------\nepoch 42 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8413\ntimer:  0.7251 sec.\n-------------\nEpoch 43/200\n-------------\n(train)\n-------------\nepoch 43 || Epoch_D_Loss:0.0007 ||Epoch_G_Loss:0.8888\ntimer:  0.7370 sec.\n-------------\nEpoch 44/200\n-------------\n(train)\n-------------\nepoch 44 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.9397\ntimer:  0.7321 sec.\n-------------\nEpoch 45/200\n-------------\n(train)\n-------------\nepoch 45 || Epoch_D_Loss:0.0589 ||Epoch_G_Loss:0.7371\ntimer:  0.7245 sec.\n-------------\nEpoch 46/200\n-------------\n(train)\n-------------\nepoch 46 || Epoch_D_Loss:0.0032 ||Epoch_G_Loss:0.7395\ntimer:  0.7276 sec.\n-------------\nEpoch 47/200\n-------------\n(train)\n-------------\nepoch 47 || Epoch_D_Loss:0.0037 ||Epoch_G_Loss:0.8705\ntimer:  0.7277 sec.\n-------------\nEpoch 48/200\n-------------\n(train)\n-------------\nepoch 48 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.8553\ntimer:  0.7288 sec.\n-------------\nEpoch 49/200\n-------------\n(train)\n-------------\nepoch 49 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.8959\ntimer:  0.7374 sec.\n-------------\nEpoch 50/200\n-------------\n(train)\n-------------\nepoch 50 || Epoch_D_Loss:0.0429 ||Epoch_G_Loss:0.9181\ntimer:  0.7258 sec.\n-------------\nEpoch 51/200\n-------------\n(train)\n-------------\nepoch 51 || Epoch_D_Loss:0.0030 ||Epoch_G_Loss:0.7838\ntimer:  0.7300 sec.\n-------------\nEpoch 52/200\n-------------\n(train)\n-------------\nepoch 52 || Epoch_D_Loss:0.0032 ||Epoch_G_Loss:0.8647\ntimer:  0.7330 sec.\n-------------\nEpoch 53/200\n-------------\n(train)\n-------------\nepoch 53 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.8791\ntimer:  0.7306 sec.\n-------------\nEpoch 54/200\n-------------\n(train)\n-------------\nepoch 54 || Epoch_D_Loss:0.0362 ||Epoch_G_Loss:0.9293\ntimer:  0.7291 sec.\n-------------\nEpoch 55/200\n-------------\n(train)\n-------------\nepoch 55 || Epoch_D_Loss:0.0123 ||Epoch_G_Loss:0.7459\ntimer:  0.7313 sec.\n-------------\nEpoch 56/200\n-------------\n(train)\n-------------\nepoch 56 || Epoch_D_Loss:0.0033 ||Epoch_G_Loss:0.8016\ntimer:  0.7408 sec.\n-------------\nEpoch 57/200\n-------------\n(train)\n-------------\nepoch 57 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8647\ntimer:  0.7294 sec.\n-------------\nEpoch 58/200\n-------------\n(train)\n-------------\nepoch 58 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8626\ntimer:  0.7275 sec.\n-------------\nEpoch 59/200\n-------------\n(train)\n-------------\nepoch 59 || Epoch_D_Loss:0.0625 ||Epoch_G_Loss:0.9511\ntimer:  0.7284 sec.\n-------------\nEpoch 60/200\n-------------\n(train)\n-------------\nepoch 60 || Epoch_D_Loss:0.0097 ||Epoch_G_Loss:0.7070\ntimer:  0.7263 sec.\n-------------\nEpoch 61/200\n-------------\n(train)\n-------------\nepoch 61 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.7738\ntimer:  0.7272 sec.\n-------------\nEpoch 62/200\n-------------\n(train)\n-------------\nepoch 62 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.7819\ntimer:  0.7280 sec.\n-------------\nEpoch 63/200\n-------------\n(train)\n-------------\nepoch 63 || Epoch_D_Loss:0.0015 ||Epoch_G_Loss:0.8568\ntimer:  0.7297 sec.\n-------------\nEpoch 64/200\n-------------\n(train)\n-------------\nepoch 64 || Epoch_D_Loss:0.0375 ||Epoch_G_Loss:0.8728\ntimer:  0.7267 sec.\n-------------\nEpoch 65/200\n-------------\n(train)\n-------------\nepoch 65 || Epoch_D_Loss:0.0063 ||Epoch_G_Loss:0.6889\ntimer:  0.7381 sec.\n-------------\nEpoch 66/200\n-------------\n(train)\n-------------\nepoch 66 || Epoch_D_Loss:0.0038 ||Epoch_G_Loss:0.8209\ntimer:  0.7297 sec.\n-------------\nEpoch 67/200\n-------------\n(train)\n-------------\nepoch 67 || Epoch_D_Loss:0.0022 ||Epoch_G_Loss:0.8709\ntimer:  0.7559 sec.\n-------------\nEpoch 68/200\n-------------\n(train)\n-------------\nepoch 68 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.8871\ntimer:  0.7298 sec.\n-------------\nEpoch 69/200\n-------------\n(train)\n-------------\nepoch 69 || Epoch_D_Loss:0.0414 ||Epoch_G_Loss:0.9295\ntimer:  0.7275 sec.\n-------------\nEpoch 70/200\n-------------\n(train)\n-------------\nepoch 70 || Epoch_D_Loss:0.0088 ||Epoch_G_Loss:0.7318\ntimer:  0.7226 sec.\n-------------\nEpoch 71/200\n-------------\n(train)\n-------------\nepoch 71 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8082\ntimer:  0.7209 sec.\n-------------\nEpoch 72/200\n-------------\n(train)\n-------------\nepoch 72 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.8387\ntimer:  0.7196 sec.\n-------------\nEpoch 73/200\n-------------\n(train)\n-------------\nepoch 73 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.9041\ntimer:  0.7261 sec.\n-------------\nEpoch 74/200\n-------------\n(train)\n-------------\nepoch 74 || Epoch_D_Loss:0.0522 ||Epoch_G_Loss:0.9280\ntimer:  0.7300 sec.\n-------------\nEpoch 75/200\n-------------\n(train)\n-------------\nepoch 75 || Epoch_D_Loss:0.0056 ||Epoch_G_Loss:0.6952\ntimer:  0.7258 sec.\n-------------\nEpoch 76/200\n-------------\n(train)\n-------------\nepoch 76 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.8027\ntimer:  0.7219 sec.\n-------------\nEpoch 77/200\n-------------\n(train)\n-------------\nepoch 77 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.8502\ntimer:  0.7210 sec.\n-------------\nEpoch 78/200\n-------------\n(train)\n-------------\nepoch 78 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.8654\ntimer:  0.7208 sec.\n-------------\nEpoch 79/200\n-------------\n(train)\n-------------\nepoch 79 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.9063\ntimer:  0.7214 sec.\n-------------\nEpoch 80/200\n-------------\n(train)\n-------------\nepoch 80 || Epoch_D_Loss:0.0458 ||Epoch_G_Loss:0.9730\ntimer:  0.7253 sec.\n-------------\nEpoch 81/200\n-------------\n(train)\n-------------\nepoch 81 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.7860\ntimer:  0.7200 sec.\n-------------\nEpoch 82/200\n-------------\n(train)\n-------------\nepoch 82 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8159\ntimer:  0.7201 sec.\n-------------\nEpoch 83/200\n-------------\n(train)\n-------------\nepoch 83 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.8737\ntimer:  0.7207 sec.\n-------------\nEpoch 84/200\n-------------\n(train)\n-------------\nepoch 84 || Epoch_D_Loss:0.0444 ||Epoch_G_Loss:0.8085\ntimer:  0.7164 sec.\n-------------\nEpoch 85/200\n-------------\n(train)\n-------------\nepoch 85 || Epoch_D_Loss:0.0100 ||Epoch_G_Loss:0.7245\ntimer:  0.7192 sec.\n-------------\nEpoch 86/200\n-------------\n(train)\n-------------\nepoch 86 || Epoch_D_Loss:0.0019 ||Epoch_G_Loss:0.7648\ntimer:  0.7202 sec.\n-------------\nEpoch 87/200\n-------------\n(train)\n-------------\nepoch 87 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8417\ntimer:  0.7172 sec.\n-------------\nEpoch 88/200\n-------------\n(train)\n-------------\nepoch 88 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8620\ntimer:  0.7171 sec.\n-------------\nEpoch 89/200\n-------------\n(train)\n-------------\nepoch 89 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.9353\ntimer:  0.7189 sec.\n-------------\nEpoch 90/200\n-------------\n(train)\n-------------\nepoch 90 || Epoch_D_Loss:0.0538 ||Epoch_G_Loss:0.9575\ntimer:  0.7199 sec.\n-------------\nEpoch 91/200\n-------------\n(train)\n-------------\nepoch 91 || Epoch_D_Loss:0.0036 ||Epoch_G_Loss:0.7752\ntimer:  0.7231 sec.\n-------------\nEpoch 92/200\n-------------\n(train)\n-------------\nepoch 92 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.8001\ntimer:  0.7212 sec.\n-------------\nEpoch 93/200\n-------------\n(train)\n-------------\nepoch 93 || Epoch_D_Loss:0.0029 ||Epoch_G_Loss:0.9208\ntimer:  0.7198 sec.\n-------------\nEpoch 94/200\n-------------\n(train)\n-------------\nepoch 94 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.9002\ntimer:  0.7233 sec.\n-------------\nEpoch 95/200\n-------------\n(train)\n-------------\nepoch 95 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.8902\ntimer:  0.7227 sec.\n-------------\nEpoch 96/200\n-------------\n(train)\n-------------\nepoch 96 || Epoch_D_Loss:0.0467 ||Epoch_G_Loss:0.8899\ntimer:  0.7380 sec.\n-------------\nEpoch 97/200\n-------------\n(train)\n-------------\nepoch 97 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.7825\ntimer:  0.7333 sec.\n-------------\nEpoch 98/200\n-------------\n(train)\n-------------\nepoch 98 || Epoch_D_Loss:0.0037 ||Epoch_G_Loss:0.8136\ntimer:  0.7191 sec.\n-------------\nEpoch 99/200\n-------------\n(train)\n-------------\nepoch 99 || Epoch_D_Loss:0.0096 ||Epoch_G_Loss:0.9419\ntimer:  0.7208 sec.\n-------------\nEpoch 100/200\n-------------\n(train)\n-------------\nepoch 100 || Epoch_D_Loss:0.0014 ||Epoch_G_Loss:0.8698\ntimer:  0.7468 sec.\n-------------\nEpoch 101/200\n-------------\n(train)\n-------------\nepoch 101 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.9751\ntimer:  0.7229 sec.\n-------------\nEpoch 102/200\n-------------\n(train)\n-------------\nepoch 102 || Epoch_D_Loss:0.0005 ||Epoch_G_Loss:0.9452\ntimer:  0.7198 sec.\n-------------\nEpoch 103/200\n-------------\n(train)\n-------------\nepoch 103 || Epoch_D_Loss:0.0007 ||Epoch_G_Loss:0.9833\ntimer:  0.7279 sec.\n-------------\nEpoch 104/200\n-------------\n(train)\n-------------\nepoch 104 || Epoch_D_Loss:0.1555 ||Epoch_G_Loss:1.0839\ntimer:  0.7222 sec.\n-------------\nEpoch 105/200\n-------------\n(train)\n-------------\nepoch 105 || Epoch_D_Loss:0.0103 ||Epoch_G_Loss:0.6298\ntimer:  0.7233 sec.\n-------------\nEpoch 106/200\n-------------\n(train)\n-------------\nepoch 106 || Epoch_D_Loss:0.0061 ||Epoch_G_Loss:0.7351\ntimer:  0.7223 sec.\n-------------\nEpoch 107/200\n-------------\n(train)\n-------------\nepoch 107 || Epoch_D_Loss:0.0036 ||Epoch_G_Loss:0.7819\ntimer:  0.7340 sec.\n-------------\nEpoch 108/200\n-------------\n(train)\n-------------\nepoch 108 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.8024\ntimer:  0.7279 sec.\n-------------\nEpoch 109/200\n-------------\n(train)\n-------------\nepoch 109 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.9204\ntimer:  0.7284 sec.\n-------------\nEpoch 110/200\n-------------\n(train)\n-------------\nepoch 110 || Epoch_D_Loss:0.0069 ||Epoch_G_Loss:1.1190\ntimer:  0.7254 sec.\n-------------\nEpoch 111/200\n-------------\n(train)\n-------------\nepoch 111 || Epoch_D_Loss:0.0006 ||Epoch_G_Loss:1.0324\ntimer:  0.7267 sec.\n-------------\nEpoch 112/200\n-------------\n(train)\n-------------\nepoch 112 || Epoch_D_Loss:0.0014 ||Epoch_G_Loss:0.9086\ntimer:  0.7268 sec.\n-------------\nEpoch 113/200\n-------------\n(train)\n-------------\nepoch 113 || Epoch_D_Loss:0.0014 ||Epoch_G_Loss:0.9236\ntimer:  0.7270 sec.\n-------------\nEpoch 114/200\n-------------\n(train)\n-------------\nepoch 114 || Epoch_D_Loss:0.0561 ||Epoch_G_Loss:0.9581\ntimer:  0.7281 sec.\n-------------\nEpoch 115/200\n-------------\n(train)\n-------------\nepoch 115 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.7752\ntimer:  0.7249 sec.\n-------------\nEpoch 116/200\n-------------\n(train)\n-------------\nepoch 116 || Epoch_D_Loss:0.0021 ||Epoch_G_Loss:0.8565\ntimer:  0.7257 sec.\n-------------\nEpoch 117/200\n-------------\n(train)\n-------------\nepoch 117 || Epoch_D_Loss:0.0317 ||Epoch_G_Loss:0.8358\ntimer:  0.7263 sec.\n-------------\nEpoch 118/200\n-------------\n(train)\n-------------\nepoch 118 || Epoch_D_Loss:0.0104 ||Epoch_G_Loss:0.7355\ntimer:  0.7370 sec.\n-------------\nEpoch 119/200\n-------------\n(train)\n-------------\nepoch 119 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.8858\ntimer:  0.7332 sec.\n-------------\nEpoch 120/200\n-------------\n(train)\n-------------\nepoch 120 || Epoch_D_Loss:0.0337 ||Epoch_G_Loss:0.7324\ntimer:  0.7300 sec.\n-------------\nEpoch 121/200\n-------------\n(train)\n-------------\nepoch 121 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.7945\ntimer:  0.7221 sec.\n-------------\nEpoch 122/200\n-------------\n(train)\n-------------\nepoch 122 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.8590\ntimer:  0.7202 sec.\n-------------\nEpoch 123/200\n-------------\n(train)\n-------------\nepoch 123 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8890\ntimer:  0.7228 sec.\n-------------\nEpoch 124/200\n-------------\n(train)\n-------------\nepoch 124 || Epoch_D_Loss:0.0006 ||Epoch_G_Loss:0.8837\ntimer:  0.7254 sec.\n-------------\nEpoch 125/200\n-------------\n(train)\n-------------\nepoch 125 || Epoch_D_Loss:0.0068 ||Epoch_G_Loss:0.9388\ntimer:  0.7226 sec.\n-------------\nEpoch 126/200\n-------------\n(train)\n-------------\nepoch 126 || Epoch_D_Loss:0.0567 ||Epoch_G_Loss:0.6943\ntimer:  0.7223 sec.\n-------------\nEpoch 127/200\n-------------\n(train)\n-------------\nepoch 127 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.7448\ntimer:  0.7190 sec.\n-------------\nEpoch 128/200\n-------------\n(train)\n-------------\nepoch 128 || Epoch_D_Loss:0.0019 ||Epoch_G_Loss:0.8421\ntimer:  0.7241 sec.\n-------------\nEpoch 129/200\n-------------\n(train)\n-------------\nepoch 129 || Epoch_D_Loss:0.0044 ||Epoch_G_Loss:0.9485\ntimer:  0.7198 sec.\n-------------\nEpoch 130/200\n-------------\n(train)\n-------------\nepoch 130 || Epoch_D_Loss:0.0442 ||Epoch_G_Loss:1.0138\ntimer:  0.7290 sec.\n-------------\nEpoch 131/200\n-------------\n(train)\n-------------\nepoch 131 || Epoch_D_Loss:0.0076 ||Epoch_G_Loss:0.7135\ntimer:  0.7240 sec.\n-------------\nEpoch 132/200\n-------------\n(train)\n-------------\nepoch 132 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.7633\ntimer:  0.7216 sec.\n-------------\nEpoch 133/200\n-------------\n(train)\n-------------\nepoch 133 || Epoch_D_Loss:0.0015 ||Epoch_G_Loss:0.8278\ntimer:  0.7209 sec.\n-------------\nEpoch 134/200\n-------------\n(train)\n-------------\nepoch 134 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.9835\ntimer:  0.7225 sec.\n-------------\nEpoch 135/200\n-------------\n(train)\n-------------\nepoch 135 || Epoch_D_Loss:0.0004 ||Epoch_G_Loss:0.9521\ntimer:  0.7191 sec.\n-------------\nEpoch 136/200\n-------------\n(train)\n-------------\nepoch 136 || Epoch_D_Loss:0.0004 ||Epoch_G_Loss:1.0310\ntimer:  0.7201 sec.\n-------------\nEpoch 137/200\n-------------\n(train)\n-------------\nepoch 137 || Epoch_D_Loss:0.0633 ||Epoch_G_Loss:1.0482\ntimer:  0.7257 sec.\n-------------\nEpoch 138/200\n-------------\n(train)\n-------------\nepoch 138 || Epoch_D_Loss:0.0215 ||Epoch_G_Loss:0.9015\ntimer:  0.7233 sec.\n-------------\nEpoch 139/200\n-------------\n(train)\n-------------\nepoch 139 || Epoch_D_Loss:0.0041 ||Epoch_G_Loss:0.8207\ntimer:  0.7242 sec.\n-------------\nEpoch 140/200\n-------------\n(train)\n-------------\nepoch 140 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8500\ntimer:  0.7291 sec.\n-------------\nEpoch 141/200\n-------------\n(train)\n-------------\nepoch 141 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.8946\ntimer:  0.7330 sec.\n-------------\nEpoch 142/200\n-------------\n(train)\n-------------\nepoch 142 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.8823\ntimer:  0.7250 sec.\n-------------\nEpoch 143/200\n-------------\n(train)\n-------------\nepoch 143 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:1.0055\ntimer:  0.7228 sec.\n-------------\nEpoch 144/200\n-------------\n(train)\n-------------\nepoch 144 || Epoch_D_Loss:0.0616 ||Epoch_G_Loss:0.9888\ntimer:  0.7235 sec.\n-------------\nEpoch 145/200\n-------------\n(train)\n-------------\nepoch 145 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8180\ntimer:  0.7214 sec.\n-------------\nEpoch 146/200\n-------------\n(train)\n-------------\nepoch 146 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8650\ntimer:  0.7242 sec.\n-------------\nEpoch 147/200\n-------------\n(train)\n-------------\nepoch 147 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8336\ntimer:  0.7255 sec.\n-------------\nEpoch 148/200\n-------------\n(train)\n-------------\nepoch 148 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.9114\ntimer:  0.7221 sec.\n-------------\nEpoch 149/200\n-------------\n(train)\n-------------\nepoch 149 || Epoch_D_Loss:0.0314 ||Epoch_G_Loss:0.9916\ntimer:  0.7217 sec.\n-------------\nEpoch 150/200\n-------------\n(train)\n-------------\nepoch 150 || Epoch_D_Loss:0.0159 ||Epoch_G_Loss:0.7072\ntimer:  0.7261 sec.\n-------------\nEpoch 151/200\n-------------\n(train)\n-------------\nepoch 151 || Epoch_D_Loss:0.0018 ||Epoch_G_Loss:0.8146\ntimer:  0.7238 sec.\n-------------\nEpoch 152/200\n-------------\n(train)\n-------------\nepoch 152 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8835\ntimer:  0.7259 sec.\n-------------\nEpoch 153/200\n-------------\n(train)\n-------------\nepoch 153 || Epoch_D_Loss:0.0078 ||Epoch_G_Loss:1.0363\ntimer:  0.7235 sec.\n-------------\nEpoch 154/200\n-------------\n(train)\n-------------\nepoch 154 || Epoch_D_Loss:0.0333 ||Epoch_G_Loss:0.8053\ntimer:  0.7234 sec.\n-------------\nEpoch 155/200\n-------------\n(train)\n-------------\nepoch 155 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.8146\ntimer:  0.7232 sec.\n-------------\nEpoch 156/200\n-------------\n(train)\n-------------\nepoch 156 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.8763\ntimer:  0.7245 sec.\n-------------\nEpoch 157/200\n-------------\n(train)\n-------------\nepoch 157 || Epoch_D_Loss:0.0037 ||Epoch_G_Loss:0.8991\ntimer:  0.7251 sec.\n-------------\nEpoch 158/200\n-------------\n(train)\n-------------\nepoch 158 || Epoch_D_Loss:0.0352 ||Epoch_G_Loss:0.9261\ntimer:  0.7250 sec.\n-------------\nEpoch 159/200\n-------------\n(train)\n-------------\nepoch 159 || Epoch_D_Loss:0.0104 ||Epoch_G_Loss:0.7463\ntimer:  0.7217 sec.\n-------------\nEpoch 160/200\n-------------\n(train)\n-------------\nepoch 160 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.8617\ntimer:  0.7248 sec.\n-------------\nEpoch 161/200\n-------------\n(train)\n-------------\nepoch 161 || Epoch_D_Loss:0.0021 ||Epoch_G_Loss:0.9127\ntimer:  0.7300 sec.\n-------------\nEpoch 162/200\n-------------\n(train)\n-------------\nepoch 162 || Epoch_D_Loss:0.0187 ||Epoch_G_Loss:0.8745\ntimer:  0.7322 sec.\n-------------\nEpoch 163/200\n-------------\n(train)\n-------------\nepoch 163 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.8871\ntimer:  0.7360 sec.\n-------------\nEpoch 164/200\n-------------\n(train)\n-------------\nepoch 164 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.8840\ntimer:  0.7259 sec.\n-------------\nEpoch 165/200\n-------------\n(train)\n-------------\nepoch 165 || Epoch_D_Loss:0.0290 ||Epoch_G_Loss:0.9592\ntimer:  0.7324 sec.\n-------------\nEpoch 166/200\n-------------\n(train)\n-------------\nepoch 166 || Epoch_D_Loss:0.0095 ||Epoch_G_Loss:0.8147\ntimer:  0.7241 sec.\n-------------\nEpoch 167/200\n-------------\n(train)\n-------------\nepoch 167 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8852\ntimer:  0.7227 sec.\n-------------\nEpoch 168/200\n-------------\n(train)\n-------------\nepoch 168 || Epoch_D_Loss:0.0124 ||Epoch_G_Loss:0.8975\ntimer:  0.7256 sec.\n-------------\nEpoch 169/200\n-------------\n(train)\n-------------\nepoch 169 || Epoch_D_Loss:0.0127 ||Epoch_G_Loss:0.8617\ntimer:  0.7270 sec.\n-------------\nEpoch 170/200\n-------------\n(train)\n-------------\nepoch 170 || Epoch_D_Loss:0.0043 ||Epoch_G_Loss:0.8565\ntimer:  0.7238 sec.\n-------------\nEpoch 171/200\n-------------\n(train)\n-------------\nepoch 171 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.9055\ntimer:  0.7263 sec.\n-------------\nEpoch 172/200\n-------------\n(train)\n-------------\nepoch 172 || Epoch_D_Loss:0.0300 ||Epoch_G_Loss:0.9339\ntimer:  0.7229 sec.\n-------------\nEpoch 173/200\n-------------\n(train)\n-------------\nepoch 173 || Epoch_D_Loss:0.0323 ||Epoch_G_Loss:0.7781\ntimer:  0.7257 sec.\n-------------\nEpoch 174/200\n-------------\n(train)\n-------------\nepoch 174 || Epoch_D_Loss:0.0057 ||Epoch_G_Loss:0.7429\ntimer:  0.7237 sec.\n-------------\nEpoch 175/200\n-------------\n(train)\n-------------\nepoch 175 || Epoch_D_Loss:0.0027 ||Epoch_G_Loss:0.8056\ntimer:  0.7228 sec.\n-------------\nEpoch 176/200\n-------------\n(train)\n-------------\nepoch 176 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.7961\ntimer:  0.7236 sec.\n-------------\nEpoch 177/200\n-------------\n(train)\n-------------\nepoch 177 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8577\ntimer:  0.7254 sec.\n-------------\nEpoch 178/200\n-------------\n(train)\n-------------\nepoch 178 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:1.0004\ntimer:  0.7239 sec.\n-------------\nEpoch 179/200\n-------------\n(train)\n-------------\nepoch 179 || Epoch_D_Loss:0.0479 ||Epoch_G_Loss:0.8469\ntimer:  0.7232 sec.\n-------------\nEpoch 180/200\n-------------\n(train)\n-------------\nepoch 180 || Epoch_D_Loss:0.0031 ||Epoch_G_Loss:0.6928\ntimer:  0.7236 sec.\n-------------\nEpoch 181/200\n-------------\n(train)\n-------------\nepoch 181 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.8195\ntimer:  0.7271 sec.\n-------------\nEpoch 182/200\n-------------\n(train)\n-------------\nepoch 182 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.8560\ntimer:  0.7243 sec.\n-------------\nEpoch 183/200\n-------------\n(train)\n-------------\nepoch 183 || Epoch_D_Loss:0.0191 ||Epoch_G_Loss:1.0310\ntimer:  0.7217 sec.\n-------------\nEpoch 184/200\n-------------\n(train)\n-------------\nepoch 184 || Epoch_D_Loss:0.0085 ||Epoch_G_Loss:0.7468\ntimer:  0.7303 sec.\n-------------\nEpoch 185/200\n-------------\n(train)\n-------------\nepoch 185 || Epoch_D_Loss:0.0032 ||Epoch_G_Loss:0.9024\ntimer:  0.7321 sec.\n-------------\nEpoch 186/200\n-------------\n(train)\n-------------\nepoch 186 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8672\ntimer:  0.7283 sec.\n-------------\nEpoch 187/200\n-------------\n(train)\n-------------\nepoch 187 || Epoch_D_Loss:0.0476 ||Epoch_G_Loss:0.9672\ntimer:  0.7208 sec.\n-------------\nEpoch 188/200\n-------------\n(train)\n-------------\nepoch 188 || Epoch_D_Loss:0.0077 ||Epoch_G_Loss:0.7749\ntimer:  0.7255 sec.\n-------------\nEpoch 189/200\n-------------\n(train)\n-------------\nepoch 189 || Epoch_D_Loss:0.0045 ||Epoch_G_Loss:0.8037\ntimer:  0.7221 sec.\n-------------\nEpoch 190/200\n-------------\n(train)\n-------------\nepoch 190 || Epoch_D_Loss:0.0031 ||Epoch_G_Loss:0.8678\ntimer:  0.7244 sec.\n-------------\nEpoch 191/200\n-------------\n(train)\n-------------\nepoch 191 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.9078\ntimer:  0.7223 sec.\n-------------\nEpoch 192/200\n-------------\n(train)\n-------------\nepoch 192 || Epoch_D_Loss:0.0018 ||Epoch_G_Loss:1.0057\ntimer:  0.7271 sec.\n-------------\nEpoch 193/200\n-------------\n(train)\n-------------\nepoch 193 || Epoch_D_Loss:0.0322 ||Epoch_G_Loss:0.8226\ntimer:  0.7226 sec.\n-------------\nEpoch 194/200\n-------------\n(train)\n-------------\nepoch 194 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8305\ntimer:  0.7231 sec.\n-------------\nEpoch 195/200\n-------------\n(train)\n-------------\nepoch 195 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.9072\ntimer:  0.7225 sec.\n-------------\nEpoch 196/200\n-------------\n(train)\n-------------\nepoch 196 || Epoch_D_Loss:0.0049 ||Epoch_G_Loss:1.0098\ntimer:  0.7306 sec.\n-------------\nEpoch 197/200\n-------------\n(train)\n-------------\nepoch 197 || Epoch_D_Loss:0.0216 ||Epoch_G_Loss:0.9132\ntimer:  0.7397 sec.\n-------------\nEpoch 198/200\n-------------\n(train)\n-------------\nepoch 198 || Epoch_D_Loss:0.0172 ||Epoch_G_Loss:0.8279\ntimer:  0.7386 sec.\n-------------\nEpoch 199/200\n-------------\n(train)\n-------------\nepoch 199 || Epoch_D_Loss:0.0021 ||Epoch_G_Loss:0.8379\ntimer:  0.7345 sec.\n\n\n\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# 입력 난수\nbatch_size = 8\nz_dim = 20\nfixed_z = torch.randn(batch_size, z_dim)\nfixed_z = fixed_z.view(fixed_z.size(0), fixed_z.size(1), 1, 1)\n\n# 화상 생성\nG_update.eval()\nfake_images = G_update(fixed_z.to(device))\n\n# 훈련 데이터\nbatch_iterator = iter(train_dataloader)  # 반복자로 변환\nimges = next(batch_iterator)  # 1번째 요소를 꺼낸다\n\n\n# 출력\nfig = plt.figure(figsize=(15, 6))\nfor i in range(0, 5):\n    # 상단에 훈련 데이터를,\n    plt.subplot(2, 5, i+1)\n    plt.imshow(imges[i][0].cpu().detach().numpy(), 'gray')\n\n    # 하단에 생성 데이터를 표시한다\n    plt.subplot(2, 5, 5+i+1)\n    plt.imshow(fake_images[i][0].cpu().detach().numpy(), 'gray')"
  }
]