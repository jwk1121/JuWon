[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JuWon Kwon",
    "section": "",
    "text": "바이오메디컬공학을 전공하고 있는 학부생 입니다.\n초음파와 딥러닝을 공부하고 있습니다."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "JuWon Kwon",
    "section": "",
    "text": "바이오메디컬공학을 전공하고 있는 학부생 입니다. 초음파와 딥러닝을 공부하고 있습니다."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "rstat101.html",
    "href": "rstat101.html",
    "title": "Tutorial",
    "section": "",
    "text": "test12414입니\n123123다"
  },
  {
    "objectID": "tutorial_1.html",
    "href": "tutorial_1.html",
    "title": "test123",
    "section": "",
    "text": "안녕하세요 권주원입니다\nqmd test 진행중입니다"
  },
  {
    "objectID": "docs/rstat/rstat101.html",
    "href": "docs/rstat/rstat101.html",
    "title": "Tutorial",
    "section": "",
    "text": "test12414입니\n123123다"
  },
  {
    "objectID": "docs/rstat/rstat_lecture.html",
    "href": "docs/rstat/rstat_lecture.html",
    "title": "rstat lecture",
    "section": "",
    "text": "1강\n2강\n3강"
  },
  {
    "objectID": "docs/rstat/index.html",
    "href": "docs/rstat/index.html",
    "title": "Quarto blog",
    "section": "",
    "text": "CNN\n\n\nCNN part1 test\n\n\n\n\nDL\n\n\nCNN\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nTutorial\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/rstat/posts/rstat101.html",
    "href": "docs/rstat/posts/rstat101.html",
    "title": "Tutorial",
    "section": "",
    "text": "test12414입니\n123123다"
  },
  {
    "objectID": "docs/rstat/posts/rstat_lecture.html",
    "href": "docs/rstat/posts/rstat_lecture.html",
    "title": "rstat lecture",
    "section": "",
    "text": "1강\n2강\n3강"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html",
    "href": "docs/rstat/posts/DL_CNN.html",
    "title": "CNN",
    "section": "",
    "text": "toc:true\n\n\nimport torch \nimport torchvision\nfrom fastai.vision.all import * \nimport time\n\n\n\n간략하게 CNN의 구조를 설명하자면 - conv layer: 커널층(선형변환처럼 feature를 늘려주는 역할) - ReLU layer: 비선형을 추가해 표현력을 늘려주는 역할 - pooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n# (굳이 (2,2) 이런식으로 안해도 됨 어차피 윈도우 사이즈는 정사각행렬이므로 상수하나만 입력해도 됨됨)\n\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv.weight.data = torch.tensor([[[[0.25, 0.25],[0.25,0.25]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]), tensor([0.]))\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임) - 해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음 - 윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함 - 첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용 - 두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n\n\n\n\n사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.\n\\(ReLU(x) = \\max(0,x)\\)\n\n\n\n\n\npooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\nmaxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_maxpooling(_X) \n\ntensor([[[ 6.,  8.],\n         [16., 18.]]])\n\n\n\nmaxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\npooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n\n이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다\n\n\n\n\n\n\n\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02&lt;00:00]\n    \n    \n\n\n\n\n\n(path/'원하는 경로').ls()\n\n\n\ntorchvision.io.read_image()\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/1').ls()])\nx_tr = torch.concat([x0, x1])/255\n\ny_tr = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/1').ls()])\nx_test = torch.concat([x0, x1])/255\n\ny_test = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\nx_tr.shape, y_tr.shape, x_test.shape, y_test.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([2115, 1]))\n\n\n\n\n\n# conv\nc1 = torch.nn.Conv2d(1, 16 , 5) # 만약 color image였다면 입력 채널의 수를 3으로 지정해야 함\nx_tr.shape ,c1(x_tr).shape\n\n(torch.Size([12665, 1, 28, 28]), torch.Size([12665, 16, 24, 24]))\n\n\n\nsize계산 공식: 윈도우(커널)사이즈가 n이면 \\(size = height(width) - ( n - 1)\\)\n\n\n# ReLU\na1 = torch.nn.ReLU()\n\n\n# maxpooling\nm1 = torch.nn.MaxPool2d(2)\nprint(m1(a1(c1(x_tr))).shape)\n\ntorch.Size([12665, 16, 12, 12])\n\n\n\n행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n\n# flatten(이미지를 한줄로 펼치는 것)\nf1 = torch.nn.Flatten()\nprint(f1(a1(m1(c1(x_tr)))).shape) #16 * 12 * 12 = 2304\n\ntorch.Size([12665, 2304])\n\n\n\n# sigmoid에 올리기 위해서는 2304 디멘젼을 1로 만들어야함\nl1=torch.nn.Linear(in_features=2304,out_features=1) \n\n\n# sigmoid (값이 0에서 1사이의 값, 즉 확률로 출력되도록)\na2 = torch.nn.Sigmoid()\nprint(a2(f1(a1(m1(c1(x_tr))))).shape)\n\ntorch.Size([12665, 2304])\n\n\n\nprint('이미지 사이즈:                     ', x_tr.shape)\nprint('conv:                              ',c1(x_tr).shape)\nprint('(ReLU) -&gt; maxpooling:              ',m1(a1(c1(x_tr))).shape)\nprint('이미지 펼치기:                     ',f1(a1(m1(c1(x_tr)))).shape)\nprint('이미지를 하나의 스칼라로 선형변환: ',l1(f1(a1(m1(c1(x_tr))))).shape)\nprint('시그모이드:                        ',a2(l1(f1(a1(m1(c1(x_tr)))))).shape)\n\n이미지 사이즈:                      torch.Size([12665, 1, 28, 28])\nconv:                               torch.Size([12665, 16, 24, 24])\n(ReLU) -&gt; maxpooling:               torch.Size([12665, 16, 12, 12])\n이미지 펼치기:                      torch.Size([12665, 2304])\n이미지를 하나의 스칼라로 선형변환:  torch.Size([12665, 1])\n시그모이드:                         torch.Size([12665, 1])\n\n\n\n\n\n\n원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\\(\\to\\) overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n# 데이터 로더에 들어갈 데이터세트 준비\nds_tr = torch.utils.data.TensorDataset(x_tr, y_tr)\nds_test = torch.utils.data.TensorDataset(x_test, y_test)\n\n\n데이터 로더는 배치크기를 지정해줘야 함\n\n\nlen(x_tr), len(x_test)\n\n(12665, 2115)\n\n\n\n데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n\nstochastic gradient descent(구용어: mini-batch gradient descent)\n\n\n\n# 데이터 로더\ndl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=1266) \ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=2115) \n\n\ndls = DataLoaders(dl_tr,dl_test)\n\n\nnet = torch.nn.Sequential(\n    # conv layer \n    c1, \n    # ReLU\n    a1, \n    # maxpool\n    m1, \n    # flatten\n    f1, \n    # linear transform (n -&gt; 1)\n    l1,\n    # Sigmoid\n    a2\n)\n\nloss_fn=torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn) # fastai의 Learner는 오미타이저의 기본값이 adam이므로 따로 지정해주지 않아도 됨\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.979273\n0.639250\n00:05\n\n\n1\n0.703008\n0.402466\n00:00\n\n\n2\n0.547401\n0.256881\n00:00\n\n\n3\n0.434025\n0.142217\n00:00\n\n\n4\n0.340996\n0.079636\n00:00\n\n\n5\n0.267902\n0.048050\n00:00\n\n\n6\n0.211895\n0.031742\n00:00\n\n\n7\n0.169176\n0.022921\n00:00\n\n\n8\n0.136331\n0.017658\n00:00\n\n\n9\n0.110770\n0.014233\n00:00\n\n\n\n\n\n\nLearner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음 &gt; python    net[0].weight 위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = ’cuda:0’라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다.\n+ tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n\nnet = net.to('cpu')\nplt.plot(net(x_tr).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(x_test).data,'.')\nplt.title(\"Testing Set\",size=15)\n\nText(0.5, 1.0, 'Testing Set')\n\n\n\n\n\n\n\n\n\n\n\n\nBCEWithLogitsLoss = Sigmoid + BCELoss\n손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음\n\n\n\n\n\n\n다중(k개) 클래스를 분류 - LossFunction: CrossEntrophyLoss\n- ActivationFunction: SoftmaxFunction - 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n\n소프트맥스 함수가 계산하는 과정은 아래와 같음 (3개를 분류할 경우) \\(softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}\\)\n\n\n\n\n\nk개의 클래스를 분류하는 모델의 Loss 계산 방법\n\nsftmax(_netout) # -&gt; 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -&gt; 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -&gt; 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n+ 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임\n+ 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n\n\n\ntype 1) int형을 사용하는 방법 (vector)\ntype 2) float형을 사용하는 방법 (one-hot encoded vector)\n만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n\ntype 1)의 경우 &lt;사슴: 0, 강아지: 1, 고양이: 2&gt; (단, 데이터 형태는는 int(정수))\ntype 2)의 경우 &lt;사슴: [1, 0, 0], 강아지: [0, 1, 0], 고양이: [0, 0, 1] &gt; (단, 데이터의 형태는 float(실수))\n\n\n\n\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n\n\ntorch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n\n\n\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\ntorch.nn.functional.one_hot(yy).float()\n\ntensor([[1., 0.],\n        [1., 0.],\n        [1., 0.],\n        ...,\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])\n\n\n\n\n\n\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\nlen(X) # 18623\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번= 1862 꽉 채워서 10번하고 3개정도 남은 걸로 한 번\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # test는 전부다 넣어서\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\n\nloss_fn = torch.nn.CrossEntropyLoss() # 여기에는 softmax함수가 내장되어 있음 \n#-&gt; net마지막에 softmax를 넣으면 안됨\n# BCEWithLogitsLoss 느낌(시그모이드 + BCE)\n\n\nlrnr = Learner(dls,net,loss_fn) # 옵티마이저는 아답이 디폴트 값이어서 굳이 안넣어도 됨\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.797101\n1.103916\n00:00\n\n\n1\n1.267011\n0.823797\n00:00\n\n\n2\n1.055513\n0.667210\n00:00\n\n\n3\n0.910746\n0.435414\n00:00\n\n\n4\n0.762887\n0.284001\n00:00\n\n\n5\n0.625515\n0.199502\n00:00\n\n\n6\n0.515352\n0.152906\n00:00\n\n\n7\n0.429145\n0.123678\n00:00\n\n\n8\n0.359694\n0.105466\n00:00\n\n\n9\n0.303888\n0.092883\n00:00\n\n\n\n\n\n\nlrnr.model.to(\"cpu\")\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n3142\n-1.138366\n-5.435792\n0.370670\n2\n\n\n3143\n-4.458741\n-4.281343\n2.052410\n2\n\n\n3144\n-2.836508\n-3.204013\n0.012610\n2\n\n\n3145\n-1.704158\n-10.621873\n2.024313\n2\n\n\n3146\n-2.467648\n-4.612999\n0.656284\n2\n\n\n\n\n\n3147 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n975\n0.432969\n-5.653580\n-1.944451\n0\n\n\n976\n2.685695\n-10.254354\n-2.466680\n0\n\n\n977\n2.474842\n-9.650204\n-2.452746\n0\n\n\n978\n1.268743\n-6.928779\n-1.419695\n0\n\n\n979\n4.371464\n-8.959077\n-4.056257\n0\n\n\n\n\n\n980 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼\n\n\n\n\n\n이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n\n\n\n\nfastai에서 지원\nfastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\ny의 형태를 주의해서 사용해야 함\n\n앞서 말한 것처럼 두 가지 타입이 있음\n\nvector + int\none-hot encoded vector + float\n\n\n\ntype 1) y의 형태가 vector + int일 때 - metrics = accuracy를 사용해야 함\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\ny.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\n\n\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nyy.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n1.128365\n0.601474\n0.463357\n0.536643\n00:00\n\n\n1\n0.684630\n0.304262\n0.975414\n0.024586\n00:00\n\n\n2\n0.503124\n0.144147\n0.989598\n0.010402\n00:00\n\n\n3\n0.373899\n0.068306\n0.996217\n0.003783\n00:00\n\n\n4\n0.281332\n0.040790\n0.996217\n0.003783\n00:00\n\n\n5\n0.215743\n0.026980\n0.996690\n0.003310\n00:00\n\n\n6\n0.168349\n0.019467\n0.996690\n0.003310\n00:00\n\n\n7\n0.133313\n0.014856\n0.998109\n0.001891\n00:00\n\n\n8\n0.106776\n0.011745\n0.998109\n0.001891\n00:00\n\n\n9\n0.086275\n0.009553\n0.999054\n0.000946\n00:00\n\n\n\n\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때 - metrics = accuracy_multi를 사용해야 함 - error_rate는 사용못함\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n\n\ny_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\nyy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\ntorch.nn.functional.one_hot() 함수 조건\n\n기본적으로 크기가 n인 벡터가 들어오길 기대\n정수가 들어오는 것을 기대\n\n하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n1.213650\n0.647844\n0.463357\n00:00\n\n\n1\n0.744869\n0.384000\n0.933570\n00:00\n\n\n2\n0.571879\n0.187119\n0.986525\n00:00\n\n\n3\n0.432096\n0.090841\n0.994563\n00:00\n\n\n4\n0.326718\n0.048868\n0.995508\n00:00\n\n\n5\n0.250235\n0.028477\n0.996217\n00:00\n\n\n6\n0.194605\n0.018616\n0.996454\n00:00\n\n\n7\n0.153338\n0.013278\n0.996927\n00:00\n\n\n8\n0.122056\n0.010130\n0.997872\n00:00\n\n\n9\n0.097957\n0.008112\n0.998109\n00:00"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#cnn-구조",
    "href": "docs/rstat/posts/DL_CNN.html#cnn-구조",
    "title": "CNN",
    "section": "",
    "text": "간략하게 CNN의 구조를 설명하자면 - conv layer: 커널층(선형변환처럼 feature를 늘려주는 역할) - ReLU layer: 비선형을 추가해 표현력을 늘려주는 역할 - pooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n# (굳이 (2,2) 이런식으로 안해도 됨 어차피 윈도우 사이즈는 정사각행렬이므로 상수하나만 입력해도 됨됨)\n\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv.weight.data = torch.tensor([[[[0.25, 0.25],[0.25,0.25]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]), tensor([0.]))\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임) - 해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음 - 윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함 - 첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용 - 두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n\n\n\n\n사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.\n\\(ReLU(x) = \\max(0,x)\\)\n\n\n\n\n\npooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\nmaxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_maxpooling(_X) \n\ntensor([[[ 6.,  8.],\n         [16., 18.]]])\n\n\n\nmaxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\npooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n\n이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#cnn-구현",
    "href": "docs/rstat/posts/DL_CNN.html#cnn-구현",
    "title": "CNN",
    "section": "",
    "text": "path = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02&lt;00:00]"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#path-사용법-데이터-준비",
    "href": "docs/rstat/posts/DL_CNN.html#path-사용법-데이터-준비",
    "title": "CNN",
    "section": "",
    "text": "(path/'원하는 경로').ls()"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#위-코드를-사용하면-폴더-안에-있는-데이터들의-이름을-출력",
    "href": "docs/rstat/posts/DL_CNN.html#위-코드를-사용하면-폴더-안에-있는-데이터들의-이름을-출력",
    "title": "CNN",
    "section": "",
    "text": "torchvision.io.read_image()\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/1').ls()])\nx_tr = torch.concat([x0, x1])/255\n\ny_tr = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/1').ls()])\nx_test = torch.concat([x0, x1])/255\n\ny_test = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\nx_tr.shape, y_tr.shape, x_test.shape, y_test.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([2115, 1]))\n\n\n\n\n\n# conv\nc1 = torch.nn.Conv2d(1, 16 , 5) # 만약 color image였다면 입력 채널의 수를 3으로 지정해야 함\nx_tr.shape ,c1(x_tr).shape\n\n(torch.Size([12665, 1, 28, 28]), torch.Size([12665, 16, 24, 24]))\n\n\n\nsize계산 공식: 윈도우(커널)사이즈가 n이면 \\(size = height(width) - ( n - 1)\\)\n\n\n# ReLU\na1 = torch.nn.ReLU()\n\n\n# maxpooling\nm1 = torch.nn.MaxPool2d(2)\nprint(m1(a1(c1(x_tr))).shape)\n\ntorch.Size([12665, 16, 12, 12])\n\n\n\n행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n\n# flatten(이미지를 한줄로 펼치는 것)\nf1 = torch.nn.Flatten()\nprint(f1(a1(m1(c1(x_tr)))).shape) #16 * 12 * 12 = 2304\n\ntorch.Size([12665, 2304])\n\n\n\n# sigmoid에 올리기 위해서는 2304 디멘젼을 1로 만들어야함\nl1=torch.nn.Linear(in_features=2304,out_features=1) \n\n\n# sigmoid (값이 0에서 1사이의 값, 즉 확률로 출력되도록)\na2 = torch.nn.Sigmoid()\nprint(a2(f1(a1(m1(c1(x_tr))))).shape)\n\ntorch.Size([12665, 2304])\n\n\n\nprint('이미지 사이즈:                     ', x_tr.shape)\nprint('conv:                              ',c1(x_tr).shape)\nprint('(ReLU) -&gt; maxpooling:              ',m1(a1(c1(x_tr))).shape)\nprint('이미지 펼치기:                     ',f1(a1(m1(c1(x_tr)))).shape)\nprint('이미지를 하나의 스칼라로 선형변환: ',l1(f1(a1(m1(c1(x_tr))))).shape)\nprint('시그모이드:                        ',a2(l1(f1(a1(m1(c1(x_tr)))))).shape)\n\n이미지 사이즈:                      torch.Size([12665, 1, 28, 28])\nconv:                               torch.Size([12665, 16, 24, 24])\n(ReLU) -&gt; maxpooling:               torch.Size([12665, 16, 12, 12])\n이미지 펼치기:                      torch.Size([12665, 2304])\n이미지를 하나의 스칼라로 선형변환:  torch.Size([12665, 1])\n시그모이드:                         torch.Size([12665, 1])\n\n\n\n\n\n\n원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\\(\\to\\) overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n# 데이터 로더에 들어갈 데이터세트 준비\nds_tr = torch.utils.data.TensorDataset(x_tr, y_tr)\nds_test = torch.utils.data.TensorDataset(x_test, y_test)\n\n\n데이터 로더는 배치크기를 지정해줘야 함\n\n\nlen(x_tr), len(x_test)\n\n(12665, 2115)\n\n\n\n데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n\nstochastic gradient descent(구용어: mini-batch gradient descent)\n\n\n\n# 데이터 로더\ndl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=1266) \ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=2115) \n\n\ndls = DataLoaders(dl_tr,dl_test)\n\n\nnet = torch.nn.Sequential(\n    # conv layer \n    c1, \n    # ReLU\n    a1, \n    # maxpool\n    m1, \n    # flatten\n    f1, \n    # linear transform (n -&gt; 1)\n    l1,\n    # Sigmoid\n    a2\n)\n\nloss_fn=torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn) # fastai의 Learner는 오미타이저의 기본값이 adam이므로 따로 지정해주지 않아도 됨\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.979273\n0.639250\n00:05\n\n\n1\n0.703008\n0.402466\n00:00\n\n\n2\n0.547401\n0.256881\n00:00\n\n\n3\n0.434025\n0.142217\n00:00\n\n\n4\n0.340996\n0.079636\n00:00\n\n\n5\n0.267902\n0.048050\n00:00\n\n\n6\n0.211895\n0.031742\n00:00\n\n\n7\n0.169176\n0.022921\n00:00\n\n\n8\n0.136331\n0.017658\n00:00\n\n\n9\n0.110770\n0.014233\n00:00\n\n\n\n\n\n\nLearner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음 &gt; python    net[0].weight 위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = ’cuda:0’라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다.\n+ tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n\nnet = net.to('cpu')\nplt.plot(net(x_tr).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(x_test).data,'.')\nplt.title(\"Testing Set\",size=15)\n\nText(0.5, 1.0, 'Testing Set')"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#loss-function",
    "href": "docs/rstat/posts/DL_CNN.html#loss-function",
    "title": "CNN",
    "section": "",
    "text": "BCEWithLogitsLoss = Sigmoid + BCELoss\n손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#k개의-클래스를-분류하는-모델",
    "href": "docs/rstat/posts/DL_CNN.html#k개의-클래스를-분류하는-모델",
    "title": "CNN",
    "section": "",
    "text": "다중(k개) 클래스를 분류 - LossFunction: CrossEntrophyLoss\n- ActivationFunction: SoftmaxFunction - 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n\n소프트맥스 함수가 계산하는 과정은 아래와 같음 (3개를 분류할 경우) \\(softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}\\)\n\n\n\n\n\nk개의 클래스를 분류하는 모델의 Loss 계산 방법\n\nsftmax(_netout) # -&gt; 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -&gt; 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -&gt; 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n+ 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임\n+ 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n\n\n\ntype 1) int형을 사용하는 방법 (vector)\ntype 2) float형을 사용하는 방법 (one-hot encoded vector)\n만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n\ntype 1)의 경우 &lt;사슴: 0, 강아지: 1, 고양이: 2&gt; (단, 데이터 형태는는 int(정수))\ntype 2)의 경우 &lt;사슴: [1, 0, 0], 강아지: [0, 1, 0], 고양이: [0, 0, 1] &gt; (단, 데이터의 형태는 float(실수))\n\n\n\n\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n\n\ntorch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n\n\n\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\ntorch.nn.functional.one_hot(yy).float()\n\ntensor([[1., 0.],\n        [1., 0.],\n        [1., 0.],\n        ...,\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])\n\n\n\n\n\n\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\nlen(X) # 18623\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번= 1862 꽉 채워서 10번하고 3개정도 남은 걸로 한 번\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # test는 전부다 넣어서\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\n\nloss_fn = torch.nn.CrossEntropyLoss() # 여기에는 softmax함수가 내장되어 있음 \n#-&gt; net마지막에 softmax를 넣으면 안됨\n# BCEWithLogitsLoss 느낌(시그모이드 + BCE)\n\n\nlrnr = Learner(dls,net,loss_fn) # 옵티마이저는 아답이 디폴트 값이어서 굳이 안넣어도 됨\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.797101\n1.103916\n00:00\n\n\n1\n1.267011\n0.823797\n00:00\n\n\n2\n1.055513\n0.667210\n00:00\n\n\n3\n0.910746\n0.435414\n00:00\n\n\n4\n0.762887\n0.284001\n00:00\n\n\n5\n0.625515\n0.199502\n00:00\n\n\n6\n0.515352\n0.152906\n00:00\n\n\n7\n0.429145\n0.123678\n00:00\n\n\n8\n0.359694\n0.105466\n00:00\n\n\n9\n0.303888\n0.092883\n00:00\n\n\n\n\n\n\nlrnr.model.to(\"cpu\")\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n3142\n-1.138366\n-5.435792\n0.370670\n2\n\n\n3143\n-4.458741\n-4.281343\n2.052410\n2\n\n\n3144\n-2.836508\n-3.204013\n0.012610\n2\n\n\n3145\n-1.704158\n-10.621873\n2.024313\n2\n\n\n3146\n-2.467648\n-4.612999\n0.656284\n2\n\n\n\n\n\n3147 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n975\n0.432969\n-5.653580\n-1.944451\n0\n\n\n976\n2.685695\n-10.254354\n-2.466680\n0\n\n\n977\n2.474842\n-9.650204\n-2.452746\n0\n\n\n978\n1.268743\n-6.928779\n-1.419695\n0\n\n\n979\n4.371464\n-8.959077\n-4.056257\n0\n\n\n\n\n\n980 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼\n\n\n\n\n\n이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n\n\n\n\nfastai에서 지원\nfastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\ny의 형태를 주의해서 사용해야 함\n\n앞서 말한 것처럼 두 가지 타입이 있음\n\nvector + int\none-hot encoded vector + float\n\n\n\ntype 1) y의 형태가 vector + int일 때 - metrics = accuracy를 사용해야 함\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\ny.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\n\n\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nyy.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n1.128365\n0.601474\n0.463357\n0.536643\n00:00\n\n\n1\n0.684630\n0.304262\n0.975414\n0.024586\n00:00\n\n\n2\n0.503124\n0.144147\n0.989598\n0.010402\n00:00\n\n\n3\n0.373899\n0.068306\n0.996217\n0.003783\n00:00\n\n\n4\n0.281332\n0.040790\n0.996217\n0.003783\n00:00\n\n\n5\n0.215743\n0.026980\n0.996690\n0.003310\n00:00\n\n\n6\n0.168349\n0.019467\n0.996690\n0.003310\n00:00\n\n\n7\n0.133313\n0.014856\n0.998109\n0.001891\n00:00\n\n\n8\n0.106776\n0.011745\n0.998109\n0.001891\n00:00\n\n\n9\n0.086275\n0.009553\n0.999054\n0.000946\n00:00\n\n\n\n\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때 - metrics = accuracy_multi를 사용해야 함 - error_rate는 사용못함\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n\n\ny_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\nyy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\ntorch.nn.functional.one_hot() 함수 조건\n\n기본적으로 크기가 n인 벡터가 들어오길 기대\n정수가 들어오는 것을 기대\n\n하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n1.213650\n0.647844\n0.463357\n00:00\n\n\n1\n0.744869\n0.384000\n0.933570\n00:00\n\n\n2\n0.571879\n0.187119\n0.986525\n00:00\n\n\n3\n0.432096\n0.090841\n0.994563\n00:00\n\n\n4\n0.326718\n0.048868\n0.995508\n00:00\n\n\n5\n0.250235\n0.028477\n0.996217\n00:00\n\n\n6\n0.194605\n0.018616\n0.996454\n00:00\n\n\n7\n0.153338\n0.013278\n0.996927\n00:00\n\n\n8\n0.122056\n0.010130\n0.997872\n00:00\n\n\n9\n0.097957\n0.008112\n0.998109\n00:00"
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "JuWon Kwon",
    "section": "",
    "text": "This is the contents of the about page for my blog."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "JuWon Kwon",
    "section": "",
    "text": "바이오메디컬공학을 전공하고 있는 학부생 입니다. 초음파와 딥러닝을 공부하고 있습니다."
  },
  {
    "objectID": "docs/DL/index.html",
    "href": "docs/DL/index.html",
    "title": "딥러닝 정리 페이지",
    "section": "",
    "text": "Response-based Knowledge Practice\n\n\nLarge Scale Model → Small Scale Model with Response-based Knowledge\n\n\n\n\nDL\n\n\nKnowledge Distillation\n\n\npaper review\n\n\n\n\n\n\n\n\n\n\n\nJul 4, 2023\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nKnowledge Distillation paper review part2\n\n\nLarge Scale Model → Small Scale Model [Distillation Scheme & Teacher–Student Architecture]\n\n\n\n\nDL\n\n\nKnowledge Distillation\n\n\npaper review\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nKnowledge Distillation paper review part1\n\n\nLarge Scale Model → Small Scale Model [Knowledge]\n\n\n\n\nDL\n\n\nKnowledge Distillation\n\n\npaper review\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nDCGAN paper review\n\n\nDeep generative model - DCGAN\n\n\n\n\nDL\n\n\nGenerative model\n\n\npaper review\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nU-net paper review\n\n\nsegmantation model - Unet\n\n\n\n\nDL\n\n\nsegmantation\n\n\npaper review\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nFCN paper review\n\n\nsegmantation\n\n\n\n\nDL\n\n\nsegmantation\n\n\npaper review\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nCNN part2\n\n\nCNN part2\n\n\n\n\nDL\n\n\nCNN\n\n\nXAI\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nCNN part1\n\n\nCNN part1\n\n\n\n\nDL\n\n\nCNN\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nRNN part3\n\n\nRNN part3\n\n\n\n\nDL\n\n\nRNN\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nRNN part2\n\n\nRNN part2\n\n\n\n\nDL\n\n\nRNN\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nRNN part1\n\n\nRNN part1\n\n\n\n\nDL\n\n\nRNN\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nANN\n\n\nANN\n\n\n\n\nDL\n\n\nANN\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nRegression part2\n\n\nRegression part2\n\n\n\n\nDL\n\n\nRegression\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nRegression part1\n\n\nRegression part1\n\n\n\n\nDL\n\n\nRegression\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2022\n\n\nJuWon\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html",
    "href": "docs/DL/posts/DL_CNN.html",
    "title": "CNN part1",
    "section": "",
    "text": "toc:true\n\n\nimport torch \nimport torchvision\nfrom fastai.vision.all import * \nimport time\n\n\n\n간략하게 CNN의 구조를 설명하자면\n\nconv layer: 커널층(선형변환처럼 feature를 늘려주는 역할)\n\nReLU layer: 비선형을 추가해 표현력을 늘려주는 역할\n\npooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n\n\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n# (굳이 (2,2) 이런식으로 안해도 됨 어차피 윈도우 사이즈는 정사각행렬이므로 상수하나만 입력해도 됨됨)\n\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv.weight.data = torch.tensor([[[[0.25, 0.25],[0.25,0.25]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]), tensor([0.]))\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임)\n\n해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음\n\n윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함\n\n첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용\n\n두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n\n\n\n\n\n\n사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.\n\\(ReLU(x) = \\max(0,x)\\)\n\n\n\n\n\npooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\nmaxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_maxpooling(_X) \n\ntensor([[[ 6.,  8.],\n         [16., 18.]]])\n\n\n\nmaxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\npooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n\n이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다\n\n\n\n\n\n\n\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02&lt;00:00]\n    \n    \n\n\n\n\n(path/'원하는 경로').ls()\n위 코드를 사용하면 폴더 안에 있는 데이터들의 이름을 출력\ntorchvision.io.read_image()\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/1').ls()])\nx_tr = torch.concat([x0, x1])/255\n\ny_tr = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/1').ls()])\nx_test = torch.concat([x0, x1])/255\n\ny_test = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\nx_tr.shape, y_tr.shape, x_test.shape, y_test.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([2115, 1]))\n\n\n\n\n\n\n# conv\nc1 = torch.nn.Conv2d(1, 16 , 5) # 만약 color image였다면 입력 채널의 수를 3으로 지정해야 함\nx_tr.shape ,c1(x_tr).shape\n\n(torch.Size([12665, 1, 28, 28]), torch.Size([12665, 16, 24, 24]))\n\n\n\nsize계산 공식: 윈도우(커널)사이즈가 n이면 \\(size = height(width) - ( n - 1)\\)\n\n\n# ReLU\na1 = torch.nn.ReLU()\n\n\n# maxpooling\nm1 = torch.nn.MaxPool2d(2)\nprint(m1(a1(c1(x_tr))).shape)\n\ntorch.Size([12665, 16, 12, 12])\n\n\n\n행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n\n# flatten(이미지를 한줄로 펼치는 것)\nf1 = torch.nn.Flatten()\nprint(f1(a1(m1(c1(x_tr)))).shape) #16 * 12 * 12 = 2304\n\ntorch.Size([12665, 2304])\n\n\n\n# sigmoid에 올리기 위해서는 2304 디멘젼을 1로 만들어야함\nl1=torch.nn.Linear(in_features=2304,out_features=1) \n\n\n# sigmoid (값이 0에서 1사이의 값, 즉 확률로 출력되도록)\na2 = torch.nn.Sigmoid()\nprint(a2(f1(a1(m1(c1(x_tr))))).shape)\n\ntorch.Size([12665, 2304])\n\n\n\nprint('이미지 사이즈:                     ', x_tr.shape)\nprint('conv:                              ',c1(x_tr).shape)\nprint('(ReLU) -&gt; maxpooling:              ',m1(a1(c1(x_tr))).shape)\nprint('이미지 펼치기:                     ',f1(a1(m1(c1(x_tr)))).shape)\nprint('이미지를 하나의 스칼라로 선형변환: ',l1(f1(a1(m1(c1(x_tr))))).shape)\nprint('시그모이드:                        ',a2(l1(f1(a1(m1(c1(x_tr)))))).shape)\n\n이미지 사이즈:                      torch.Size([12665, 1, 28, 28])\nconv:                               torch.Size([12665, 16, 24, 24])\n(ReLU) -&gt; maxpooling:               torch.Size([12665, 16, 12, 12])\n이미지 펼치기:                      torch.Size([12665, 2304])\n이미지를 하나의 스칼라로 선형변환:  torch.Size([12665, 1])\n시그모이드:                         torch.Size([12665, 1])\n\n\n\n\n\n\n원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\\(\\to\\) overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n# 데이터 로더에 들어갈 데이터세트 준비\nds_tr = torch.utils.data.TensorDataset(x_tr, y_tr)\nds_test = torch.utils.data.TensorDataset(x_test, y_test)\n\n\n데이터 로더는 배치크기를 지정해줘야 함\n\n\nlen(x_tr), len(x_test)\n\n(12665, 2115)\n\n\n\n데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n\nstochastic gradient descent(구용어: mini-batch gradient descent)\n\n\n\n# 데이터 로더\ndl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=1266) \ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=2115) \n\n\ndls = DataLoaders(dl_tr,dl_test)\n\n\nnet = torch.nn.Sequential(\n    # conv layer \n    c1, \n    # ReLU\n    a1, \n    # maxpool\n    m1, \n    # flatten\n    f1, \n    # linear transform (n -&gt; 1)\n    l1,\n    # Sigmoid\n    a2\n)\n\nloss_fn=torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn) # fastai의 Learner는 오미타이저의 기본값이 adam이므로 따로 지정해주지 않아도 됨\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.979273\n0.639250\n00:05\n\n\n1\n0.703008\n0.402466\n00:00\n\n\n2\n0.547401\n0.256881\n00:00\n\n\n3\n0.434025\n0.142217\n00:00\n\n\n4\n0.340996\n0.079636\n00:00\n\n\n5\n0.267902\n0.048050\n00:00\n\n\n6\n0.211895\n0.031742\n00:00\n\n\n7\n0.169176\n0.022921\n00:00\n\n\n8\n0.136331\n0.017658\n00:00\n\n\n9\n0.110770\n0.014233\n00:00\n\n\n\n\n\n\nLearner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음 &gt; python    net[0].weight 위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = ’cuda:0’라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다.\n+ tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n\nnet = net.to('cpu')\nplt.plot(net(x_tr).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(x_test).data,'.')\nplt.title(\"Testing Set\",size=15)\n\nText(0.5, 1.0, 'Testing Set')\n\n\n\n\n\n\n\n\n\n\n\n\nBCEWithLogitsLoss = Sigmoid + BCELoss\n손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음\n\n\n\n\n\n다중(k개) 클래스를 분류\n- LossFunction: CrossEntrophyLoss\n- ActivationFunction: SoftmaxFunction\n- 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n소프트맥스 함수가 계산하는 과정은 아래와 같음\n\\(softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}\\) (3개를 분류할 경우)\n\n\n\n\n\nk개의 클래스를 분류하는 모델의 Loss 계산 방법\n\nsftmax(_netout) # -&gt; 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -&gt; 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n이를 수식으로 정리하면 다음과 같음\nCrossEntropy = \\(- \\log (Softmax(x)) * Ground Truth_{onehot}(y)\\)\n만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -&gt; 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n+ 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임\n+ 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n\n\n\ntype 1) int형을 사용하는 방법 (vector)\ntype 2) float형을 사용하는 방법 (one-hot encoded vector)\n만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n\ntype 1)의 경우 &lt;사슴: 0, 강아지: 1, 고양이: 2&gt; (단, 데이터 형태는는 int(정수))\ntype 2)의 경우 &lt;사슴: [1, 0, 0], 강아지: [0, 1, 0], 고양이: [0, 0, 1] &gt; (단, 데이터의 형태는 float(실수))\n\n\n\n\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n\n\ntorch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n\n\n\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\ntorch.nn.functional.one_hot(yy).float()\n\ntensor([[1., 0.],\n        [1., 0.],\n        [1., 0.],\n        ...,\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])\n\n\n\n\n\n\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\nlen(X) # 18623\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번= 1862 꽉 채워서 10번하고 3개정도 남은 걸로 한 번\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # test는 전부다 넣어서\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\n\nloss_fn = torch.nn.CrossEntropyLoss() # 여기에는 softmax함수가 내장되어 있음 \n#-&gt; net마지막에 softmax를 넣으면 안됨\n# BCEWithLogitsLoss 느낌(시그모이드 + BCE)\n\n\nlrnr = Learner(dls,net,loss_fn) # 옵티마이저는 아답이 디폴트 값이어서 굳이 안넣어도 됨\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.797101\n1.103916\n00:00\n\n\n1\n1.267011\n0.823797\n00:00\n\n\n2\n1.055513\n0.667210\n00:00\n\n\n3\n0.910746\n0.435414\n00:00\n\n\n4\n0.762887\n0.284001\n00:00\n\n\n5\n0.625515\n0.199502\n00:00\n\n\n6\n0.515352\n0.152906\n00:00\n\n\n7\n0.429145\n0.123678\n00:00\n\n\n8\n0.359694\n0.105466\n00:00\n\n\n9\n0.303888\n0.092883\n00:00\n\n\n\n\n\n\nlrnr.model.to(\"cpu\")\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n3142\n-1.138366\n-5.435792\n0.370670\n2\n\n\n3143\n-4.458741\n-4.281343\n2.052410\n2\n\n\n3144\n-2.836508\n-3.204013\n0.012610\n2\n\n\n3145\n-1.704158\n-10.621873\n2.024313\n2\n\n\n3146\n-2.467648\n-4.612999\n0.656284\n2\n\n\n\n\n\n3147 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n975\n0.432969\n-5.653580\n-1.944451\n0\n\n\n976\n2.685695\n-10.254354\n-2.466680\n0\n\n\n977\n2.474842\n-9.650204\n-2.452746\n0\n\n\n978\n1.268743\n-6.928779\n-1.419695\n0\n\n\n979\n4.371464\n-8.959077\n-4.056257\n0\n\n\n\n\n\n980 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼\n\n\n\n\n\n이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n\n\n\n\nfastai에서 지원\nfastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\ny의 형태를 주의해서 사용해야 함\n\n앞서 말한 것처럼 두 가지 타입이 있음\n\nvector + int\none-hot encoded vector + float\n\n\n\ntype 1) y의 형태가 vector + int일 때 - metrics = accuracy를 사용해야 함\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\ny.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\n\n\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nyy.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n1.128365\n0.601474\n0.463357\n0.536643\n00:00\n\n\n1\n0.684630\n0.304262\n0.975414\n0.024586\n00:00\n\n\n2\n0.503124\n0.144147\n0.989598\n0.010402\n00:00\n\n\n3\n0.373899\n0.068306\n0.996217\n0.003783\n00:00\n\n\n4\n0.281332\n0.040790\n0.996217\n0.003783\n00:00\n\n\n5\n0.215743\n0.026980\n0.996690\n0.003310\n00:00\n\n\n6\n0.168349\n0.019467\n0.996690\n0.003310\n00:00\n\n\n7\n0.133313\n0.014856\n0.998109\n0.001891\n00:00\n\n\n8\n0.106776\n0.011745\n0.998109\n0.001891\n00:00\n\n\n9\n0.086275\n0.009553\n0.999054\n0.000946\n00:00\n\n\n\n\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때 - metrics = accuracy_multi를 사용해야 함 - error_rate는 사용못함\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n\n\ny_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\nyy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\ntorch.nn.functional.one_hot() 함수 조건\n\n기본적으로 크기가 n인 벡터가 들어오길 기대\n정수가 들어오는 것을 기대\n\n하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n1.213650\n0.647844\n0.463357\n00:00\n\n\n1\n0.744869\n0.384000\n0.933570\n00:00\n\n\n2\n0.571879\n0.187119\n0.986525\n00:00\n\n\n3\n0.432096\n0.090841\n0.994563\n00:00\n\n\n4\n0.326718\n0.048868\n0.995508\n00:00\n\n\n5\n0.250235\n0.028477\n0.996217\n00:00\n\n\n6\n0.194605\n0.018616\n0.996454\n00:00\n\n\n7\n0.153338\n0.013278\n0.996927\n00:00\n\n\n8\n0.122056\n0.010130\n0.997872\n00:00\n\n\n9\n0.097957\n0.008112\n0.998109\n00:00"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#cnn-구조",
    "href": "docs/DL/posts/DL_CNN.html#cnn-구조",
    "title": "CNN part1",
    "section": "",
    "text": "간략하게 CNN의 구조를 설명하자면\n\nconv layer: 커널층(선형변환처럼 feature를 늘려주는 역할)\n\nReLU layer: 비선형을 추가해 표현력을 늘려주는 역할\n\npooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n\n\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n# (굳이 (2,2) 이런식으로 안해도 됨 어차피 윈도우 사이즈는 정사각행렬이므로 상수하나만 입력해도 됨됨)\n\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv.weight.data = torch.tensor([[[[0.25, 0.25],[0.25,0.25]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]), tensor([0.]))\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임)\n\n해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음\n\n윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함\n\n첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용\n\n두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n\n\n\n\n\n\n사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.\n\\(ReLU(x) = \\max(0,x)\\)\n\n\n\n\n\npooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\nmaxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_maxpooling(_X) \n\ntensor([[[ 6.,  8.],\n         [16., 18.]]])\n\n\n\nmaxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\npooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n\n이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#cnn-구현",
    "href": "docs/DL/posts/DL_CNN.html#cnn-구현",
    "title": "CNN part1",
    "section": "",
    "text": "path = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02&lt;00:00]\n    \n    \n\n\n\n\n(path/'원하는 경로').ls()\n위 코드를 사용하면 폴더 안에 있는 데이터들의 이름을 출력\ntorchvision.io.read_image()\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/1').ls()])\nx_tr = torch.concat([x0, x1])/255\n\ny_tr = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/1').ls()])\nx_test = torch.concat([x0, x1])/255\n\ny_test = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\nx_tr.shape, y_tr.shape, x_test.shape, y_test.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([2115, 1]))\n\n\n\n\n\n\n# conv\nc1 = torch.nn.Conv2d(1, 16 , 5) # 만약 color image였다면 입력 채널의 수를 3으로 지정해야 함\nx_tr.shape ,c1(x_tr).shape\n\n(torch.Size([12665, 1, 28, 28]), torch.Size([12665, 16, 24, 24]))\n\n\n\nsize계산 공식: 윈도우(커널)사이즈가 n이면 \\(size = height(width) - ( n - 1)\\)\n\n\n# ReLU\na1 = torch.nn.ReLU()\n\n\n# maxpooling\nm1 = torch.nn.MaxPool2d(2)\nprint(m1(a1(c1(x_tr))).shape)\n\ntorch.Size([12665, 16, 12, 12])\n\n\n\n행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n\n# flatten(이미지를 한줄로 펼치는 것)\nf1 = torch.nn.Flatten()\nprint(f1(a1(m1(c1(x_tr)))).shape) #16 * 12 * 12 = 2304\n\ntorch.Size([12665, 2304])\n\n\n\n# sigmoid에 올리기 위해서는 2304 디멘젼을 1로 만들어야함\nl1=torch.nn.Linear(in_features=2304,out_features=1) \n\n\n# sigmoid (값이 0에서 1사이의 값, 즉 확률로 출력되도록)\na2 = torch.nn.Sigmoid()\nprint(a2(f1(a1(m1(c1(x_tr))))).shape)\n\ntorch.Size([12665, 2304])\n\n\n\nprint('이미지 사이즈:                     ', x_tr.shape)\nprint('conv:                              ',c1(x_tr).shape)\nprint('(ReLU) -&gt; maxpooling:              ',m1(a1(c1(x_tr))).shape)\nprint('이미지 펼치기:                     ',f1(a1(m1(c1(x_tr)))).shape)\nprint('이미지를 하나의 스칼라로 선형변환: ',l1(f1(a1(m1(c1(x_tr))))).shape)\nprint('시그모이드:                        ',a2(l1(f1(a1(m1(c1(x_tr)))))).shape)\n\n이미지 사이즈:                      torch.Size([12665, 1, 28, 28])\nconv:                               torch.Size([12665, 16, 24, 24])\n(ReLU) -&gt; maxpooling:               torch.Size([12665, 16, 12, 12])\n이미지 펼치기:                      torch.Size([12665, 2304])\n이미지를 하나의 스칼라로 선형변환:  torch.Size([12665, 1])\n시그모이드:                         torch.Size([12665, 1])\n\n\n\n\n\n\n원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\\(\\to\\) overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n# 데이터 로더에 들어갈 데이터세트 준비\nds_tr = torch.utils.data.TensorDataset(x_tr, y_tr)\nds_test = torch.utils.data.TensorDataset(x_test, y_test)\n\n\n데이터 로더는 배치크기를 지정해줘야 함\n\n\nlen(x_tr), len(x_test)\n\n(12665, 2115)\n\n\n\n데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n\nstochastic gradient descent(구용어: mini-batch gradient descent)\n\n\n\n# 데이터 로더\ndl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=1266) \ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=2115) \n\n\ndls = DataLoaders(dl_tr,dl_test)\n\n\nnet = torch.nn.Sequential(\n    # conv layer \n    c1, \n    # ReLU\n    a1, \n    # maxpool\n    m1, \n    # flatten\n    f1, \n    # linear transform (n -&gt; 1)\n    l1,\n    # Sigmoid\n    a2\n)\n\nloss_fn=torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn) # fastai의 Learner는 오미타이저의 기본값이 adam이므로 따로 지정해주지 않아도 됨\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.979273\n0.639250\n00:05\n\n\n1\n0.703008\n0.402466\n00:00\n\n\n2\n0.547401\n0.256881\n00:00\n\n\n3\n0.434025\n0.142217\n00:00\n\n\n4\n0.340996\n0.079636\n00:00\n\n\n5\n0.267902\n0.048050\n00:00\n\n\n6\n0.211895\n0.031742\n00:00\n\n\n7\n0.169176\n0.022921\n00:00\n\n\n8\n0.136331\n0.017658\n00:00\n\n\n9\n0.110770\n0.014233\n00:00\n\n\n\n\n\n\nLearner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음 &gt; python    net[0].weight 위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = ’cuda:0’라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다.\n+ tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n\nnet = net.to('cpu')\nplt.plot(net(x_tr).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(x_test).data,'.')\nplt.title(\"Testing Set\",size=15)\n\nText(0.5, 1.0, 'Testing Set')"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#path-사용법-데이터-준비",
    "href": "docs/DL/posts/DL_CNN.html#path-사용법-데이터-준비",
    "title": "CNN part1",
    "section": "",
    "text": "(path/'원하는 경로').ls()"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#위-코드를-사용하면-폴더-안에-있는-데이터들의-이름을-출력",
    "href": "docs/DL/posts/DL_CNN.html#위-코드를-사용하면-폴더-안에-있는-데이터들의-이름을-출력",
    "title": "CNN part1",
    "section": "",
    "text": "torchvision.io.read_image()\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/1').ls()])\nx_tr = torch.concat([x0, x1])/255\n\ny_tr = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/1').ls()])\nx_test = torch.concat([x0, x1])/255\n\ny_test = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\nx_tr.shape, y_tr.shape, x_test.shape, y_test.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([2115, 1]))\n\n\n\n\n\n# conv\nc1 = torch.nn.Conv2d(1, 16 , 5) # 만약 color image였다면 입력 채널의 수를 3으로 지정해야 함\nx_tr.shape ,c1(x_tr).shape\n\n(torch.Size([12665, 1, 28, 28]), torch.Size([12665, 16, 24, 24]))\n\n\n\nsize계산 공식: 윈도우(커널)사이즈가 n이면 \\(size = height(width) - ( n - 1)\\)\n\n\n# ReLU\na1 = torch.nn.ReLU()\n\n\n# maxpooling\nm1 = torch.nn.MaxPool2d(2)\nprint(m1(a1(c1(x_tr))).shape)\n\ntorch.Size([12665, 16, 12, 12])\n\n\n\n행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n\n# flatten(이미지를 한줄로 펼치는 것)\nf1 = torch.nn.Flatten()\nprint(f1(a1(m1(c1(x_tr)))).shape) #16 * 12 * 12 = 2304\n\ntorch.Size([12665, 2304])\n\n\n\n# sigmoid에 올리기 위해서는 2304 디멘젼을 1로 만들어야함\nl1=torch.nn.Linear(in_features=2304,out_features=1) \n\n\n# sigmoid (값이 0에서 1사이의 값, 즉 확률로 출력되도록)\na2 = torch.nn.Sigmoid()\nprint(a2(f1(a1(m1(c1(x_tr))))).shape)\n\ntorch.Size([12665, 2304])\n\n\n\nprint('이미지 사이즈:                     ', x_tr.shape)\nprint('conv:                              ',c1(x_tr).shape)\nprint('(ReLU) -&gt; maxpooling:              ',m1(a1(c1(x_tr))).shape)\nprint('이미지 펼치기:                     ',f1(a1(m1(c1(x_tr)))).shape)\nprint('이미지를 하나의 스칼라로 선형변환: ',l1(f1(a1(m1(c1(x_tr))))).shape)\nprint('시그모이드:                        ',a2(l1(f1(a1(m1(c1(x_tr)))))).shape)\n\n이미지 사이즈:                      torch.Size([12665, 1, 28, 28])\nconv:                               torch.Size([12665, 16, 24, 24])\n(ReLU) -&gt; maxpooling:               torch.Size([12665, 16, 12, 12])\n이미지 펼치기:                      torch.Size([12665, 2304])\n이미지를 하나의 스칼라로 선형변환:  torch.Size([12665, 1])\n시그모이드:                         torch.Size([12665, 1])\n\n\n\n\n\n\n원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\\(\\to\\) overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n# 데이터 로더에 들어갈 데이터세트 준비\nds_tr = torch.utils.data.TensorDataset(x_tr, y_tr)\nds_test = torch.utils.data.TensorDataset(x_test, y_test)\n\n\n데이터 로더는 배치크기를 지정해줘야 함\n\n\nlen(x_tr), len(x_test)\n\n(12665, 2115)\n\n\n\n데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n\nstochastic gradient descent(구용어: mini-batch gradient descent)\n\n\n\n# 데이터 로더\ndl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=1266) \ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=2115) \n\n\ndls = DataLoaders(dl_tr,dl_test)\n\n\nnet = torch.nn.Sequential(\n    # conv layer \n    c1, \n    # ReLU\n    a1, \n    # maxpool\n    m1, \n    # flatten\n    f1, \n    # linear transform (n -&gt; 1)\n    l1,\n    # Sigmoid\n    a2\n)\n\nloss_fn=torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn) # fastai의 Learner는 오미타이저의 기본값이 adam이므로 따로 지정해주지 않아도 됨\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.979273\n0.639250\n00:05\n\n\n1\n0.703008\n0.402466\n00:00\n\n\n2\n0.547401\n0.256881\n00:00\n\n\n3\n0.434025\n0.142217\n00:00\n\n\n4\n0.340996\n0.079636\n00:00\n\n\n5\n0.267902\n0.048050\n00:00\n\n\n6\n0.211895\n0.031742\n00:00\n\n\n7\n0.169176\n0.022921\n00:00\n\n\n8\n0.136331\n0.017658\n00:00\n\n\n9\n0.110770\n0.014233\n00:00\n\n\n\n\n\n\nLearner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음 &gt; python    net[0].weight 위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = ’cuda:0’라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다.\n+ tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n\nnet = net.to('cpu')\nplt.plot(net(x_tr).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(x_test).data,'.')\nplt.title(\"Testing Set\",size=15)\n\nText(0.5, 1.0, 'Testing Set')"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#loss-function",
    "href": "docs/DL/posts/DL_CNN.html#loss-function",
    "title": "CNN part1",
    "section": "",
    "text": "BCEWithLogitsLoss = Sigmoid + BCELoss\n손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#k개의-클래스를-분류하는-모델",
    "href": "docs/DL/posts/DL_CNN.html#k개의-클래스를-분류하는-모델",
    "title": "CNN part1",
    "section": "",
    "text": "다중(k개) 클래스를 분류\n- LossFunction: CrossEntrophyLoss\n- ActivationFunction: SoftmaxFunction\n- 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n소프트맥스 함수가 계산하는 과정은 아래와 같음\n\\(softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}\\) (3개를 분류할 경우)\n\n\n\n\n\nk개의 클래스를 분류하는 모델의 Loss 계산 방법\n\nsftmax(_netout) # -&gt; 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -&gt; 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n이를 수식으로 정리하면 다음과 같음\nCrossEntropy = \\(- \\log (Softmax(x)) * Ground Truth_{onehot}(y)\\)\n만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -&gt; 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n+ 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임\n+ 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n\n\n\ntype 1) int형을 사용하는 방법 (vector)\ntype 2) float형을 사용하는 방법 (one-hot encoded vector)\n만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n\ntype 1)의 경우 &lt;사슴: 0, 강아지: 1, 고양이: 2&gt; (단, 데이터 형태는는 int(정수))\ntype 2)의 경우 &lt;사슴: [1, 0, 0], 강아지: [0, 1, 0], 고양이: [0, 0, 1] &gt; (단, 데이터의 형태는 float(실수))\n\n\n\n\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n\n\ntorch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n\n\n\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\ntorch.nn.functional.one_hot(yy).float()\n\ntensor([[1., 0.],\n        [1., 0.],\n        [1., 0.],\n        ...,\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])\n\n\n\n\n\n\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\nlen(X) # 18623\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번= 1862 꽉 채워서 10번하고 3개정도 남은 걸로 한 번\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # test는 전부다 넣어서\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\n\nloss_fn = torch.nn.CrossEntropyLoss() # 여기에는 softmax함수가 내장되어 있음 \n#-&gt; net마지막에 softmax를 넣으면 안됨\n# BCEWithLogitsLoss 느낌(시그모이드 + BCE)\n\n\nlrnr = Learner(dls,net,loss_fn) # 옵티마이저는 아답이 디폴트 값이어서 굳이 안넣어도 됨\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.797101\n1.103916\n00:00\n\n\n1\n1.267011\n0.823797\n00:00\n\n\n2\n1.055513\n0.667210\n00:00\n\n\n3\n0.910746\n0.435414\n00:00\n\n\n4\n0.762887\n0.284001\n00:00\n\n\n5\n0.625515\n0.199502\n00:00\n\n\n6\n0.515352\n0.152906\n00:00\n\n\n7\n0.429145\n0.123678\n00:00\n\n\n8\n0.359694\n0.105466\n00:00\n\n\n9\n0.303888\n0.092883\n00:00\n\n\n\n\n\n\nlrnr.model.to(\"cpu\")\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n3142\n-1.138366\n-5.435792\n0.370670\n2\n\n\n3143\n-4.458741\n-4.281343\n2.052410\n2\n\n\n3144\n-2.836508\n-3.204013\n0.012610\n2\n\n\n3145\n-1.704158\n-10.621873\n2.024313\n2\n\n\n3146\n-2.467648\n-4.612999\n0.656284\n2\n\n\n\n\n\n3147 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n975\n0.432969\n-5.653580\n-1.944451\n0\n\n\n976\n2.685695\n-10.254354\n-2.466680\n0\n\n\n977\n2.474842\n-9.650204\n-2.452746\n0\n\n\n978\n1.268743\n-6.928779\n-1.419695\n0\n\n\n979\n4.371464\n-8.959077\n-4.056257\n0\n\n\n\n\n\n980 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼\n\n\n\n\n\n이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n\n\n\n\nfastai에서 지원\nfastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\ny의 형태를 주의해서 사용해야 함\n\n앞서 말한 것처럼 두 가지 타입이 있음\n\nvector + int\none-hot encoded vector + float\n\n\n\ntype 1) y의 형태가 vector + int일 때 - metrics = accuracy를 사용해야 함\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\ny.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\n\n\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nyy.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n1.128365\n0.601474\n0.463357\n0.536643\n00:00\n\n\n1\n0.684630\n0.304262\n0.975414\n0.024586\n00:00\n\n\n2\n0.503124\n0.144147\n0.989598\n0.010402\n00:00\n\n\n3\n0.373899\n0.068306\n0.996217\n0.003783\n00:00\n\n\n4\n0.281332\n0.040790\n0.996217\n0.003783\n00:00\n\n\n5\n0.215743\n0.026980\n0.996690\n0.003310\n00:00\n\n\n6\n0.168349\n0.019467\n0.996690\n0.003310\n00:00\n\n\n7\n0.133313\n0.014856\n0.998109\n0.001891\n00:00\n\n\n8\n0.106776\n0.011745\n0.998109\n0.001891\n00:00\n\n\n9\n0.086275\n0.009553\n0.999054\n0.000946\n00:00\n\n\n\n\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때 - metrics = accuracy_multi를 사용해야 함 - error_rate는 사용못함\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n\n\ny_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\nyy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\ntorch.nn.functional.one_hot() 함수 조건\n\n기본적으로 크기가 n인 벡터가 들어오길 기대\n정수가 들어오는 것을 기대\n\n하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n1.213650\n0.647844\n0.463357\n00:00\n\n\n1\n0.744869\n0.384000\n0.933570\n00:00\n\n\n2\n0.571879\n0.187119\n0.986525\n00:00\n\n\n3\n0.432096\n0.090841\n0.994563\n00:00\n\n\n4\n0.326718\n0.048868\n0.995508\n00:00\n\n\n5\n0.250235\n0.028477\n0.996217\n00:00\n\n\n6\n0.194605\n0.018616\n0.996454\n00:00\n\n\n7\n0.153338\n0.013278\n0.996927\n00:00\n\n\n8\n0.122056\n0.010130\n0.997872\n00:00\n\n\n9\n0.097957\n0.008112\n0.998109\n00:00"
  },
  {
    "objectID": "docs/DL/posts/rstat101.html",
    "href": "docs/DL/posts/rstat101.html",
    "title": "Tutorial",
    "section": "",
    "text": "test12414입니\n123123다"
  },
  {
    "objectID": "docs/DL/rstat_lecture.html",
    "href": "docs/DL/rstat_lecture.html",
    "title": "rstat lecture",
    "section": "",
    "text": "1강\n2강\n3강"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "JuWon Kwon",
    "section": "",
    "text": "바이오메디컬공학을 전공하고 있는 학부생 입니다.\n초음파와 딥러닝을 공부하고 있습니다."
  },
  {
    "objectID": "docs/DL/posts/DL_CNN_2(XAI_정리).html",
    "href": "docs/DL/posts/DL_CNN_2(XAI_정리).html",
    "title": "CNN part2",
    "section": "",
    "text": "toc:true\n\n\nimport torch \nimport torchvision\nfrom fastai.vision.all import * \n\n\n\n\n누군가가 만들어 놓은 모델의 구조를 사용하는 것\nfine_tune을 사용하면 전이학습이 되어 학습시간이 줄어들음\nlrnr.fit을 사용하면 그냥 학습을 시키는 것\n\n\n\n\npath = untar_data(URLs.CIFAR)\n\n\npath.ls() # path에는 labels라는 텍스트 파일과 test,train이라는 폴더가 있음\n\n(#3) [Path('/root/.fastai/data/cifar10/labels.txt'),Path('/root/.fastai/data/cifar10/test'),Path('/root/.fastai/data/cifar10/train')]\n\n\n\n!ls /root/.fastai/data/cifar10/train\n\nairplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n\n\n\ntrain 폴더 안에는 위위와 같은 폴더들이 있음\n각 폴더에는 이름과 맞는 이미지 파일이 있음\n10개의 클래스\n앞 장에서 만든 CNN Architecture를 사용하면 정확도가 매우 떨어짐\n\n학습과정은 패스했음\n\n더 복잡하고 정교한 모델을 만들어야 함 \\(\\to\\) transfer learning 사용\n\n\n\n\n\nResNet의 weights를 그대로 가져와 학습에 사용\n\n\n\n\ndls = ImageDataLoaders.from_folder(path,train='train',valid='test')\n\n\n_x, _y = dls.one_batch()\n_x.shape, _y.shape\n\n(torch.Size([64, 3, 32, 32]), torch.Size([64]))\n\n\n\nx의 배치크기는 64(= 하나의 배치에 64개의 데이터(이미지)가 있음을 의미), 채널은 3(컬러 이미지), size는 32 x 32이다.\ny는 x의 레이블로 64개의 이미지가 있으므로 64개의 값이 y에 저장됨\n\n\n_y\n\nTensorCategory([5, 2, 8, 6, 8, 5, 1, 2, 1, 0, 4, 8, 7, 7, 0, 4, 2, 6, 8, 6, 6,\n                2, 0, 7, 1, 9, 5, 4, 3, 6, 4, 9, 6, 4, 3, 7, 3, 9, 3, 6, 5, 3,\n                6, 6, 6, 8, 6, 3, 5, 3, 5, 9, 5, 3, 9, 0, 7, 3, 6, 6, 7, 7, 8,\n                9], device='cuda:0')\n\n\n\n#collapse_output\nnet = torchvision.models.resnet18(weights=torchvision.models.resnet.ResNet18_Weights.IMAGENET1K_V1)\nnet\n\n\n마지막 출력층을 확인해보면 1000개를 출력함\n하지만 우리는 10개의 클래스를 구분하는 모델을 만들 것이므로 1000을 10으로 바꿔야 함\n\n\nnet.fc = torch.nn.Linear(in_features=512, out_features=10) \n\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\nlrnr = Learner(dls, net, loss_fn, metrics = accuracy)\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.804860\n0.821773\n0.724900\n01:15\n\n\n1\n0.640089\n0.679617\n0.773700\n01:09\n\n\n2\n0.519974\n0.647182\n0.783100\n01:05\n\n\n3\n0.405207\n0.564490\n0.811400\n01:14\n\n\n4\n0.344672\n0.683868\n0.783300\n01:05\n\n\n5\n0.269288\n0.737170\n0.785200\n01:12\n\n\n6\n0.272949\n0.788109\n0.769800\n01:12\n\n\n7\n0.188042\n0.690548\n0.808600\n01:10\n\n\n8\n0.175680\n0.736700\n0.800500\n01:09\n\n\n9\n0.151409\n0.821169\n0.795500\n01:06\n\n\n\n\n\n\n\n\n\noverview에서 fastai를 사용한 방법 (fastai로만 모델을 구현)\n\n\npath = untar_data(URLs.PETS)/'images'\n\nfiles= get_image_files(path)\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:08&lt;00:00]\n    \n    \n\n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.175012\n0.026964\n0.989851\n01:54\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.047284\n0.022501\n0.994587\n02:29\n\n\n\n\n\n\n아래의 코드를 사용하면 network의 구조를 볼 수 있음\n\nlrnr.model()\n\n\n\n\n\n\n# 위의 코드(fastai로 transfer learning) 똑같음\npath = untar_data(URLs.PETS)/'images'\n\nfiles= get_image_files(path)\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:04&lt;00:00]\n    \n    \n\n\n\npath.ls()\n\n(#7393) [Path('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_7.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_97.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/chihuahua_135.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/keeshond_110.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_54.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/wheaten_terrier_11.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/British_Shorthair_62.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/german_shorthaired_171.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/havanese_84.jpg')...]\n\n\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg')\nximg\n\n\n\n\n\nx= first(dls.test_dl([ximg]))[0]\nx.shape # 위 이미지의 shape\n\ntorch.Size([1, 3, 512, 512])\n\n\n\ntorch.nn.AdaptiveAvgPool2d()\n\npooling을 평균으로 하는 것으로 output_size = 1이라면 채널당 하나의 값을 출력해줌\n위의 이미지는 채널이 3개이므로 이미지 하나당 3개의 값을 출력\n\n\n\nap = torch.nn.AdaptiveAvgPool2d(output_size=1) # output size가 1이라는 것은 (1x1)로 출력하겠다는 뜻\n# 만약 2이면 출력이 (2 x 2) 즉 채널당 4개의 값이 나오는 것임\nap(x)\n\nTensorImage([[[[0.6086]],\n\n              [[0.5648]],\n\n              [[0.5058]]]], device='cuda:0')\n\n\n\n\n\n데이터 로더를 사용해 불러온 이미지 데이트의 차원을 확인해보면 [1, 3, 512, 512]이다. 이는 배치사이즈, 채널, 높이, 너비 순으로 되어있는 것임\n이러한 데이터를 높이, 너비, 채널 순으로 바꾸고 싶음 \\(\\to\\) torch.einsum()사용\n\ntorch.einsum('ij,jk-&gt;ik',tensor1, tensor2) # 크기가 i x j인 tensor1과 j x k인 tensor2의 행렬 곱을 해줌\n\ntorch.Size([1, 3, 512, 512]) \\(\\to\\) torch.Size([512, 512, 3]) 형태로 만들고 싶음\n\n→ tensor의 차원을 변경하거나 계산을 할 때에는 torch.einsum()함수를 사용하면 좋음\n\nx_new = torch.einsum('ocij -&gt; ijc',x.to('cpu'))\n\n\nplt는 높이 x 너비 x 채널 순으로 데이터가 입력되길 기대\n만약 x 즉, 채널 x 높이 x 너비 순으로 되어있는 데이터를 입력하면 에러가 발생함\n\n\nplt.imshow(x_new)\n\n&lt;matplotlib.image.AxesImage at 0x7fb7e874cf10&gt;\n\n\n\n\n\n\n\n\n\n\nCAM(Class Activation Mapping): CNN의 판단근거를 시각화하는 기술\n\n\n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.208073\n0.010794\n0.995940\n01:55\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.047890\n0.002702\n0.998647\n02:25\n\n\n\n\n\n\nlrnr.model 함수를 사용하면 resnet34의 구조를 볼 수 있음\n구조를 확인해보면 resnet34는 두 개의 모델이 합쳐져 있음\n첫 번째 모델은 그대로 사용하고 두 번째 모델만 약간 수정하여 사용해보자\n\n\n\n\n\nnet1 = lrnr.model[0]\nnet2 = lrnr.model[1]\n\n\nnet1.to('cpu')\nnet2.to('cpu')\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): fastai.layers.Flatten(full=False)\n  (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=1024, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\nBatchNorm1d과 Dropout, ReLU 층은 파라미터 개수에 변화를 주지 않음\npooling층과 Linear층을 주목해야 함\n\n\n_X, _y = dls.one_batch() \n\n\n_X = _X.to('cpu')\n# net1(_X).shape #torch.Size([64, 512, 16, 16]) 이러한 형태로 출력됨\n\n\nnet2= torch.nn.Sequential(\n    torch.nn.AdaptiveAvgPool2d(output_size=1), # (64,512,16,16) -&gt; (64,512,1,1) \n    torch.nn.Flatten(), # (64,512,1,1) -&gt; (64,512) \n    torch.nn.Linear(512,2,bias=False) # (64,512) -&gt; (64,2) \n)\n\n\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\n\nlrnr2= Learner(dls,net,metrics=accuracy)\n\nlrnr2.fine_tune(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.232032\n1.522452\n0.836265\n02:25\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.134071\n0.816453\n0.830176\n02:24\n\n\n1\n0.138622\n0.150047\n0.936401\n02:25\n\n\n2\n0.095163\n0.100338\n0.960081\n02:24\n\n\n3\n0.051146\n0.039767\n0.984438\n02:28\n\n\n4\n0.024118\n0.044908\n0.982409\n02:30\n\n\n\n\n\n\n\n\n- net2의 순서 바꾸기 전 전체 네트워크:\n\\[\\underset{(1,3,512,512)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{ap}{\\to} \\underset{(1,512,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,512)}{{\\boldsymbol \\sharp}}\\overset{linear}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [-9.0358,  9.0926]\\]\n\n위의 수식을 아래의 수식으로 바꾸고 싶음\n\n순서만 바꾸는 것임 -&gt; 결과는 똑같음\n\n\n\\[\\underset{(1,3,224,224)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{linear}{\\to} \\underset{(1,2,16,16)}{{\\bf why}}\\overset{ap}{\\to} \\underset{(1,2,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [−9.0358,9.0926]\\]\n\n여기서 주목해야 하는 것은 \\(why\\)\n\\(why\\)의 값들을 평균을 내고 이 값을 선형변환하여 하나의 값을 만듦 [−9.0358 or 9.0926]\n\\(why\\)의 tensor를 확인해보자!\n바꾸기 전 net2의 구조 중 일부분 (배치를 넣는 것이 아니라 하나의 이미지를 net에 넣을때임)\n\nX = net1(x)\n\nX.shape = (1, 512, 16, 16)\n\nXX = torch.nn.AdaptiveAvgPool2d(X)\n\nXX.shape =(1, 512, 1, 1)\n\nXXX= torch.nn.Flatten(XX)\n\nXXX.shape -&gt; (1,512)\n\nXXX @ net2[2].weight \\(\\to\\) shape(1, 2)\n\nnet2[2].weight.shape -&gt; (512, 2)\n\n\n\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg')\nx = first(dls.test_dl([ximg]))[0]\n\n\n# 위의 정리된 것들을 보면\n# net2[2].weight의 크기는 (512, 2)\n# net1(x)의 크기는 (1, 512, 16, 16)\n# why의 크기를 (1, 2, 16, 16)으로 만들고 싶음\nwhy = torch.einsum('cb,abij-&gt;acij',\n                   net2[2].weight,#(512, 2)\n                   net1(x) # (1, 512, 16, 16)\n                   )\nwhy.shape\n\ntorch.Size([1, 2, 16, 16])\n\n\n\n# net2[0]은 avg pooling layer -&gt; 높이와 폭을 0으로 만듦\n# (1, 2, 16, 16) -&gt; (1, 2, 1, 1)\nnet2[0](why)\n\nTensorImage([[[[-7.2395]],\n\n              [[ 7.5647]]]], device='cuda:0', grad_fn=&lt;AliasBackward0&gt;)\n\n\n\n\n\n\n\ntensor의 평균 -9.0358\n해당 tensor의 평균이 크다면 고양이라고 판단을 함\n\n하지만 이번 예시에서는 평균이 음수가 나옴 -&gt; 해당 값이 작으면 작을수록 고양이가 아니라고 생각하는 근거가 됨\n\n그러므로 해당 tensor에서 엄청 작은 값들은 고양이가 아니라고 판단하는 근거가 되는 점들임\n\n\nwhy[0,0,:,:].to(torch.int64)\n\nTensorImage([[  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    1,    1,    0,    1,    1,    1,    1,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    1,    1,    0,    0,   -1,   -2,   -2,\n                -1,    0,    2,    2,    1],\n             [   0,    0,    0,    0,    0,    0,   -2,  -10,  -17,  -18,  -17,\n               -14,   -7,    2,    5,    1],\n             [  -1,    0,    0,    0,    0,    0,   -7,  -26,  -45,  -43,  -38,\n               -34,  -16,    0,    3,    0],\n             [  -1,    0,    0,   -1,   -2,   -3,  -10,  -32,  -56,  -58,  -50,\n               -43,  -22,   -2,    1,    1],\n             [   1,    2,    0,   -6,  -14,  -12,  -11,  -25,  -45,  -66,  -71,\n               -49,  -20,    0,    2,    2],\n             [   8,   10,    2,   -9,  -19,  -18,  -10,  -13,  -34,  -74, -100,\n               -65,  -21,    1,    1,    0],\n             [  11,   12,    2,   -6,  -13,  -13,   -7,   -5,  -23,  -60,  -82,\n               -63,  -21,    0,    0,    0],\n             [   7,    6,    1,    0,   -2,   -4,   -3,   -1,  -12,  -32,  -45,\n               -36,  -14,   -1,    0,   -2],\n             [   0,    1,    0,    1,    2,    0,    0,    0,   -1,   -7,  -12,\n               -11,   -5,   -1,   -1,   -2],\n             [  -2,   -1,   -1,   -1,   -1,   -1,    0,    1,    4,    1,   -1,\n                -1,   -2,   -2,   -2,   -3],\n             [  -4,   -4,   -4,   -5,   -4,   -4,   -4,   -2,   -2,   -3,   -4,\n                -5,   -5,   -5,   -5,   -4],\n             [  -5,   -6,   -6,   -6,   -6,   -6,   -5,   -5,   -5,   -5,   -6,\n                -6,   -6,   -6,   -5,   -5]], device='cuda:0')\n\n\n\n\n\n\ntensor의 평균 9.0926\n해당 tensor는 강아지라고 판단하는 tensor라고 생각하면 됨\n그러므로 해당 tensor에서 엄청 작은 값들은 고양이가 아니라고 판단하는 근거가 되는 점들임\n\n\nwhy[0,1,:,:].to(torch.int64)\n\nTensorImage([[ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 2,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  1,  2,  2,  2,  0, -1, -1,  0],\n             [ 0,  0,  0,  0,  0,  0,  2, 10, 16, 18, 17, 14,  7, -1, -3, -1],\n             [ 1,  1,  0,  0,  0,  0,  6, 25, 43, 42, 37, 32, 16,  1, -2,  0],\n             [ 1,  0,  0,  1,  2,  3, 10, 32, 55, 58, 50, 42, 21,  3,  0,  0],\n             [ 0, -1,  0,  6, 13, 12, 11, 24, 44, 65, 69, 49, 20,  1, -1, -1],\n             [-7, -8, -1,  9, 18, 17, 10, 13, 33, 72, 97, 65, 22,  0,  0,  0],\n             [-9, -9, -2,  5, 12, 13,  7,  5, 22, 58, 81, 62, 21,  1,  0,  1],\n             [-6, -6, -1,  1,  2,  4,  3,  1, 11, 31, 43, 34, 14,  1,  1,  2],\n             [ 0,  0,  0,  0,  0,  0,  1,  0,  1,  7, 12, 10,  5,  1,  1,  3],\n             [ 3,  2,  2,  2,  2,  1,  1,  0, -2,  0,  1,  2,  2,  3,  3,  4],\n             [ 5,  6,  6,  6,  6,  6,  5,  3,  4,  5,  5,  6,  6,  6,  6,  6],\n             [ 5,  6,  7,  7,  7,  7,  6,  6,  6,  6,  6,  7,  7,  7,  6,  6]],\n            device='cuda:0')\n\n\n\n\n\n\n\n\nwhy의 값과 이미지를 동시에 출력한다면 그림의 어떠한 부분을 보고 강아지라고 판단을 한 것인지 알 수 있음\n\n\nwhy_cat = why[0,0,:,:] # 해당 tensor의 값이 작으면 작을 수록 고양이가 아니라고 판단하는 근거가 됨\nwhy_dog = why[0,1,:,:] # 해당 tensor의 값이 크면 클수록 강아지라고 판단하는 근거가 됨\n\n\n밑에서 cmap을 magma로 지정하였는 이는 값이 작은 부분은 검정색 큰 부분은 노란색으로 표시함\n\n검정 -&gt; 보라 -&gt; 빨강 -&gt; 노랑\n\n이를 해석하면 why_cat에서 값이 가장 작은 부분, 즉 검정색이 고양이가 아니라고 판단함\nwhy_dog에서는 값이 가장 큰 부분, 즉 노란색이 강아지라고 판단하는 영역을 표시함\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma')\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc99c0810&gt;\n\n\n\n\n\n\nwhy의 크기 조절\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc90e9410&gt;\n\n\n\n\n\n\n크기조절 후 겹쳐그리기\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc8df78d0&gt;\n\n\n\n\n\n\n\n\n# why값을 확률로 출력하기\nsftmax=torch.nn.Softmax(dim=1)\nsftmax(net(x))\ncatprob, dogprob = sftmax(net(x))[0,0].item(), sftmax(net(x))[0,1].item()\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[0].set_title('catprob= %f' % catprob) \nax[1].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].set_title('dogprob=%f' % dogprob)\n\nText(0.5, 1.0, 'dogprob=1.000000')\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=0 \nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -&gt; acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob&gt;dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN_2(XAI_정리).html#transfer-learning",
    "href": "docs/DL/posts/DL_CNN_2(XAI_정리).html#transfer-learning",
    "title": "CNN part2",
    "section": "",
    "text": "누군가가 만들어 놓은 모델의 구조를 사용하는 것\nfine_tune을 사용하면 전이학습이 되어 학습시간이 줄어들음\nlrnr.fit을 사용하면 그냥 학습을 시키는 것\n\n\n\n\npath = untar_data(URLs.CIFAR)\n\n\npath.ls() # path에는 labels라는 텍스트 파일과 test,train이라는 폴더가 있음\n\n(#3) [Path('/root/.fastai/data/cifar10/labels.txt'),Path('/root/.fastai/data/cifar10/test'),Path('/root/.fastai/data/cifar10/train')]\n\n\n\n!ls /root/.fastai/data/cifar10/train\n\nairplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n\n\n\ntrain 폴더 안에는 위위와 같은 폴더들이 있음\n각 폴더에는 이름과 맞는 이미지 파일이 있음\n10개의 클래스\n앞 장에서 만든 CNN Architecture를 사용하면 정확도가 매우 떨어짐\n\n학습과정은 패스했음\n\n더 복잡하고 정교한 모델을 만들어야 함 \\(\\to\\) transfer learning 사용\n\n\n\n\n\nResNet의 weights를 그대로 가져와 학습에 사용\n\n\n\n\ndls = ImageDataLoaders.from_folder(path,train='train',valid='test')\n\n\n_x, _y = dls.one_batch()\n_x.shape, _y.shape\n\n(torch.Size([64, 3, 32, 32]), torch.Size([64]))\n\n\n\nx의 배치크기는 64(= 하나의 배치에 64개의 데이터(이미지)가 있음을 의미), 채널은 3(컬러 이미지), size는 32 x 32이다.\ny는 x의 레이블로 64개의 이미지가 있으므로 64개의 값이 y에 저장됨\n\n\n_y\n\nTensorCategory([5, 2, 8, 6, 8, 5, 1, 2, 1, 0, 4, 8, 7, 7, 0, 4, 2, 6, 8, 6, 6,\n                2, 0, 7, 1, 9, 5, 4, 3, 6, 4, 9, 6, 4, 3, 7, 3, 9, 3, 6, 5, 3,\n                6, 6, 6, 8, 6, 3, 5, 3, 5, 9, 5, 3, 9, 0, 7, 3, 6, 6, 7, 7, 8,\n                9], device='cuda:0')\n\n\n\n#collapse_output\nnet = torchvision.models.resnet18(weights=torchvision.models.resnet.ResNet18_Weights.IMAGENET1K_V1)\nnet\n\n\n마지막 출력층을 확인해보면 1000개를 출력함\n하지만 우리는 10개의 클래스를 구분하는 모델을 만들 것이므로 1000을 10으로 바꿔야 함\n\n\nnet.fc = torch.nn.Linear(in_features=512, out_features=10) \n\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\nlrnr = Learner(dls, net, loss_fn, metrics = accuracy)\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.804860\n0.821773\n0.724900\n01:15\n\n\n1\n0.640089\n0.679617\n0.773700\n01:09\n\n\n2\n0.519974\n0.647182\n0.783100\n01:05\n\n\n3\n0.405207\n0.564490\n0.811400\n01:14\n\n\n4\n0.344672\n0.683868\n0.783300\n01:05\n\n\n5\n0.269288\n0.737170\n0.785200\n01:12\n\n\n6\n0.272949\n0.788109\n0.769800\n01:12\n\n\n7\n0.188042\n0.690548\n0.808600\n01:10\n\n\n8\n0.175680\n0.736700\n0.800500\n01:09\n\n\n9\n0.151409\n0.821169\n0.795500\n01:06\n\n\n\n\n\n\n\n\n\noverview에서 fastai를 사용한 방법 (fastai로만 모델을 구현)\n\n\npath = untar_data(URLs.PETS)/'images'\n\nfiles= get_image_files(path)\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:08&lt;00:00]\n    \n    \n\n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.175012\n0.026964\n0.989851\n01:54\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.047284\n0.022501\n0.994587\n02:29\n\n\n\n\n\n\n아래의 코드를 사용하면 network의 구조를 볼 수 있음\n\nlrnr.model()"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN_2(XAI_정리).html#데이터-확인",
    "href": "docs/DL/posts/DL_CNN_2(XAI_정리).html#데이터-확인",
    "title": "CNN part2",
    "section": "",
    "text": "# 위의 코드(fastai로 transfer learning) 똑같음\npath = untar_data(URLs.PETS)/'images'\n\nfiles= get_image_files(path)\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:04&lt;00:00]\n    \n    \n\n\n\npath.ls()\n\n(#7393) [Path('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_7.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_97.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/chihuahua_135.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/keeshond_110.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_54.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/wheaten_terrier_11.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/British_Shorthair_62.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/german_shorthaired_171.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/havanese_84.jpg')...]\n\n\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg')\nximg\n\n\n\n\n\nx= first(dls.test_dl([ximg]))[0]\nx.shape # 위 이미지의 shape\n\ntorch.Size([1, 3, 512, 512])\n\n\n\ntorch.nn.AdaptiveAvgPool2d()\n\npooling을 평균으로 하는 것으로 output_size = 1이라면 채널당 하나의 값을 출력해줌\n위의 이미지는 채널이 3개이므로 이미지 하나당 3개의 값을 출력\n\n\n\nap = torch.nn.AdaptiveAvgPool2d(output_size=1) # output size가 1이라는 것은 (1x1)로 출력하겠다는 뜻\n# 만약 2이면 출력이 (2 x 2) 즉 채널당 4개의 값이 나오는 것임\nap(x)\n\nTensorImage([[[[0.6086]],\n\n              [[0.5648]],\n\n              [[0.5058]]]], device='cuda:0')\n\n\n\n\n\n데이터 로더를 사용해 불러온 이미지 데이트의 차원을 확인해보면 [1, 3, 512, 512]이다. 이는 배치사이즈, 채널, 높이, 너비 순으로 되어있는 것임\n이러한 데이터를 높이, 너비, 채널 순으로 바꾸고 싶음 \\(\\to\\) torch.einsum()사용\n\ntorch.einsum('ij,jk-&gt;ik',tensor1, tensor2) # 크기가 i x j인 tensor1과 j x k인 tensor2의 행렬 곱을 해줌\n\ntorch.Size([1, 3, 512, 512]) \\(\\to\\) torch.Size([512, 512, 3]) 형태로 만들고 싶음\n\n→ tensor의 차원을 변경하거나 계산을 할 때에는 torch.einsum()함수를 사용하면 좋음\n\nx_new = torch.einsum('ocij -&gt; ijc',x.to('cpu'))\n\n\nplt는 높이 x 너비 x 채널 순으로 데이터가 입력되길 기대\n만약 x 즉, 채널 x 높이 x 너비 순으로 되어있는 데이터를 입력하면 에러가 발생함\n\n\nplt.imshow(x_new)\n\n&lt;matplotlib.image.AxesImage at 0x7fb7e874cf10&gt;"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN_2(XAI_정리).html#cam-구현",
    "href": "docs/DL/posts/DL_CNN_2(XAI_정리).html#cam-구현",
    "title": "CNN part2",
    "section": "",
    "text": "CAM(Class Activation Mapping): CNN의 판단근거를 시각화하는 기술\n\n\n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.208073\n0.010794\n0.995940\n01:55\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.047890\n0.002702\n0.998647\n02:25\n\n\n\n\n\n\nlrnr.model 함수를 사용하면 resnet34의 구조를 볼 수 있음\n구조를 확인해보면 resnet34는 두 개의 모델이 합쳐져 있음\n첫 번째 모델은 그대로 사용하고 두 번째 모델만 약간 수정하여 사용해보자\n\n\n\n\n\nnet1 = lrnr.model[0]\nnet2 = lrnr.model[1]\n\n\nnet1.to('cpu')\nnet2.to('cpu')\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): fastai.layers.Flatten(full=False)\n  (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=1024, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\nBatchNorm1d과 Dropout, ReLU 층은 파라미터 개수에 변화를 주지 않음\npooling층과 Linear층을 주목해야 함\n\n\n_X, _y = dls.one_batch() \n\n\n_X = _X.to('cpu')\n# net1(_X).shape #torch.Size([64, 512, 16, 16]) 이러한 형태로 출력됨\n\n\nnet2= torch.nn.Sequential(\n    torch.nn.AdaptiveAvgPool2d(output_size=1), # (64,512,16,16) -&gt; (64,512,1,1) \n    torch.nn.Flatten(), # (64,512,1,1) -&gt; (64,512) \n    torch.nn.Linear(512,2,bias=False) # (64,512) -&gt; (64,2) \n)\n\n\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\n\nlrnr2= Learner(dls,net,metrics=accuracy)\n\nlrnr2.fine_tune(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.232032\n1.522452\n0.836265\n02:25\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.134071\n0.816453\n0.830176\n02:24\n\n\n1\n0.138622\n0.150047\n0.936401\n02:25\n\n\n2\n0.095163\n0.100338\n0.960081\n02:24\n\n\n3\n0.051146\n0.039767\n0.984438\n02:28\n\n\n4\n0.024118\n0.044908\n0.982409\n02:30\n\n\n\n\n\n\n\n\n- net2의 순서 바꾸기 전 전체 네트워크:\n\\[\\underset{(1,3,512,512)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{ap}{\\to} \\underset{(1,512,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,512)}{{\\boldsymbol \\sharp}}\\overset{linear}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [-9.0358,  9.0926]\\]\n\n위의 수식을 아래의 수식으로 바꾸고 싶음\n\n순서만 바꾸는 것임 -&gt; 결과는 똑같음\n\n\n\\[\\underset{(1,3,224,224)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{linear}{\\to} \\underset{(1,2,16,16)}{{\\bf why}}\\overset{ap}{\\to} \\underset{(1,2,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [−9.0358,9.0926]\\]\n\n여기서 주목해야 하는 것은 \\(why\\)\n\\(why\\)의 값들을 평균을 내고 이 값을 선형변환하여 하나의 값을 만듦 [−9.0358 or 9.0926]\n\\(why\\)의 tensor를 확인해보자!\n바꾸기 전 net2의 구조 중 일부분 (배치를 넣는 것이 아니라 하나의 이미지를 net에 넣을때임)\n\nX = net1(x)\n\nX.shape = (1, 512, 16, 16)\n\nXX = torch.nn.AdaptiveAvgPool2d(X)\n\nXX.shape =(1, 512, 1, 1)\n\nXXX= torch.nn.Flatten(XX)\n\nXXX.shape -&gt; (1,512)\n\nXXX @ net2[2].weight \\(\\to\\) shape(1, 2)\n\nnet2[2].weight.shape -&gt; (512, 2)\n\n\n\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg')\nx = first(dls.test_dl([ximg]))[0]\n\n\n# 위의 정리된 것들을 보면\n# net2[2].weight의 크기는 (512, 2)\n# net1(x)의 크기는 (1, 512, 16, 16)\n# why의 크기를 (1, 2, 16, 16)으로 만들고 싶음\nwhy = torch.einsum('cb,abij-&gt;acij',\n                   net2[2].weight,#(512, 2)\n                   net1(x) # (1, 512, 16, 16)\n                   )\nwhy.shape\n\ntorch.Size([1, 2, 16, 16])\n\n\n\n# net2[0]은 avg pooling layer -&gt; 높이와 폭을 0으로 만듦\n# (1, 2, 16, 16) -&gt; (1, 2, 1, 1)\nnet2[0](why)\n\nTensorImage([[[[-7.2395]],\n\n              [[ 7.5647]]]], device='cuda:0', grad_fn=&lt;AliasBackward0&gt;)\n\n\n\n\n\n\n\ntensor의 평균 -9.0358\n해당 tensor의 평균이 크다면 고양이라고 판단을 함\n\n하지만 이번 예시에서는 평균이 음수가 나옴 -&gt; 해당 값이 작으면 작을수록 고양이가 아니라고 생각하는 근거가 됨\n\n그러므로 해당 tensor에서 엄청 작은 값들은 고양이가 아니라고 판단하는 근거가 되는 점들임\n\n\nwhy[0,0,:,:].to(torch.int64)\n\nTensorImage([[  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    1,    1,    0,    1,    1,    1,    1,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    1,    1,    0,    0,   -1,   -2,   -2,\n                -1,    0,    2,    2,    1],\n             [   0,    0,    0,    0,    0,    0,   -2,  -10,  -17,  -18,  -17,\n               -14,   -7,    2,    5,    1],\n             [  -1,    0,    0,    0,    0,    0,   -7,  -26,  -45,  -43,  -38,\n               -34,  -16,    0,    3,    0],\n             [  -1,    0,    0,   -1,   -2,   -3,  -10,  -32,  -56,  -58,  -50,\n               -43,  -22,   -2,    1,    1],\n             [   1,    2,    0,   -6,  -14,  -12,  -11,  -25,  -45,  -66,  -71,\n               -49,  -20,    0,    2,    2],\n             [   8,   10,    2,   -9,  -19,  -18,  -10,  -13,  -34,  -74, -100,\n               -65,  -21,    1,    1,    0],\n             [  11,   12,    2,   -6,  -13,  -13,   -7,   -5,  -23,  -60,  -82,\n               -63,  -21,    0,    0,    0],\n             [   7,    6,    1,    0,   -2,   -4,   -3,   -1,  -12,  -32,  -45,\n               -36,  -14,   -1,    0,   -2],\n             [   0,    1,    0,    1,    2,    0,    0,    0,   -1,   -7,  -12,\n               -11,   -5,   -1,   -1,   -2],\n             [  -2,   -1,   -1,   -1,   -1,   -1,    0,    1,    4,    1,   -1,\n                -1,   -2,   -2,   -2,   -3],\n             [  -4,   -4,   -4,   -5,   -4,   -4,   -4,   -2,   -2,   -3,   -4,\n                -5,   -5,   -5,   -5,   -4],\n             [  -5,   -6,   -6,   -6,   -6,   -6,   -5,   -5,   -5,   -5,   -6,\n                -6,   -6,   -6,   -5,   -5]], device='cuda:0')\n\n\n\n\n\n\ntensor의 평균 9.0926\n해당 tensor는 강아지라고 판단하는 tensor라고 생각하면 됨\n그러므로 해당 tensor에서 엄청 작은 값들은 고양이가 아니라고 판단하는 근거가 되는 점들임\n\n\nwhy[0,1,:,:].to(torch.int64)\n\nTensorImage([[ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 2,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  1,  2,  2,  2,  0, -1, -1,  0],\n             [ 0,  0,  0,  0,  0,  0,  2, 10, 16, 18, 17, 14,  7, -1, -3, -1],\n             [ 1,  1,  0,  0,  0,  0,  6, 25, 43, 42, 37, 32, 16,  1, -2,  0],\n             [ 1,  0,  0,  1,  2,  3, 10, 32, 55, 58, 50, 42, 21,  3,  0,  0],\n             [ 0, -1,  0,  6, 13, 12, 11, 24, 44, 65, 69, 49, 20,  1, -1, -1],\n             [-7, -8, -1,  9, 18, 17, 10, 13, 33, 72, 97, 65, 22,  0,  0,  0],\n             [-9, -9, -2,  5, 12, 13,  7,  5, 22, 58, 81, 62, 21,  1,  0,  1],\n             [-6, -6, -1,  1,  2,  4,  3,  1, 11, 31, 43, 34, 14,  1,  1,  2],\n             [ 0,  0,  0,  0,  0,  0,  1,  0,  1,  7, 12, 10,  5,  1,  1,  3],\n             [ 3,  2,  2,  2,  2,  1,  1,  0, -2,  0,  1,  2,  2,  3,  3,  4],\n             [ 5,  6,  6,  6,  6,  6,  5,  3,  4,  5,  5,  6,  6,  6,  6,  6],\n             [ 5,  6,  7,  7,  7,  7,  6,  6,  6,  6,  6,  7,  7,  7,  6,  6]],\n            device='cuda:0')\n\n\n\n\n\n\n\n\nwhy의 값과 이미지를 동시에 출력한다면 그림의 어떠한 부분을 보고 강아지라고 판단을 한 것인지 알 수 있음\n\n\nwhy_cat = why[0,0,:,:] # 해당 tensor의 값이 작으면 작을 수록 고양이가 아니라고 판단하는 근거가 됨\nwhy_dog = why[0,1,:,:] # 해당 tensor의 값이 크면 클수록 강아지라고 판단하는 근거가 됨\n\n\n밑에서 cmap을 magma로 지정하였는 이는 값이 작은 부분은 검정색 큰 부분은 노란색으로 표시함\n\n검정 -&gt; 보라 -&gt; 빨강 -&gt; 노랑\n\n이를 해석하면 why_cat에서 값이 가장 작은 부분, 즉 검정색이 고양이가 아니라고 판단함\nwhy_dog에서는 값이 가장 큰 부분, 즉 노란색이 강아지라고 판단하는 영역을 표시함\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma')\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc99c0810&gt;\n\n\n\n\n\n\nwhy의 크기 조절\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc90e9410&gt;\n\n\n\n\n\n\n크기조절 후 겹쳐그리기\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc8df78d0&gt;\n\n\n\n\n\n\n\n\n# why값을 확률로 출력하기\nsftmax=torch.nn.Softmax(dim=1)\nsftmax(net(x))\ncatprob, dogprob = sftmax(net(x))[0,0].item(), sftmax(net(x))[0,1].item()\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[0].set_title('catprob= %f' % catprob) \nax[1].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].set_title('dogprob=%f' % dogprob)\n\nText(0.5, 1.0, 'dogprob=1.000000')\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=0 \nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -&gt; acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob&gt;dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()"
  },
  {
    "objectID": "docs/DL/posts/FCN_summary.html",
    "href": "docs/DL/posts/FCN_summary.html",
    "title": "FCN paper review",
    "section": "",
    "text": "semantic segmantation의 초석을 닦은 연구\nsegmantation = (1) semantic segmantaion (2) instance segmantation\n\n\n\n\nimage.png\n\n\n\nbiomedical image에서는 semantic segmentation을 사용 ex) 의료 영상\n\n\n\n\n\n\nimage.png\n\n\n\nend-to-end방식\npixel 단위로 클래스 예측 (from supervised pre-trained)\nencoder - decoder 형태\n유명한 CNN 아키텍쳐의 특징을 갖고 있음\n\n\n\n\n\n\nimage.png\n\n\n1 encoder 부분(Downsampling)에서는 3 * 3 Conv-&gt; 3 * 3 Conv -&gt; pooling을 진행하는 구조\n2 image segmantation은 기본적으로 CNN의 구조를 따라감. 단, CNN 구조의 마지막에 있는 FC layer는 갖고 오지 않음\n\n\n\nimage.png\n\n\n\nFC layer를 사용하게 되면 위치정보가 소실됨 -&gt; Conv layer 사용\nConv Layer는 1x1 Conv layer(Google Net)를 사용하여 채널의 수를 조절\n채널의 수는 객체의 수와 동일하게 맞춤\n\n3 [\\(H\\), \\(W\\)] 이미지가 [conv - conv - pooling] (VGG의 구조)를 n번 통과한다면 이미지의 크기는 [\\(H/2^n\\), \\(W/2^n\\)]가 되게 됨\n\n\n\nimage.png\n\n\n\n이를 다시 말하면 픽셀 하나가 \\(2^n\\)개의 픽셀의 정보를 갖고 있다는 뜻 -&gt; 위치정보를 애매하게 알게 됨\nupsampling 과정에서 대략적인(?) segmantation이 진행됨\n이를 보완하기 위해서 skip connection(ResNet)을 이용\n\n\n\n\n\n\n\nimage.png\n\n\n\n1x1 Conv를 사용해 채널수를 맞추어 Upsampling한 Feature map과 합침\n위 과정을 지난 Feature map을 다시 한번 Upsampling한 후 Feature map과 합침\n마지막에 몇 배로 Upsampling을 했는지에 따라서 이름이 바뀜\n\nex) 위의 그림은 마지막에 8배로 Upsampling을 했으므로 FCN-8s"
  },
  {
    "objectID": "docs/DL/posts/FCN_summary.html#fcn-structure",
    "href": "docs/DL/posts/FCN_summary.html#fcn-structure",
    "title": "FCN paper review",
    "section": "",
    "text": "image.png\n\n\n\nend-to-end방식\npixel 단위로 클래스 예측 (from supervised pre-trained)\nencoder - decoder 형태\n유명한 CNN 아키텍쳐의 특징을 갖고 있음\n\n\n\n\n\n\nimage.png\n\n\n1 encoder 부분(Downsampling)에서는 3 * 3 Conv-&gt; 3 * 3 Conv -&gt; pooling을 진행하는 구조\n2 image segmantation은 기본적으로 CNN의 구조를 따라감. 단, CNN 구조의 마지막에 있는 FC layer는 갖고 오지 않음\n\n\n\nimage.png\n\n\n\nFC layer를 사용하게 되면 위치정보가 소실됨 -&gt; Conv layer 사용\nConv Layer는 1x1 Conv layer(Google Net)를 사용하여 채널의 수를 조절\n채널의 수는 객체의 수와 동일하게 맞춤\n\n3 [\\(H\\), \\(W\\)] 이미지가 [conv - conv - pooling] (VGG의 구조)를 n번 통과한다면 이미지의 크기는 [\\(H/2^n\\), \\(W/2^n\\)]가 되게 됨\n\n\n\nimage.png\n\n\n\n이를 다시 말하면 픽셀 하나가 \\(2^n\\)개의 픽셀의 정보를 갖고 있다는 뜻 -&gt; 위치정보를 애매하게 알게 됨\nupsampling 과정에서 대략적인(?) segmantation이 진행됨\n이를 보완하기 위해서 skip connection(ResNet)을 이용\n\n\n\n\n\n\n\nimage.png\n\n\n\n1x1 Conv를 사용해 채널수를 맞추어 Upsampling한 Feature map과 합침\n위 과정을 지난 Feature map을 다시 한번 Upsampling한 후 Feature map과 합침\n마지막에 몇 배로 Upsampling을 했는지에 따라서 이름이 바뀜\n\nex) 위의 그림은 마지막에 8배로 Upsampling을 했으므로 FCN-8s"
  },
  {
    "objectID": "docs/DL/posts/U-net_summary.html",
    "href": "docs/DL/posts/U-net_summary.html",
    "title": "U-net paper review",
    "section": "",
    "text": "image.png\n\n\n\n\n\n\nimport argparse\n\nimport os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom torchvision import transforms, datasets\n\n+ CNN output size 계산\n$ outputsize = + 1$\n\n아래에서 정의하는 CBR2d를 위 식에 적용해보면 outputsize = inputsize인 것을 알 수 있음\n이미지의 사이즈는 maxpooling을 사용하여 Width와 Height를 절반씩 줄여주는 것을 확인할 수 있음\n\n\nclass Unet(nn.Module):\n    def __init__(self):\n        super(Unet, self).__init__()\n\n        def CBR2d(in_channels, out_channels, kernel_size =3, stride = 1, padding = 1, bias = True): #convolution + bathnormalization + Relu\n            layers = []\n            layers = layers + [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n                                          kernel_size=kernel_size, stride=stride, padding= padding,\n                                          bias=bias)]\n            layers = layers + [nn.BatchNorm2d(num_features = out_channels)]\n            layers = layers + [nn.ReLU()]\n\n            cbr = nn.Sequential(*layers)\n\n            return cbr\n        \n        # contracting(encoder)\n        self.enc1_1 = CBR2d(in_channels=1, out_channels=64)\n        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n\n        self.pool3 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n\n        self.pool4 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n\n        ## Expansive (decoder)\n        self.dec5_2 = CBR2d(in_channels=1024, out_channels=512)\n        \n        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec4_2 = CBR2d(in_channels= 2 * 512, out_channels=512)\n        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n        \n        self.unpool3 = nn.ConvTranspose2d(in_channels= 256, out_channels=256, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec3_2 = CBR2d(in_channels= 2 * 256, out_channels=256)\n        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n\n        self.unpool2 = nn.ConvTranspose2d(in_channels= 128, out_channels=128, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec2_2 = CBR2d(in_channels= 2 * 128, out_channels=128)\n        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n        \n        self.unpool1 = nn.ConvTranspose2d(in_channels= 64, out_channels=64, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec1_2 = CBR2d(in_channels= 2 * 64, out_channels=64)\n        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n\n        self.fc = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1,stride=1, padding = 0, bias = True)\n\n    def forward(self, x):\n        enc1_1 = self.enc1_1(x)\n        enc1_2 = self.enc1_2(enc1_1)\n        pool1 = self.pool1(enc1_2)\n\n        enc2_1 = self.enc2_1(pool1)\n        enc2_2 = self.enc2_2(enc2_1)\n        pool2 = self.pool2(enc2_2)\n\n        enc3_1 = self.enc3_1(pool2)\n        enc3_2 = self.enc3_2(enc3_1)\n        pool3 = self.pool3(enc3_2)\n\n        enc4_1 = self.enc4_1(pool3)\n        enc4_2 = self.enc4_2(enc4_1)\n        pool4 = self.pool4(enc4_2)\n\n        enc5_1 = self.enc5_1(pool4)\n\n        dec5_1 = self.dec5_1(enc5_1)\n\n        unpool4 = self.unpool4(dec5_1)\n        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n        dec4_2 = self.dec4_2(cat4)\n        dec4_1 = self.dec4_1(dec4_2)\n\n        unpool3 = self.unpool3(dec4_1)\n        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n        dec3_2 = self.dec3_2(cat3)\n        dec3_1 = self.dec3_1(dec3_2)\n\n        unpool2 = self.unpool2(dec3_1)\n        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n        dec2_2 = self.dec2_2(cat2)\n        dec2_1 = self.dec2_1(dec2_2)\n\n        unpool1 = self.unpool1(dec2_1)\n        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n        dec1_2 = self.dec1_2(cat1)\n        dec1_1 = self.dec1_1(dec1_2)\n\n        x = self.fc(dec1_1)\n\n        return x\n\n\n\n\n\n\n사용 예시\n\nself.unpool1 = nn.ConvTranspose2d(in_channels= 64, out_channels=64, kernel_size=2, stride=2, padding= 0,bias=True)\nhttps://cumulu-s.tistory.com/29\n\n\n\n- dim = 1로 지정하면 채널을 기준으로 concat  \n- dim = 0일 때 -&gt; batch를 기준으로\n- dim = 2일 때 -&gt; height를 기준으로\n- dim = 3일 떄 -&gt; width를 기준으로\n\na = torch.tensor([[[[1,2,3,4],[5,6,7,8],[9,10,11,12]]]])\nb = torch.tensor([[[[1,2,3,4],[5,6,7,8],[9,10,11,12]]]])\nprint(a)\n\ntensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]])\n\n\n\ntest_dim0 = torch.cat((a,b),dim = 0)\nprint(\"dim = 0일 때: \\n\",test_dim0, '\\n', test_dim0.shape)\n\ndim = 0일 때: \n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]],\n\n\n        [[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]]) \n torch.Size([2, 1, 3, 4])\n\n\n\ntest_dim1 = torch.cat((a,b),dim = 1)\nprint(\"dim = 1일 때: \\n\",test_dim1, '\\n', test_dim1.shape)\n\ndim = 1일 때: \n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]],\n\n         [[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]]) \n torch.Size([1, 2, 3, 4])\n\n\n\ntest_dim2 = torch.cat((a,b),dim = 2)\nprint(\"dim = 2일 때: \\n\",test_dim2, '\\n', test_dim2.shape)\n\ndim = 2일 때: \n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12],\n          [ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]]) \n torch.Size([1, 1, 6, 4])\n\n\n\ntest_dim3 = torch.cat((a,b),dim = 3)\nprint(\"dim = 3일 때: \\n\",test_dim3, '\\n', test_dim3.shape)\n\ndim = 3일 때: \n tensor([[[[ 1,  2,  3,  4,  1,  2,  3,  4],\n          [ 5,  6,  7,  8,  5,  6,  7,  8],\n          [ 9, 10, 11, 12,  9, 10, 11, 12]]]]) \n torch.Size([1, 1, 3, 8])\n\n\n\n\n\n\n\n- loss 곡선\n\n\n\nimage.png\n\n\n- 만들어진 annotation\n\n\n\n\n\n\n\nimport os\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\n## 트레이닝 파라미터 설정\nlr = 1e-3\nbatch_size = 2\nnum_epoch = 50\n\ndata_dir = './datasets'\nckpt_dir = './checkpoint'\nlog_dir = './log'\n\n\n##\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n\n        lst_data = os.listdir(self.data_dir)\n\n        lst_label = [f for f in lst_data if f.startswith('label')]\n        lst_input = [f for f in lst_data if f.startswith('input')]\n\n        lst_label.sort()\n        lst_input.sort()\n\n        self.lst_label = lst_label\n        self.lst_input = lst_input\n\n    def __len__(self):\n        return len(self.lst_label)\n\n    def __getitem__(self, index):\n        label = np.load(os.path.join(self.data_dir, self.lst_label[index]))\n        input = np.load(os.path.join(self.data_dir, self.lst_input[index]))\n\n        label = label/255.0\n        input = input/255.0\n\n        if label.ndim == 2:\n            label = label[:, :, np.newaxis]\n        if input.ndim == 2:\n            input = input[:, :, np.newaxis]\n\n        data = {'input': input, 'label': label}\n\n        if self.transform:\n            data = self.transform(data)\n\n        return data\n##\nclass ToTensor(object):\n    def __call__(self, data):\n        label, input = data['label'], data['input']\n\n        label = label.transpose((2, 0, 1)).astype(np.float32)\n        input = input.transpose((2, 0, 1)).astype(np.float32)\n\n        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n        return data\n\nclass Normalization(object):\n    def __init__(self, mean = 0.5, std = 0.5):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, data):\n        label, input = data['label'], data['input']\n\n        input = (input - self.mean) / self.std\n        data = {'label': label, 'input': input}\n\n        return data\n\n\nclass RandomFlip(object):\n    def __call__(self, data):\n        label, input = data['label'], data['input']\n\n        if np.random.rand() &gt; 0.5:\n            label = np.fliplr(label)\n            input = np.fliplr(input)\n\n        if np.random.rand() &gt; 0.5:\n            label = np.flipud(label)\n            input = np.flipud(input)\n\n        data = {'label': label, 'input': input}\n\n        return data\n\n## 네트워크 구축하기\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n            layers = []\n            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n                                 kernel_size=kernel_size, stride=stride, padding=padding,\n                                 bias=bias)]\n            layers += [nn.BatchNorm2d(num_features=out_channels)]\n            layers += [nn.ReLU()]\n\n            cbr = nn.Sequential(*layers)\n\n            return cbr\n\n        # Contracting path\n        self.enc1_1 = CBR2d(in_channels=1, out_channels=64)\n        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n\n        self.pool3 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n\n        self.pool4 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n\n        # Expansive path\n        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n\n        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec4_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n\n        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec3_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n\n        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec2_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n\n        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec1_2 = CBR2d(in_channels=2 * 64, out_channels=64)\n        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n\n        self.fc = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n\n    def forward(self, x):\n        enc1_1 = self.enc1_1(x)\n        enc1_2 = self.enc1_2(enc1_1)\n        pool1 = self.pool1(enc1_2)\n\n        enc2_1 = self.enc2_1(pool1)\n        enc2_2 = self.enc2_2(enc2_1)\n        pool2 = self.pool2(enc2_2)\n\n        enc3_1 = self.enc3_1(pool2)\n        enc3_2 = self.enc3_2(enc3_1)\n        pool3 = self.pool3(enc3_2)\n\n        enc4_1 = self.enc4_1(pool3)\n        enc4_2 = self.enc4_2(enc4_1)\n        pool4 = self.pool4(enc4_2)\n\n        enc5_1 = self.enc5_1(pool4)\n\n        dec5_1 = self.dec5_1(enc5_1)\n\n        unpool4 = self.unpool4(dec5_1)\n        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n        dec4_2 = self.dec4_2(cat4)\n        dec4_1 = self.dec4_1(dec4_2)\n\n        unpool3 = self.unpool3(dec4_1)\n        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n        dec3_2 = self.dec3_2(cat3)\n        dec3_1 = self.dec3_1(dec3_2)\n\n        unpool2 = self.unpool2(dec3_1)\n        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n        dec2_2 = self.dec2_2(cat2)\n        dec2_1 = self.dec2_1(dec2_2)\n\n        unpool1 = self.unpool1(dec2_1)\n        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n        dec1_2 = self.dec1_2(cat1)\n        dec1_1 = self.dec1_1(dec1_2)\n\n        x = self.fc(dec1_1)\n\n        return x\n\n## 네트워크 학습하기\ntransform = transforms.Compose([Normalization(mean=0.5, std=0.5), RandomFlip(), ToTensor()])\n\ndataset_train = Dataset(data_dir=os.path.join(data_dir, 'train'), transform=transform)\nloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n\ndataset_val = Dataset(data_dir=os.path.join(data_dir, 'val'), transform=transform)\nloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True, num_workers=0)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n## 네트워크 생성하기\nnet = UNet().to(device)\n\nfn_loss = nn.BCEWithLogitsLoss().to(device)\noptim = torch.optim.Adam(net.parameters(),lr=lr)\n\nnum_data_train = len(dataset_train)\nnum_data_val = len(dataset_val)\n\nnum_batch_train = np.ceil(num_data_train / batch_size)\nnum_batch_val = np.ceil(num_data_val / batch_size)\n\n## function\nfn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\nfn_denorm = lambda x, mean, std: (x * std) + mean\nfn_clss = lambda x: 1.0*(x&gt;0.5)\n\n## tensorboard를 사용하기 위한 SummaryWriter 설정\nwriter_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\nwriter_val = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))\n\n## 네트워크 학습시키기\nst_epoch = 0\n\n## 네트워크 저장하기\ndef save(ckpt_dir, net, optim, epoch):\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n\n    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n               \"%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n\n\n## 네트워크 불러오기\ndef load(ckpt_dir, net, optim):\n    if not os.path.exists(ckpt_dir):\n        epoch = 0\n        return net, optim, epoch\n\n    ckpt_lst = os.listdir(ckpt_dir)\n    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n\n    dict_model = torch.load('%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n\n    net.load_state_dict(dict_model['net'])\n    optim.load_state_dict(dict_model['optim'])\n    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n\n    return net, optim, epoch\n\n#net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n\nfor epoch in range(st_epoch + 1, num_epoch + 1):\n    net.train()\n    loss_arr = []\n\n    for batch, data in enumerate(loader_train, 1):\n        # forward pass\n        label = data['label'].to(device)\n        input = data['input'].to(device)\n\n        output = net(input)\n\n        # backward pass\n        optim.zero_grad()\n\n        loss = fn_loss(output, label)\n        loss.backward()\n\n        optim.step()\n\n        # 손실함수 계산\n        loss_arr += [loss.item()]\n\n        print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f\" %\n              (epoch, num_epoch, batch, num_batch_train, np.mean(loss_arr)))\n\n        # Tensorboard 저장하기\n        label = fn_tonumpy(label)\n        input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n        output = fn_tonumpy(fn_clss(output))\n\n        writer_train.add_image('label', label, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n        writer_train.add_image('input', input, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n        writer_train.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n\n    writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n\n    with torch.no_grad():\n        net.eval()\n        loss_arr = []\n\n        for batch, data in enumerate(loader_val, 1):\n            # forward pass\n            label = data['label'].to(device)\n            input = data['input'].to(device)\n\n            output = net(input)\n\n            # 손실함수 계산하기\n            loss = fn_loss(output, label)\n\n            loss_arr += [loss.item()]\n\n            print(\"VALID: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f\" %\n                  (epoch, num_epoch, batch, num_batch_val, np.mean(loss_arr)))\n\n            # Tensorboard 저장하기\n            label = fn_tonumpy(label)\n            input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n            output = fn_tonumpy(fn_clss(output))\n\n            writer_val.add_image('label', label, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n            writer_val.add_image('input', input, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n            writer_val.add_image('output', output, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n\n    writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n\n    if epoch % 50 == 0:\n        save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch)\n\nwriter_train.close()\nwriter_val.close()"
  },
  {
    "objectID": "docs/DL/posts/U-net_summary.html#논문에-나오는-구조",
    "href": "docs/DL/posts/U-net_summary.html#논문에-나오는-구조",
    "title": "U-net paper review",
    "section": "",
    "text": "image.png"
  },
  {
    "objectID": "docs/DL/posts/U-net_summary.html#논문을-기반으로-네트워크-구현",
    "href": "docs/DL/posts/U-net_summary.html#논문을-기반으로-네트워크-구현",
    "title": "U-net paper review",
    "section": "",
    "text": "import argparse\n\nimport os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom torchvision import transforms, datasets\n\n+ CNN output size 계산\n$ outputsize = + 1$\n\n아래에서 정의하는 CBR2d를 위 식에 적용해보면 outputsize = inputsize인 것을 알 수 있음\n이미지의 사이즈는 maxpooling을 사용하여 Width와 Height를 절반씩 줄여주는 것을 확인할 수 있음\n\n\nclass Unet(nn.Module):\n    def __init__(self):\n        super(Unet, self).__init__()\n\n        def CBR2d(in_channels, out_channels, kernel_size =3, stride = 1, padding = 1, bias = True): #convolution + bathnormalization + Relu\n            layers = []\n            layers = layers + [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n                                          kernel_size=kernel_size, stride=stride, padding= padding,\n                                          bias=bias)]\n            layers = layers + [nn.BatchNorm2d(num_features = out_channels)]\n            layers = layers + [nn.ReLU()]\n\n            cbr = nn.Sequential(*layers)\n\n            return cbr\n        \n        # contracting(encoder)\n        self.enc1_1 = CBR2d(in_channels=1, out_channels=64)\n        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n\n        self.pool3 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n\n        self.pool4 = nn.MaxPool2d(kernel_size=2)\n        \n        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n\n        ## Expansive (decoder)\n        self.dec5_2 = CBR2d(in_channels=1024, out_channels=512)\n        \n        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec4_2 = CBR2d(in_channels= 2 * 512, out_channels=512)\n        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n        \n        self.unpool3 = nn.ConvTranspose2d(in_channels= 256, out_channels=256, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec3_2 = CBR2d(in_channels= 2 * 256, out_channels=256)\n        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n\n        self.unpool2 = nn.ConvTranspose2d(in_channels= 128, out_channels=128, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec2_2 = CBR2d(in_channels= 2 * 128, out_channels=128)\n        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n        \n        self.unpool1 = nn.ConvTranspose2d(in_channels= 64, out_channels=64, \n                                          kernel_size=2, stride=2, padding= 0,bias=True)\n        \n        self.dec1_2 = CBR2d(in_channels= 2 * 64, out_channels=64)\n        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n\n        self.fc = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1,stride=1, padding = 0, bias = True)\n\n    def forward(self, x):\n        enc1_1 = self.enc1_1(x)\n        enc1_2 = self.enc1_2(enc1_1)\n        pool1 = self.pool1(enc1_2)\n\n        enc2_1 = self.enc2_1(pool1)\n        enc2_2 = self.enc2_2(enc2_1)\n        pool2 = self.pool2(enc2_2)\n\n        enc3_1 = self.enc3_1(pool2)\n        enc3_2 = self.enc3_2(enc3_1)\n        pool3 = self.pool3(enc3_2)\n\n        enc4_1 = self.enc4_1(pool3)\n        enc4_2 = self.enc4_2(enc4_1)\n        pool4 = self.pool4(enc4_2)\n\n        enc5_1 = self.enc5_1(pool4)\n\n        dec5_1 = self.dec5_1(enc5_1)\n\n        unpool4 = self.unpool4(dec5_1)\n        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n        dec4_2 = self.dec4_2(cat4)\n        dec4_1 = self.dec4_1(dec4_2)\n\n        unpool3 = self.unpool3(dec4_1)\n        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n        dec3_2 = self.dec3_2(cat3)\n        dec3_1 = self.dec3_1(dec3_2)\n\n        unpool2 = self.unpool2(dec3_1)\n        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n        dec2_2 = self.dec2_2(cat2)\n        dec2_1 = self.dec2_1(dec2_2)\n\n        unpool1 = self.unpool1(dec2_1)\n        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n        dec1_2 = self.dec1_2(cat1)\n        dec1_1 = self.dec1_1(dec1_2)\n\n        x = self.fc(dec1_1)\n\n        return x\n\n\n\n\n\n\n사용 예시\n\nself.unpool1 = nn.ConvTranspose2d(in_channels= 64, out_channels=64, kernel_size=2, stride=2, padding= 0,bias=True)\nhttps://cumulu-s.tistory.com/29\n\n\n\n- dim = 1로 지정하면 채널을 기준으로 concat  \n- dim = 0일 때 -&gt; batch를 기준으로\n- dim = 2일 때 -&gt; height를 기준으로\n- dim = 3일 떄 -&gt; width를 기준으로\n\na = torch.tensor([[[[1,2,3,4],[5,6,7,8],[9,10,11,12]]]])\nb = torch.tensor([[[[1,2,3,4],[5,6,7,8],[9,10,11,12]]]])\nprint(a)\n\ntensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]])\n\n\n\ntest_dim0 = torch.cat((a,b),dim = 0)\nprint(\"dim = 0일 때: \\n\",test_dim0, '\\n', test_dim0.shape)\n\ndim = 0일 때: \n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]],\n\n\n        [[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]]) \n torch.Size([2, 1, 3, 4])\n\n\n\ntest_dim1 = torch.cat((a,b),dim = 1)\nprint(\"dim = 1일 때: \\n\",test_dim1, '\\n', test_dim1.shape)\n\ndim = 1일 때: \n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]],\n\n         [[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]]) \n torch.Size([1, 2, 3, 4])\n\n\n\ntest_dim2 = torch.cat((a,b),dim = 2)\nprint(\"dim = 2일 때: \\n\",test_dim2, '\\n', test_dim2.shape)\n\ndim = 2일 때: \n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12],\n          [ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12]]]]) \n torch.Size([1, 1, 6, 4])\n\n\n\ntest_dim3 = torch.cat((a,b),dim = 3)\nprint(\"dim = 3일 때: \\n\",test_dim3, '\\n', test_dim3.shape)\n\ndim = 3일 때: \n tensor([[[[ 1,  2,  3,  4,  1,  2,  3,  4],\n          [ 5,  6,  7,  8,  5,  6,  7,  8],\n          [ 9, 10, 11, 12,  9, 10, 11, 12]]]]) \n torch.Size([1, 1, 3, 8])"
  },
  {
    "objectID": "docs/DL/posts/U-net_summary.html#결과",
    "href": "docs/DL/posts/U-net_summary.html#결과",
    "title": "U-net paper review",
    "section": "",
    "text": "- loss 곡선\n\n\n\nimage.png\n\n\n- 만들어진 annotation"
  },
  {
    "objectID": "docs/DL/posts/U-net_summary.html#참고-u-net-훈련을-위한-.py-파일",
    "href": "docs/DL/posts/U-net_summary.html#참고-u-net-훈련을-위한-.py-파일",
    "title": "U-net paper review",
    "section": "",
    "text": "import os\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\n## 트레이닝 파라미터 설정\nlr = 1e-3\nbatch_size = 2\nnum_epoch = 50\n\ndata_dir = './datasets'\nckpt_dir = './checkpoint'\nlog_dir = './log'\n\n\n##\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n\n        lst_data = os.listdir(self.data_dir)\n\n        lst_label = [f for f in lst_data if f.startswith('label')]\n        lst_input = [f for f in lst_data if f.startswith('input')]\n\n        lst_label.sort()\n        lst_input.sort()\n\n        self.lst_label = lst_label\n        self.lst_input = lst_input\n\n    def __len__(self):\n        return len(self.lst_label)\n\n    def __getitem__(self, index):\n        label = np.load(os.path.join(self.data_dir, self.lst_label[index]))\n        input = np.load(os.path.join(self.data_dir, self.lst_input[index]))\n\n        label = label/255.0\n        input = input/255.0\n\n        if label.ndim == 2:\n            label = label[:, :, np.newaxis]\n        if input.ndim == 2:\n            input = input[:, :, np.newaxis]\n\n        data = {'input': input, 'label': label}\n\n        if self.transform:\n            data = self.transform(data)\n\n        return data\n##\nclass ToTensor(object):\n    def __call__(self, data):\n        label, input = data['label'], data['input']\n\n        label = label.transpose((2, 0, 1)).astype(np.float32)\n        input = input.transpose((2, 0, 1)).astype(np.float32)\n\n        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n        return data\n\nclass Normalization(object):\n    def __init__(self, mean = 0.5, std = 0.5):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, data):\n        label, input = data['label'], data['input']\n\n        input = (input - self.mean) / self.std\n        data = {'label': label, 'input': input}\n\n        return data\n\n\nclass RandomFlip(object):\n    def __call__(self, data):\n        label, input = data['label'], data['input']\n\n        if np.random.rand() &gt; 0.5:\n            label = np.fliplr(label)\n            input = np.fliplr(input)\n\n        if np.random.rand() &gt; 0.5:\n            label = np.flipud(label)\n            input = np.flipud(input)\n\n        data = {'label': label, 'input': input}\n\n        return data\n\n## 네트워크 구축하기\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n            layers = []\n            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n                                 kernel_size=kernel_size, stride=stride, padding=padding,\n                                 bias=bias)]\n            layers += [nn.BatchNorm2d(num_features=out_channels)]\n            layers += [nn.ReLU()]\n\n            cbr = nn.Sequential(*layers)\n\n            return cbr\n\n        # Contracting path\n        self.enc1_1 = CBR2d(in_channels=1, out_channels=64)\n        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n\n        self.pool3 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n\n        self.pool4 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n\n        # Expansive path\n        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n\n        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec4_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n\n        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec3_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n\n        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec2_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n\n        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec1_2 = CBR2d(in_channels=2 * 64, out_channels=64)\n        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n\n        self.fc = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n\n    def forward(self, x):\n        enc1_1 = self.enc1_1(x)\n        enc1_2 = self.enc1_2(enc1_1)\n        pool1 = self.pool1(enc1_2)\n\n        enc2_1 = self.enc2_1(pool1)\n        enc2_2 = self.enc2_2(enc2_1)\n        pool2 = self.pool2(enc2_2)\n\n        enc3_1 = self.enc3_1(pool2)\n        enc3_2 = self.enc3_2(enc3_1)\n        pool3 = self.pool3(enc3_2)\n\n        enc4_1 = self.enc4_1(pool3)\n        enc4_2 = self.enc4_2(enc4_1)\n        pool4 = self.pool4(enc4_2)\n\n        enc5_1 = self.enc5_1(pool4)\n\n        dec5_1 = self.dec5_1(enc5_1)\n\n        unpool4 = self.unpool4(dec5_1)\n        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n        dec4_2 = self.dec4_2(cat4)\n        dec4_1 = self.dec4_1(dec4_2)\n\n        unpool3 = self.unpool3(dec4_1)\n        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n        dec3_2 = self.dec3_2(cat3)\n        dec3_1 = self.dec3_1(dec3_2)\n\n        unpool2 = self.unpool2(dec3_1)\n        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n        dec2_2 = self.dec2_2(cat2)\n        dec2_1 = self.dec2_1(dec2_2)\n\n        unpool1 = self.unpool1(dec2_1)\n        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n        dec1_2 = self.dec1_2(cat1)\n        dec1_1 = self.dec1_1(dec1_2)\n\n        x = self.fc(dec1_1)\n\n        return x\n\n## 네트워크 학습하기\ntransform = transforms.Compose([Normalization(mean=0.5, std=0.5), RandomFlip(), ToTensor()])\n\ndataset_train = Dataset(data_dir=os.path.join(data_dir, 'train'), transform=transform)\nloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n\ndataset_val = Dataset(data_dir=os.path.join(data_dir, 'val'), transform=transform)\nloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True, num_workers=0)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n## 네트워크 생성하기\nnet = UNet().to(device)\n\nfn_loss = nn.BCEWithLogitsLoss().to(device)\noptim = torch.optim.Adam(net.parameters(),lr=lr)\n\nnum_data_train = len(dataset_train)\nnum_data_val = len(dataset_val)\n\nnum_batch_train = np.ceil(num_data_train / batch_size)\nnum_batch_val = np.ceil(num_data_val / batch_size)\n\n## function\nfn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\nfn_denorm = lambda x, mean, std: (x * std) + mean\nfn_clss = lambda x: 1.0*(x&gt;0.5)\n\n## tensorboard를 사용하기 위한 SummaryWriter 설정\nwriter_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\nwriter_val = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))\n\n## 네트워크 학습시키기\nst_epoch = 0\n\n## 네트워크 저장하기\ndef save(ckpt_dir, net, optim, epoch):\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n\n    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n               \"%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n\n\n## 네트워크 불러오기\ndef load(ckpt_dir, net, optim):\n    if not os.path.exists(ckpt_dir):\n        epoch = 0\n        return net, optim, epoch\n\n    ckpt_lst = os.listdir(ckpt_dir)\n    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n\n    dict_model = torch.load('%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n\n    net.load_state_dict(dict_model['net'])\n    optim.load_state_dict(dict_model['optim'])\n    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n\n    return net, optim, epoch\n\n#net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n\nfor epoch in range(st_epoch + 1, num_epoch + 1):\n    net.train()\n    loss_arr = []\n\n    for batch, data in enumerate(loader_train, 1):\n        # forward pass\n        label = data['label'].to(device)\n        input = data['input'].to(device)\n\n        output = net(input)\n\n        # backward pass\n        optim.zero_grad()\n\n        loss = fn_loss(output, label)\n        loss.backward()\n\n        optim.step()\n\n        # 손실함수 계산\n        loss_arr += [loss.item()]\n\n        print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f\" %\n              (epoch, num_epoch, batch, num_batch_train, np.mean(loss_arr)))\n\n        # Tensorboard 저장하기\n        label = fn_tonumpy(label)\n        input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n        output = fn_tonumpy(fn_clss(output))\n\n        writer_train.add_image('label', label, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n        writer_train.add_image('input', input, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n        writer_train.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n\n    writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n\n    with torch.no_grad():\n        net.eval()\n        loss_arr = []\n\n        for batch, data in enumerate(loader_val, 1):\n            # forward pass\n            label = data['label'].to(device)\n            input = data['input'].to(device)\n\n            output = net(input)\n\n            # 손실함수 계산하기\n            loss = fn_loss(output, label)\n\n            loss_arr += [loss.item()]\n\n            print(\"VALID: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f\" %\n                  (epoch, num_epoch, batch, num_batch_val, np.mean(loss_arr)))\n\n            # Tensorboard 저장하기\n            label = fn_tonumpy(label)\n            input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n            output = fn_tonumpy(fn_clss(output))\n\n            writer_val.add_image('label', label, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n            writer_val.add_image('input', input, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n            writer_val.add_image('output', output, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n\n    writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n\n    if epoch % 50 == 0:\n        save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch)\n\nwriter_train.close()\nwriter_val.close()"
  },
  {
    "objectID": "docs/DL/posts/DCGAN.html",
    "href": "docs/DL/posts/DCGAN.html",
    "title": "DCGAN paper review",
    "section": "",
    "text": "[DCGAN 장점] - 대부분의 상황에서 안정적으로 학습이 됨 - word2vec과 같이 DCGAN으로 학습된 Generator가 벡터 산술 연산이 가능한 성질을 갖음\n\n\nDCGAN이 학습한 필터를 시각화하여 보여줌 -&gt; 특정 필터들이 이미지의 특정 물체를 학습했음을 보여줌\n\n\n\n\nAlt text\n\n\n\n성능면에서 비지도 학습 알고리즘에 비해 우수함\n\n[APPROACH AND MODEL ARCHITECTURE]\nDCGAN은\n- Max Pooling To Strided Convolution - Fully-Connected Layer 삭제 - BatchNormalization을 추가함 -&gt; deep한 모델이더라도 gradient의 흐름이 잘 전달됨 - ReLU와 Leaky ReLU를 사용\n[DETAILS OF ADVERSARIAL TRAINING]\n\n모델 및 옵티마이저\n\n\nmini-batch Stochastic Gradient Descent(SGD) a with batch size of 128\nAll weight: zero-centered Normal distribution with std 0.02\nLeaky ReLU: slope 0.2\nOptimizer: Adam (GAN에서는 momentum 사용)\nlearning late: 0.0002(0.001은 너무 커서..)\nD’s criterion= \\(\\log(D(x))\\)(real data) + \\(\\log(1 - D(G(x)))\\)(fake data)\nG’s criterion = \\(\\log(D(G(z)))\\)\n\n데이터\n\nLSUN\nFACES\nIMAGENET-1K\n\n\nDCGAN에서 중요한 기준 - NOT MIMICKING TRAIN DATA -&gt; 단순히 학습 데이터를 모방하면 안됨! - “Walking in the latent Space” -&gt; G의 input z의 공간인 latent Space에서 z1에서 z2로 살짝 이동한다 하더라도 급작스러운 변화가 일어나지 않고 물흐르듯 부드러운 변화를 보여줘야 한다.\n\n\n\n\n\n\n\nimport random\nimport math\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom PIL import Image\n\nimport torch\nimport torch.utils.data as data\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision.datasets as dsets\nfrom torchvision import transforms\n\ntorch.manual_seed(1234)\nnp.random.seed(1234)\nrandom.seed(1234)\n\n\n\n\ndef conv_dim(i,k,s,p):\n    '''\n    nn.Conv2d 사용할 때 사이즈 계산\n    i: input image size\n    k: kernel size\n    s: stride\n    p: padding\n    '''\n    out = (i - k + 2*p)/s + 1\n    return out  \n\n\ndef convt_dim(i,k,s,p):\n    '''\n    nn.ConvTranspose2d 사용할 때 사이즈 계산\n    i: input image size\n    k: kernel size\n    s: stride\n    p: padding\n    '''\n    out = (i-1) * s - 2 * p + k\n    return out  \n\n\n\n\n\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim = 20, image_size = 64):\n        super(Generator, self).__init__()\n\n        # layer1 -&gt; W(H) * 4\n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, image_size * 8, kernel_size = 4, stride = 1),\n            nn.BatchNorm2d(image_size * 8),\n            nn.ReLU(inplace=True))\n        \n        # layer2 -&gt; W(H) * 2\n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 8, image_size * 4, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size * 4),\n            nn.ReLU(inplace=True))\n        \n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 4, image_size * 2, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size * 2),\n            nn.ReLU(inplace=True))\n        \n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 2, image_size, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size),\n            nn.ReLU(inplace=True))\n        \n        self.last = nn.Sequential(\n            nn.ConvTranspose2d(image_size, 1, kernel_size= 4, stride=2, padding=1), # 흑백 이미지이므로 출력 차원을 1으로 지정한 것\n            nn.Tanh())\n        \n    def forward(self, z):\n        out = self.layer1(z)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        out = out.type(torch.FloatTensor)\n        return out\n\n\n\n\nG = Generator(z_dim=20, image_size=64)\ninput_z = torch.randn(1, 20)\n\n# tensor size -&gt; (1, 20, 1, 1) \n# pytorch: (batch_size, channel, height, width)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n\nfake_img = G(input_z)\n\n\n# fake_img[0][0].size() -&gt; (64, 64)\nimg_transformed = fake_img[0][0].detach().numpy()\nplt.imshow(img_transformed, 'gray')\nplt.show()\n\n\n\n\n\n\n\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, z_dim=20, image_size=64):\n        super(Discriminator, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, image_size, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(image_size, image_size*2, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(image_size*2, image_size*4, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(image_size*4, image_size*8, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.last = nn.Conv2d(image_size*8, 1, kernel_size=4, stride=1)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        out = out.type(torch.FloatTensor)\n        return out\n\n\nD = Discriminator(z_dim=20, image_size=64)\n\nd_out = D(fake_img)\n\n# 출력 d_out에 Sigmoid를 곱해 0에서 1로 변환\nprint(nn.Sigmoid()(d_out))\n\ntensor([[[[0.4980]]]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\n\n\n\nmini_batch_size = 2\n\n# 정답 라벨 생성 -&gt; torch.tensor([1,1])\nlabel_real = torch.full((mini_batch_size,), 1)\n\n# 가짜 라벨 생성 -&gt; torch.tensor([0,0])\nlabel_fake = torch.full((mini_batch_size,), 0)\n\n\n# 가짜 이미지 생성\ninput_z = torch.randn(mini_batch_size, 20)\n# tensor size -&gt; (2,20,1,1)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1) \n# G에 의해 tensor size -&gt; (2,1,64,64)\nfake_images = G(input_z)\n# D에 의해 tensor size -&gt; (2,1,1,1)\nd_out_fake = D(fake_images)\n\n\n\n\\(maximize\\) \\(log(D(x))\\)+\\(log(1 - D(G(z)))\\)\n# loss function 정의\nloss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n\n# 진짜 이미지 판정\nd_out_real = D(x)\n\n# 가짜 이미지 생성\ninput_z = torch.randn(mini_batch_size, 20)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1) \nfake_images = G(input_z)\nd_out_fake = D(fake_images)\n\n# 오차를 계산\nd_loss_real = loss_fn(d_out_real.view(-1), label_real)\nd_loss_fake = loss_fn(d_out_fake.view(-1), label_fake)\nd_loss = d_loss_real + d_loss_fake\n\n\n\n\\(maximize\\) \\(log\\)\\((D(G(z)))\\)\n# 가짜 화상을 생성해 판정\ninput_z = torch.randn(mini_batch_size, 20)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\nfake_images = G(input_z)\nd_out_fake = D(fake_images)\n\n# 오차를 계산\ng_loss = criterion(d_out_fake.view(-1), label_real)\n\n\n\n\n\n\n\nimport os\nimport urllib.request\nimport zipfile\nimport tarfile\n\n\nfrom sklearn.datasets import fetch_openml\n\nmnist_F = fetch_openml('mnist_784', version=1, data_home=\"./data/\")  \n\n\n\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', version=1, data_home=\"./data/\")  \n\n\nX = mnist.data.to_numpy()\ny = mnist.target.to_numpy()\n\n\ndata_dir_path = \"./data/img_78/\"\nif not os.path.exists(data_dir_path):\n    os.mkdir(data_dir_path)\n\n\n# MNIST에서 숫자7, 8의 화상만 \"img_78\" 폴더에 화상으로 저장해 나간다\ncount7=0\ncount8=0\nmax_num=200  # 화상은 200장씩 작성한다\n\nfor i in range(len(X)):\n    \n    # 화상7 작성\n    if (y[i] is \"7\") and (count7&lt;max_num):\n        file_path=\"./data/img_78/img_7_\"+str(count7)+\".jpg\"\n        im_f=(X[i].reshape(28, 28))  # 화상을 28×28의 형태로 변경\n        pil_img_f = Image.fromarray(im_f.astype(np.uint8))  # 화상을 PIL으로\n        pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\n        pil_img_f.save(file_path)  # 저장\n        count7+=1 \n    \n    # 화상8 작성\n    if (y[i] is \"8\") and (count8&lt;max_num):\n        file_path=\"./data/img_78/img_8_\"+str(count8)+\".jpg\"\n        im_f=(X[i].reshape(28, 28))  # 화상을 28×28의 형태로 변경\n        pil_img_f = Image.fromarray(im_f.astype(np.uint8))  # 화상을 PIL으로\n        pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\n        pil_img_f.save(file_path)  # 저장\n        count8+=1 \n\n&lt;&gt;:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n&lt;&gt;:18: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n&lt;&gt;:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n&lt;&gt;:18: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if (y[i] is \"7\") and (count7&lt;max_num):\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:18: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if (y[i] is \"8\") and (count8&lt;max_num):\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:13: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n  pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:22: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n  pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\n\n\n\ndef make_datapath_list():\n    train_img_list = list() \n    for img_idx in range(200):\n        img_path = \"./data/img_78/img_7_\" + str(img_idx)+'.jpg'\n        train_img_list.append(img_path)\n\n        img_path = \"./data/img_78/img_8_\" + str(img_idx)+'.jpg'\n        train_img_list.append(img_path)\n\n    return train_img_list\n\n\n\n\n\n\nclass ImageTransform():\n    \"\"\"이미지 전처리 클래스\"\"\"\n\n    def __init__(self, mean, std):\n        self.data_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n\n    def __call__(self, img):\n        return self.data_transform(img)\n\n\nclass GAN_Img_Dataset(data.Dataset):\n    \"\"\"Dataset 클래스. PyTorch의 Dataset 클래스를 상속\"\"\"\n\n    def __init__(self, file_list, transform):\n        self.file_list = file_list\n        self.transform = transform\n\n    def __len__(self):\n        '''이미지 개수 반환'''\n        return len(self.file_list)\n\n    def __getitem__(self, index):\n        '''전처리된 이미지를 Tensor 형식 데이터로 변환'''\n\n        img_path = self.file_list[index]\n        img = Image.open(img_path)  # [높이][폭]흑백\n\n        # 이미지 전처리\n        img_transformed = self.transform(img)\n        img_transformed = img_transformed.type(torch.FloatTensor)\n        return img_transformed\n\n\n# DataLoader 작성과 동작 확인\n\n# 파일 리스트를 작성\ntrain_img_list=make_datapath_list()\n\n# Dataset 작성\nmean = (0.5,)\nstd = (0.5,)\ntrain_dataset = GAN_Img_Dataset(file_list=train_img_list, transform=ImageTransform(mean, std))\n\n# DataLoader 작성\nbatch_size = 64\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True)\n\n# 동작 확인\nbatch_iterator = iter(train_dataloader)  # 반복자로 변환\nimges = next(batch_iterator)  # 1번째 요소를 꺼낸다\nprint(imges.size())  # torch.Size([64, 1, 64, 64])\n\ntorch.Size([64, 1, 64, 64])\n\n\n\n\n\n\n\n# 네트워크 초기화\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        # Conv2dとConvTranspose2d 초기화\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n    elif classname.find('BatchNorm') != -1:\n        # BatchNorm2d 초기화\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# 초기화 실시\nG.apply(weights_init)\nD.apply(weights_init)\n\nprint(\"네트워크 초기화 완료\")\n\n네트워크 초기화 완료\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");device\n\ndevice(type='cuda', index=0)\n\n\n\n# 모델을 학습시키는 함수를 작성\ndef train_model(G, D, dataloader, num_epochs):\n\n    # GPU가 사용 가능한지 확인\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"사용 장치: \", device)\n\n    # 최적화 기법 설정\n    g_lr, d_lr = 0.0001, 0.0004\n    beta1, beta2 = 0.0, 0.9\n    g_optimizer = torch.optim.Adam(G.parameters(), g_lr, [beta1, beta2])\n    d_optimizer = torch.optim.Adam(D.parameters(), d_lr, [beta1, beta2])\n\n    # 오차함수 정의\n    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n\n    # 파라미터를 하드코딩\n    z_dim = 20\n    mini_batch_size = 64\n\n    # 네트워크를 GPU로\n    G.to(device)\n    D.to(device)\n\n    G.train()  # 모델을 훈련 모드로\n    D.train()  # 모델을 훈련 모드로\n\n    # 네트워크가 어느 정도 고정되면, 고속화시킨다\n    torch.backends.cudnn.benchmark = True\n\n    num_train_imgs = len(dataloader.dataset)\n    batch_size = dataloader.batch_size\n\n    # 반복 카운터 설정\n    iteration = 1\n    logs = []\n\n    # epoch 루프\n    for epoch in range(num_epochs):\n\n        # 개시 시간을 저장\n        t_epoch_start = time.time()\n        epoch_g_loss = 0.0  # epoch의 손실합\n        epoch_d_loss = 0.0  # epoch의 손실합\n\n        print('-------------')\n        print('Epoch {}/{}'.format(epoch, num_epochs))\n        print('-------------')\n        print('(train)')\n\n        # 데이터 로더에서 minibatch씩 꺼내는 루프\n        for imges in dataloader:\n\n            # --------------------\n            # 1. Discriminator 학습\n            # --------------------\n            # 미니 배치 크기가 1이면, 배치 노멀라이제이션에서 에러가 발생하므로 피한다\n            if imges.size()[0] == 1:\n                continue\n\n            # GPU가 사용 가능하면 GPU로 데이터를 보낸다\n            imges = imges.to(device)\n\n            # 정답 라벨과 가짜 라벨 작성\n            # epoch의 마지막 반복은 미니 배치 수가 줄어든다\n            mini_batch_size = imges.size()[0]\n            label_real = torch.full((mini_batch_size,), 1).to(device)\n            label_fake = torch.full((mini_batch_size,), 0).to(device)\n\n            # 진짜 이미지 판정\n            d_out_real = D(imges)\n\n            # 가짜 이미지 생성해 판정\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1).to(device)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images.to(device))\n\n            # 오차를 계산\n            d_loss_real = criterion(d_out_real.view(-1).to(device), label_real.float())\n            d_loss_fake = criterion(d_out_fake.view(-1).to(device), label_fake.float())\n            d_loss = d_loss_real + d_loss_fake\n\n            # 역전파\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n\n            d_loss.backward()\n            d_optimizer.step()\n\n            # 2. Generator 학습\n            # --------------------\n            # 가짜 이미지 생성해 판정\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1).to(device)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images.to(device))\n\n            # 오차를 계산\n            g_loss = criterion(d_out_fake.view(-1).to(device), label_real.float().to(device))\n\n            # 역전파\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n            g_loss.backward()\n            g_optimizer.step()\n\n            # --------------------\n            # 3. 기록\n            # --------------------\n            epoch_d_loss += d_loss.item()\n            epoch_g_loss += g_loss.item()\n            iteration += 1\n\n        # epoch의 phase별 loss와 정답률\n        t_epoch_finish = time.time()\n        print('-------------')\n        print('epoch {} || Epoch_D_Loss:{:.4f} ||Epoch_G_Loss:{:.4f}'.format(\n            epoch, epoch_d_loss/batch_size, epoch_g_loss/batch_size))\n        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n        t_epoch_start = time.time()\n\n    return G, D\n\n\nnum_epochs = 200\nG_update, D_update = train_model(G, D, dataloader=train_dataloader, num_epochs=num_epochs)\n\n사용 장치:  cuda:0\n-------------\nEpoch 0/200\n-------------\n(train)\n-------------\nepoch 0 || Epoch_D_Loss:0.0147 ||Epoch_G_Loss:0.7212\ntimer:  0.9429 sec.\n-------------\nEpoch 1/200\n-------------\n(train)\n-------------\nepoch 1 || Epoch_D_Loss:0.0108 ||Epoch_G_Loss:0.8667\ntimer:  0.7360 sec.\n-------------\nEpoch 2/200\n-------------\n(train)\n-------------\nepoch 2 || Epoch_D_Loss:0.0266 ||Epoch_G_Loss:0.6338\ntimer:  0.7420 sec.\n-------------\nEpoch 3/200\n-------------\n(train)\n-------------\nepoch 3 || Epoch_D_Loss:0.0029 ||Epoch_G_Loss:0.7435\ntimer:  0.7325 sec.\n-------------\nEpoch 4/200\n-------------\n(train)\n-------------\nepoch 4 || Epoch_D_Loss:0.0038 ||Epoch_G_Loss:0.7857\ntimer:  0.7349 sec.\n-------------\nEpoch 5/200\n-------------\n(train)\n-------------\nepoch 5 || Epoch_D_Loss:0.0128 ||Epoch_G_Loss:0.7397\ntimer:  0.7323 sec.\n-------------\nEpoch 6/200\n-------------\n(train)\n-------------\nepoch 6 || Epoch_D_Loss:0.0058 ||Epoch_G_Loss:0.9182\ntimer:  0.7353 sec.\n-------------\nEpoch 7/200\n-------------\n(train)\n-------------\nepoch 7 || Epoch_D_Loss:0.0561 ||Epoch_G_Loss:0.6317\ntimer:  0.7301 sec.\n-------------\nEpoch 8/200\n-------------\n(train)\n-------------\nepoch 8 || Epoch_D_Loss:0.0046 ||Epoch_G_Loss:0.7038\ntimer:  0.7350 sec.\n-------------\nEpoch 9/200\n-------------\n(train)\n-------------\nepoch 9 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.7608\ntimer:  0.7320 sec.\n-------------\nEpoch 10/200\n-------------\n(train)\n-------------\nepoch 10 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.7988\ntimer:  0.7388 sec.\n-------------\nEpoch 11/200\n-------------\n(train)\n-------------\nepoch 11 || Epoch_D_Loss:0.0035 ||Epoch_G_Loss:0.8410\ntimer:  0.7402 sec.\n-------------\nEpoch 12/200\n-------------\n(train)\n-------------\nepoch 12 || Epoch_D_Loss:0.0554 ||Epoch_G_Loss:0.9072\ntimer:  0.7309 sec.\n-------------\nEpoch 13/200\n-------------\n(train)\n-------------\nepoch 13 || Epoch_D_Loss:0.0054 ||Epoch_G_Loss:0.6871\ntimer:  0.7408 sec.\n-------------\nEpoch 14/200\n-------------\n(train)\n-------------\nepoch 14 || Epoch_D_Loss:0.0027 ||Epoch_G_Loss:0.8104\ntimer:  0.7420 sec.\n-------------\nEpoch 15/200\n-------------\n(train)\n-------------\nepoch 15 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8349\ntimer:  0.7335 sec.\n-------------\nEpoch 16/200\n-------------\n(train)\n-------------\nepoch 16 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8269\ntimer:  0.7387 sec.\n-------------\nEpoch 17/200\n-------------\n(train)\n-------------\nepoch 17 || Epoch_D_Loss:0.0113 ||Epoch_G_Loss:0.9406\ntimer:  0.7346 sec.\n-------------\nEpoch 18/200\n-------------\n(train)\n-------------\nepoch 18 || Epoch_D_Loss:0.0484 ||Epoch_G_Loss:0.7943\ntimer:  0.7341 sec.\n-------------\nEpoch 19/200\n-------------\n(train)\n-------------\nepoch 19 || Epoch_D_Loss:0.0038 ||Epoch_G_Loss:0.7086\ntimer:  0.7367 sec.\n-------------\nEpoch 20/200\n-------------\n(train)\n-------------\nepoch 20 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8174\ntimer:  0.7461 sec.\n-------------\nEpoch 21/200\n-------------\n(train)\n-------------\nepoch 21 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.9420\ntimer:  0.7323 sec.\n-------------\nEpoch 22/200\n-------------\n(train)\n-------------\nepoch 22 || Epoch_D_Loss:0.0253 ||Epoch_G_Loss:0.7721\ntimer:  0.7278 sec.\n-------------\nEpoch 23/200\n-------------\n(train)\n-------------\nepoch 23 || Epoch_D_Loss:0.0093 ||Epoch_G_Loss:0.7429\ntimer:  0.7256 sec.\n-------------\nEpoch 24/200\n-------------\n(train)\n-------------\nepoch 24 || Epoch_D_Loss:0.0123 ||Epoch_G_Loss:0.8130\ntimer:  0.7300 sec.\n-------------\nEpoch 25/200\n-------------\n(train)\n-------------\nepoch 25 || Epoch_D_Loss:0.0103 ||Epoch_G_Loss:0.8479\ntimer:  0.7365 sec.\n-------------\nEpoch 26/200\n-------------\n(train)\n-------------\nepoch 26 || Epoch_D_Loss:0.0015 ||Epoch_G_Loss:0.8154\ntimer:  0.7331 sec.\n-------------\nEpoch 27/200\n-------------\n(train)\n-------------\nepoch 27 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8887\ntimer:  0.7366 sec.\n-------------\nEpoch 28/200\n-------------\n(train)\n-------------\nepoch 28 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:1.0104\ntimer:  0.7301 sec.\n-------------\nEpoch 29/200\n-------------\n(train)\n-------------\nepoch 29 || Epoch_D_Loss:0.0665 ||Epoch_G_Loss:0.6415\ntimer:  0.7291 sec.\n-------------\nEpoch 30/200\n-------------\n(train)\n-------------\nepoch 30 || Epoch_D_Loss:0.0060 ||Epoch_G_Loss:0.7702\ntimer:  0.7330 sec.\n-------------\nEpoch 31/200\n-------------\n(train)\n-------------\nepoch 31 || Epoch_D_Loss:0.0018 ||Epoch_G_Loss:0.8099\ntimer:  0.7369 sec.\n-------------\nEpoch 32/200\n-------------\n(train)\n-------------\nepoch 32 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8387\ntimer:  0.7320 sec.\n-------------\nEpoch 33/200\n-------------\n(train)\n-------------\nepoch 33 || Epoch_D_Loss:0.0116 ||Epoch_G_Loss:0.9780\ntimer:  0.7371 sec.\n-------------\nEpoch 34/200\n-------------\n(train)\n-------------\nepoch 34 || Epoch_D_Loss:0.0895 ||Epoch_G_Loss:0.6259\ntimer:  0.7390 sec.\n-------------\nEpoch 35/200\n-------------\n(train)\n-------------\nepoch 35 || Epoch_D_Loss:0.0040 ||Epoch_G_Loss:0.7130\ntimer:  0.7300 sec.\n-------------\nEpoch 36/200\n-------------\n(train)\n-------------\nepoch 36 || Epoch_D_Loss:0.0033 ||Epoch_G_Loss:0.7479\ntimer:  0.7291 sec.\n-------------\nEpoch 37/200\n-------------\n(train)\n-------------\nepoch 37 || Epoch_D_Loss:0.0019 ||Epoch_G_Loss:0.7853\ntimer:  0.7266 sec.\n-------------\nEpoch 38/200\n-------------\n(train)\n-------------\nepoch 38 || Epoch_D_Loss:0.0034 ||Epoch_G_Loss:0.7967\ntimer:  0.7288 sec.\n-------------\nEpoch 39/200\n-------------\n(train)\n-------------\nepoch 39 || Epoch_D_Loss:0.0376 ||Epoch_G_Loss:0.9572\ntimer:  0.7312 sec.\n-------------\nEpoch 40/200\n-------------\n(train)\n-------------\nepoch 40 || Epoch_D_Loss:0.0112 ||Epoch_G_Loss:0.7412\ntimer:  0.7272 sec.\n-------------\nEpoch 41/200\n-------------\n(train)\n-------------\nepoch 41 || Epoch_D_Loss:0.0022 ||Epoch_G_Loss:0.8254\ntimer:  0.7251 sec.\n-------------\nEpoch 42/200\n-------------\n(train)\n-------------\nepoch 42 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8413\ntimer:  0.7251 sec.\n-------------\nEpoch 43/200\n-------------\n(train)\n-------------\nepoch 43 || Epoch_D_Loss:0.0007 ||Epoch_G_Loss:0.8888\ntimer:  0.7370 sec.\n-------------\nEpoch 44/200\n-------------\n(train)\n-------------\nepoch 44 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.9397\ntimer:  0.7321 sec.\n-------------\nEpoch 45/200\n-------------\n(train)\n-------------\nepoch 45 || Epoch_D_Loss:0.0589 ||Epoch_G_Loss:0.7371\ntimer:  0.7245 sec.\n-------------\nEpoch 46/200\n-------------\n(train)\n-------------\nepoch 46 || Epoch_D_Loss:0.0032 ||Epoch_G_Loss:0.7395\ntimer:  0.7276 sec.\n-------------\nEpoch 47/200\n-------------\n(train)\n-------------\nepoch 47 || Epoch_D_Loss:0.0037 ||Epoch_G_Loss:0.8705\ntimer:  0.7277 sec.\n-------------\nEpoch 48/200\n-------------\n(train)\n-------------\nepoch 48 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.8553\ntimer:  0.7288 sec.\n-------------\nEpoch 49/200\n-------------\n(train)\n-------------\nepoch 49 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.8959\ntimer:  0.7374 sec.\n-------------\nEpoch 50/200\n-------------\n(train)\n-------------\nepoch 50 || Epoch_D_Loss:0.0429 ||Epoch_G_Loss:0.9181\ntimer:  0.7258 sec.\n-------------\nEpoch 51/200\n-------------\n(train)\n-------------\nepoch 51 || Epoch_D_Loss:0.0030 ||Epoch_G_Loss:0.7838\ntimer:  0.7300 sec.\n-------------\nEpoch 52/200\n-------------\n(train)\n-------------\nepoch 52 || Epoch_D_Loss:0.0032 ||Epoch_G_Loss:0.8647\ntimer:  0.7330 sec.\n-------------\nEpoch 53/200\n-------------\n(train)\n-------------\nepoch 53 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.8791\ntimer:  0.7306 sec.\n-------------\nEpoch 54/200\n-------------\n(train)\n-------------\nepoch 54 || Epoch_D_Loss:0.0362 ||Epoch_G_Loss:0.9293\ntimer:  0.7291 sec.\n-------------\nEpoch 55/200\n-------------\n(train)\n-------------\nepoch 55 || Epoch_D_Loss:0.0123 ||Epoch_G_Loss:0.7459\ntimer:  0.7313 sec.\n-------------\nEpoch 56/200\n-------------\n(train)\n-------------\nepoch 56 || Epoch_D_Loss:0.0033 ||Epoch_G_Loss:0.8016\ntimer:  0.7408 sec.\n-------------\nEpoch 57/200\n-------------\n(train)\n-------------\nepoch 57 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8647\ntimer:  0.7294 sec.\n-------------\nEpoch 58/200\n-------------\n(train)\n-------------\nepoch 58 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8626\ntimer:  0.7275 sec.\n-------------\nEpoch 59/200\n-------------\n(train)\n-------------\nepoch 59 || Epoch_D_Loss:0.0625 ||Epoch_G_Loss:0.9511\ntimer:  0.7284 sec.\n-------------\nEpoch 60/200\n-------------\n(train)\n-------------\nepoch 60 || Epoch_D_Loss:0.0097 ||Epoch_G_Loss:0.7070\ntimer:  0.7263 sec.\n-------------\nEpoch 61/200\n-------------\n(train)\n-------------\nepoch 61 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.7738\ntimer:  0.7272 sec.\n-------------\nEpoch 62/200\n-------------\n(train)\n-------------\nepoch 62 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.7819\ntimer:  0.7280 sec.\n-------------\nEpoch 63/200\n-------------\n(train)\n-------------\nepoch 63 || Epoch_D_Loss:0.0015 ||Epoch_G_Loss:0.8568\ntimer:  0.7297 sec.\n-------------\nEpoch 64/200\n-------------\n(train)\n-------------\nepoch 64 || Epoch_D_Loss:0.0375 ||Epoch_G_Loss:0.8728\ntimer:  0.7267 sec.\n-------------\nEpoch 65/200\n-------------\n(train)\n-------------\nepoch 65 || Epoch_D_Loss:0.0063 ||Epoch_G_Loss:0.6889\ntimer:  0.7381 sec.\n-------------\nEpoch 66/200\n-------------\n(train)\n-------------\nepoch 66 || Epoch_D_Loss:0.0038 ||Epoch_G_Loss:0.8209\ntimer:  0.7297 sec.\n-------------\nEpoch 67/200\n-------------\n(train)\n-------------\nepoch 67 || Epoch_D_Loss:0.0022 ||Epoch_G_Loss:0.8709\ntimer:  0.7559 sec.\n-------------\nEpoch 68/200\n-------------\n(train)\n-------------\nepoch 68 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.8871\ntimer:  0.7298 sec.\n-------------\nEpoch 69/200\n-------------\n(train)\n-------------\nepoch 69 || Epoch_D_Loss:0.0414 ||Epoch_G_Loss:0.9295\ntimer:  0.7275 sec.\n-------------\nEpoch 70/200\n-------------\n(train)\n-------------\nepoch 70 || Epoch_D_Loss:0.0088 ||Epoch_G_Loss:0.7318\ntimer:  0.7226 sec.\n-------------\nEpoch 71/200\n-------------\n(train)\n-------------\nepoch 71 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8082\ntimer:  0.7209 sec.\n-------------\nEpoch 72/200\n-------------\n(train)\n-------------\nepoch 72 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.8387\ntimer:  0.7196 sec.\n-------------\nEpoch 73/200\n-------------\n(train)\n-------------\nepoch 73 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.9041\ntimer:  0.7261 sec.\n-------------\nEpoch 74/200\n-------------\n(train)\n-------------\nepoch 74 || Epoch_D_Loss:0.0522 ||Epoch_G_Loss:0.9280\ntimer:  0.7300 sec.\n-------------\nEpoch 75/200\n-------------\n(train)\n-------------\nepoch 75 || Epoch_D_Loss:0.0056 ||Epoch_G_Loss:0.6952\ntimer:  0.7258 sec.\n-------------\nEpoch 76/200\n-------------\n(train)\n-------------\nepoch 76 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.8027\ntimer:  0.7219 sec.\n-------------\nEpoch 77/200\n-------------\n(train)\n-------------\nepoch 77 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.8502\ntimer:  0.7210 sec.\n-------------\nEpoch 78/200\n-------------\n(train)\n-------------\nepoch 78 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.8654\ntimer:  0.7208 sec.\n-------------\nEpoch 79/200\n-------------\n(train)\n-------------\nepoch 79 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.9063\ntimer:  0.7214 sec.\n-------------\nEpoch 80/200\n-------------\n(train)\n-------------\nepoch 80 || Epoch_D_Loss:0.0458 ||Epoch_G_Loss:0.9730\ntimer:  0.7253 sec.\n-------------\nEpoch 81/200\n-------------\n(train)\n-------------\nepoch 81 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.7860\ntimer:  0.7200 sec.\n-------------\nEpoch 82/200\n-------------\n(train)\n-------------\nepoch 82 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8159\ntimer:  0.7201 sec.\n-------------\nEpoch 83/200\n-------------\n(train)\n-------------\nepoch 83 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.8737\ntimer:  0.7207 sec.\n-------------\nEpoch 84/200\n-------------\n(train)\n-------------\nepoch 84 || Epoch_D_Loss:0.0444 ||Epoch_G_Loss:0.8085\ntimer:  0.7164 sec.\n-------------\nEpoch 85/200\n-------------\n(train)\n-------------\nepoch 85 || Epoch_D_Loss:0.0100 ||Epoch_G_Loss:0.7245\ntimer:  0.7192 sec.\n-------------\nEpoch 86/200\n-------------\n(train)\n-------------\nepoch 86 || Epoch_D_Loss:0.0019 ||Epoch_G_Loss:0.7648\ntimer:  0.7202 sec.\n-------------\nEpoch 87/200\n-------------\n(train)\n-------------\nepoch 87 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8417\ntimer:  0.7172 sec.\n-------------\nEpoch 88/200\n-------------\n(train)\n-------------\nepoch 88 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8620\ntimer:  0.7171 sec.\n-------------\nEpoch 89/200\n-------------\n(train)\n-------------\nepoch 89 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.9353\ntimer:  0.7189 sec.\n-------------\nEpoch 90/200\n-------------\n(train)\n-------------\nepoch 90 || Epoch_D_Loss:0.0538 ||Epoch_G_Loss:0.9575\ntimer:  0.7199 sec.\n-------------\nEpoch 91/200\n-------------\n(train)\n-------------\nepoch 91 || Epoch_D_Loss:0.0036 ||Epoch_G_Loss:0.7752\ntimer:  0.7231 sec.\n-------------\nEpoch 92/200\n-------------\n(train)\n-------------\nepoch 92 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.8001\ntimer:  0.7212 sec.\n-------------\nEpoch 93/200\n-------------\n(train)\n-------------\nepoch 93 || Epoch_D_Loss:0.0029 ||Epoch_G_Loss:0.9208\ntimer:  0.7198 sec.\n-------------\nEpoch 94/200\n-------------\n(train)\n-------------\nepoch 94 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.9002\ntimer:  0.7233 sec.\n-------------\nEpoch 95/200\n-------------\n(train)\n-------------\nepoch 95 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.8902\ntimer:  0.7227 sec.\n-------------\nEpoch 96/200\n-------------\n(train)\n-------------\nepoch 96 || Epoch_D_Loss:0.0467 ||Epoch_G_Loss:0.8899\ntimer:  0.7380 sec.\n-------------\nEpoch 97/200\n-------------\n(train)\n-------------\nepoch 97 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.7825\ntimer:  0.7333 sec.\n-------------\nEpoch 98/200\n-------------\n(train)\n-------------\nepoch 98 || Epoch_D_Loss:0.0037 ||Epoch_G_Loss:0.8136\ntimer:  0.7191 sec.\n-------------\nEpoch 99/200\n-------------\n(train)\n-------------\nepoch 99 || Epoch_D_Loss:0.0096 ||Epoch_G_Loss:0.9419\ntimer:  0.7208 sec.\n-------------\nEpoch 100/200\n-------------\n(train)\n-------------\nepoch 100 || Epoch_D_Loss:0.0014 ||Epoch_G_Loss:0.8698\ntimer:  0.7468 sec.\n-------------\nEpoch 101/200\n-------------\n(train)\n-------------\nepoch 101 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.9751\ntimer:  0.7229 sec.\n-------------\nEpoch 102/200\n-------------\n(train)\n-------------\nepoch 102 || Epoch_D_Loss:0.0005 ||Epoch_G_Loss:0.9452\ntimer:  0.7198 sec.\n-------------\nEpoch 103/200\n-------------\n(train)\n-------------\nepoch 103 || Epoch_D_Loss:0.0007 ||Epoch_G_Loss:0.9833\ntimer:  0.7279 sec.\n-------------\nEpoch 104/200\n-------------\n(train)\n-------------\nepoch 104 || Epoch_D_Loss:0.1555 ||Epoch_G_Loss:1.0839\ntimer:  0.7222 sec.\n-------------\nEpoch 105/200\n-------------\n(train)\n-------------\nepoch 105 || Epoch_D_Loss:0.0103 ||Epoch_G_Loss:0.6298\ntimer:  0.7233 sec.\n-------------\nEpoch 106/200\n-------------\n(train)\n-------------\nepoch 106 || Epoch_D_Loss:0.0061 ||Epoch_G_Loss:0.7351\ntimer:  0.7223 sec.\n-------------\nEpoch 107/200\n-------------\n(train)\n-------------\nepoch 107 || Epoch_D_Loss:0.0036 ||Epoch_G_Loss:0.7819\ntimer:  0.7340 sec.\n-------------\nEpoch 108/200\n-------------\n(train)\n-------------\nepoch 108 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.8024\ntimer:  0.7279 sec.\n-------------\nEpoch 109/200\n-------------\n(train)\n-------------\nepoch 109 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.9204\ntimer:  0.7284 sec.\n-------------\nEpoch 110/200\n-------------\n(train)\n-------------\nepoch 110 || Epoch_D_Loss:0.0069 ||Epoch_G_Loss:1.1190\ntimer:  0.7254 sec.\n-------------\nEpoch 111/200\n-------------\n(train)\n-------------\nepoch 111 || Epoch_D_Loss:0.0006 ||Epoch_G_Loss:1.0324\ntimer:  0.7267 sec.\n-------------\nEpoch 112/200\n-------------\n(train)\n-------------\nepoch 112 || Epoch_D_Loss:0.0014 ||Epoch_G_Loss:0.9086\ntimer:  0.7268 sec.\n-------------\nEpoch 113/200\n-------------\n(train)\n-------------\nepoch 113 || Epoch_D_Loss:0.0014 ||Epoch_G_Loss:0.9236\ntimer:  0.7270 sec.\n-------------\nEpoch 114/200\n-------------\n(train)\n-------------\nepoch 114 || Epoch_D_Loss:0.0561 ||Epoch_G_Loss:0.9581\ntimer:  0.7281 sec.\n-------------\nEpoch 115/200\n-------------\n(train)\n-------------\nepoch 115 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.7752\ntimer:  0.7249 sec.\n-------------\nEpoch 116/200\n-------------\n(train)\n-------------\nepoch 116 || Epoch_D_Loss:0.0021 ||Epoch_G_Loss:0.8565\ntimer:  0.7257 sec.\n-------------\nEpoch 117/200\n-------------\n(train)\n-------------\nepoch 117 || Epoch_D_Loss:0.0317 ||Epoch_G_Loss:0.8358\ntimer:  0.7263 sec.\n-------------\nEpoch 118/200\n-------------\n(train)\n-------------\nepoch 118 || Epoch_D_Loss:0.0104 ||Epoch_G_Loss:0.7355\ntimer:  0.7370 sec.\n-------------\nEpoch 119/200\n-------------\n(train)\n-------------\nepoch 119 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.8858\ntimer:  0.7332 sec.\n-------------\nEpoch 120/200\n-------------\n(train)\n-------------\nepoch 120 || Epoch_D_Loss:0.0337 ||Epoch_G_Loss:0.7324\ntimer:  0.7300 sec.\n-------------\nEpoch 121/200\n-------------\n(train)\n-------------\nepoch 121 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.7945\ntimer:  0.7221 sec.\n-------------\nEpoch 122/200\n-------------\n(train)\n-------------\nepoch 122 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.8590\ntimer:  0.7202 sec.\n-------------\nEpoch 123/200\n-------------\n(train)\n-------------\nepoch 123 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8890\ntimer:  0.7228 sec.\n-------------\nEpoch 124/200\n-------------\n(train)\n-------------\nepoch 124 || Epoch_D_Loss:0.0006 ||Epoch_G_Loss:0.8837\ntimer:  0.7254 sec.\n-------------\nEpoch 125/200\n-------------\n(train)\n-------------\nepoch 125 || Epoch_D_Loss:0.0068 ||Epoch_G_Loss:0.9388\ntimer:  0.7226 sec.\n-------------\nEpoch 126/200\n-------------\n(train)\n-------------\nepoch 126 || Epoch_D_Loss:0.0567 ||Epoch_G_Loss:0.6943\ntimer:  0.7223 sec.\n-------------\nEpoch 127/200\n-------------\n(train)\n-------------\nepoch 127 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.7448\ntimer:  0.7190 sec.\n-------------\nEpoch 128/200\n-------------\n(train)\n-------------\nepoch 128 || Epoch_D_Loss:0.0019 ||Epoch_G_Loss:0.8421\ntimer:  0.7241 sec.\n-------------\nEpoch 129/200\n-------------\n(train)\n-------------\nepoch 129 || Epoch_D_Loss:0.0044 ||Epoch_G_Loss:0.9485\ntimer:  0.7198 sec.\n-------------\nEpoch 130/200\n-------------\n(train)\n-------------\nepoch 130 || Epoch_D_Loss:0.0442 ||Epoch_G_Loss:1.0138\ntimer:  0.7290 sec.\n-------------\nEpoch 131/200\n-------------\n(train)\n-------------\nepoch 131 || Epoch_D_Loss:0.0076 ||Epoch_G_Loss:0.7135\ntimer:  0.7240 sec.\n-------------\nEpoch 132/200\n-------------\n(train)\n-------------\nepoch 132 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.7633\ntimer:  0.7216 sec.\n-------------\nEpoch 133/200\n-------------\n(train)\n-------------\nepoch 133 || Epoch_D_Loss:0.0015 ||Epoch_G_Loss:0.8278\ntimer:  0.7209 sec.\n-------------\nEpoch 134/200\n-------------\n(train)\n-------------\nepoch 134 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.9835\ntimer:  0.7225 sec.\n-------------\nEpoch 135/200\n-------------\n(train)\n-------------\nepoch 135 || Epoch_D_Loss:0.0004 ||Epoch_G_Loss:0.9521\ntimer:  0.7191 sec.\n-------------\nEpoch 136/200\n-------------\n(train)\n-------------\nepoch 136 || Epoch_D_Loss:0.0004 ||Epoch_G_Loss:1.0310\ntimer:  0.7201 sec.\n-------------\nEpoch 137/200\n-------------\n(train)\n-------------\nepoch 137 || Epoch_D_Loss:0.0633 ||Epoch_G_Loss:1.0482\ntimer:  0.7257 sec.\n-------------\nEpoch 138/200\n-------------\n(train)\n-------------\nepoch 138 || Epoch_D_Loss:0.0215 ||Epoch_G_Loss:0.9015\ntimer:  0.7233 sec.\n-------------\nEpoch 139/200\n-------------\n(train)\n-------------\nepoch 139 || Epoch_D_Loss:0.0041 ||Epoch_G_Loss:0.8207\ntimer:  0.7242 sec.\n-------------\nEpoch 140/200\n-------------\n(train)\n-------------\nepoch 140 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8500\ntimer:  0.7291 sec.\n-------------\nEpoch 141/200\n-------------\n(train)\n-------------\nepoch 141 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.8946\ntimer:  0.7330 sec.\n-------------\nEpoch 142/200\n-------------\n(train)\n-------------\nepoch 142 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.8823\ntimer:  0.7250 sec.\n-------------\nEpoch 143/200\n-------------\n(train)\n-------------\nepoch 143 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:1.0055\ntimer:  0.7228 sec.\n-------------\nEpoch 144/200\n-------------\n(train)\n-------------\nepoch 144 || Epoch_D_Loss:0.0616 ||Epoch_G_Loss:0.9888\ntimer:  0.7235 sec.\n-------------\nEpoch 145/200\n-------------\n(train)\n-------------\nepoch 145 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8180\ntimer:  0.7214 sec.\n-------------\nEpoch 146/200\n-------------\n(train)\n-------------\nepoch 146 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8650\ntimer:  0.7242 sec.\n-------------\nEpoch 147/200\n-------------\n(train)\n-------------\nepoch 147 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8336\ntimer:  0.7255 sec.\n-------------\nEpoch 148/200\n-------------\n(train)\n-------------\nepoch 148 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.9114\ntimer:  0.7221 sec.\n-------------\nEpoch 149/200\n-------------\n(train)\n-------------\nepoch 149 || Epoch_D_Loss:0.0314 ||Epoch_G_Loss:0.9916\ntimer:  0.7217 sec.\n-------------\nEpoch 150/200\n-------------\n(train)\n-------------\nepoch 150 || Epoch_D_Loss:0.0159 ||Epoch_G_Loss:0.7072\ntimer:  0.7261 sec.\n-------------\nEpoch 151/200\n-------------\n(train)\n-------------\nepoch 151 || Epoch_D_Loss:0.0018 ||Epoch_G_Loss:0.8146\ntimer:  0.7238 sec.\n-------------\nEpoch 152/200\n-------------\n(train)\n-------------\nepoch 152 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8835\ntimer:  0.7259 sec.\n-------------\nEpoch 153/200\n-------------\n(train)\n-------------\nepoch 153 || Epoch_D_Loss:0.0078 ||Epoch_G_Loss:1.0363\ntimer:  0.7235 sec.\n-------------\nEpoch 154/200\n-------------\n(train)\n-------------\nepoch 154 || Epoch_D_Loss:0.0333 ||Epoch_G_Loss:0.8053\ntimer:  0.7234 sec.\n-------------\nEpoch 155/200\n-------------\n(train)\n-------------\nepoch 155 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.8146\ntimer:  0.7232 sec.\n-------------\nEpoch 156/200\n-------------\n(train)\n-------------\nepoch 156 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.8763\ntimer:  0.7245 sec.\n-------------\nEpoch 157/200\n-------------\n(train)\n-------------\nepoch 157 || Epoch_D_Loss:0.0037 ||Epoch_G_Loss:0.8991\ntimer:  0.7251 sec.\n-------------\nEpoch 158/200\n-------------\n(train)\n-------------\nepoch 158 || Epoch_D_Loss:0.0352 ||Epoch_G_Loss:0.9261\ntimer:  0.7250 sec.\n-------------\nEpoch 159/200\n-------------\n(train)\n-------------\nepoch 159 || Epoch_D_Loss:0.0104 ||Epoch_G_Loss:0.7463\ntimer:  0.7217 sec.\n-------------\nEpoch 160/200\n-------------\n(train)\n-------------\nepoch 160 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.8617\ntimer:  0.7248 sec.\n-------------\nEpoch 161/200\n-------------\n(train)\n-------------\nepoch 161 || Epoch_D_Loss:0.0021 ||Epoch_G_Loss:0.9127\ntimer:  0.7300 sec.\n-------------\nEpoch 162/200\n-------------\n(train)\n-------------\nepoch 162 || Epoch_D_Loss:0.0187 ||Epoch_G_Loss:0.8745\ntimer:  0.7322 sec.\n-------------\nEpoch 163/200\n-------------\n(train)\n-------------\nepoch 163 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.8871\ntimer:  0.7360 sec.\n-------------\nEpoch 164/200\n-------------\n(train)\n-------------\nepoch 164 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.8840\ntimer:  0.7259 sec.\n-------------\nEpoch 165/200\n-------------\n(train)\n-------------\nepoch 165 || Epoch_D_Loss:0.0290 ||Epoch_G_Loss:0.9592\ntimer:  0.7324 sec.\n-------------\nEpoch 166/200\n-------------\n(train)\n-------------\nepoch 166 || Epoch_D_Loss:0.0095 ||Epoch_G_Loss:0.8147\ntimer:  0.7241 sec.\n-------------\nEpoch 167/200\n-------------\n(train)\n-------------\nepoch 167 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8852\ntimer:  0.7227 sec.\n-------------\nEpoch 168/200\n-------------\n(train)\n-------------\nepoch 168 || Epoch_D_Loss:0.0124 ||Epoch_G_Loss:0.8975\ntimer:  0.7256 sec.\n-------------\nEpoch 169/200\n-------------\n(train)\n-------------\nepoch 169 || Epoch_D_Loss:0.0127 ||Epoch_G_Loss:0.8617\ntimer:  0.7270 sec.\n-------------\nEpoch 170/200\n-------------\n(train)\n-------------\nepoch 170 || Epoch_D_Loss:0.0043 ||Epoch_G_Loss:0.8565\ntimer:  0.7238 sec.\n-------------\nEpoch 171/200\n-------------\n(train)\n-------------\nepoch 171 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.9055\ntimer:  0.7263 sec.\n-------------\nEpoch 172/200\n-------------\n(train)\n-------------\nepoch 172 || Epoch_D_Loss:0.0300 ||Epoch_G_Loss:0.9339\ntimer:  0.7229 sec.\n-------------\nEpoch 173/200\n-------------\n(train)\n-------------\nepoch 173 || Epoch_D_Loss:0.0323 ||Epoch_G_Loss:0.7781\ntimer:  0.7257 sec.\n-------------\nEpoch 174/200\n-------------\n(train)\n-------------\nepoch 174 || Epoch_D_Loss:0.0057 ||Epoch_G_Loss:0.7429\ntimer:  0.7237 sec.\n-------------\nEpoch 175/200\n-------------\n(train)\n-------------\nepoch 175 || Epoch_D_Loss:0.0027 ||Epoch_G_Loss:0.8056\ntimer:  0.7228 sec.\n-------------\nEpoch 176/200\n-------------\n(train)\n-------------\nepoch 176 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.7961\ntimer:  0.7236 sec.\n-------------\nEpoch 177/200\n-------------\n(train)\n-------------\nepoch 177 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8577\ntimer:  0.7254 sec.\n-------------\nEpoch 178/200\n-------------\n(train)\n-------------\nepoch 178 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:1.0004\ntimer:  0.7239 sec.\n-------------\nEpoch 179/200\n-------------\n(train)\n-------------\nepoch 179 || Epoch_D_Loss:0.0479 ||Epoch_G_Loss:0.8469\ntimer:  0.7232 sec.\n-------------\nEpoch 180/200\n-------------\n(train)\n-------------\nepoch 180 || Epoch_D_Loss:0.0031 ||Epoch_G_Loss:0.6928\ntimer:  0.7236 sec.\n-------------\nEpoch 181/200\n-------------\n(train)\n-------------\nepoch 181 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.8195\ntimer:  0.7271 sec.\n-------------\nEpoch 182/200\n-------------\n(train)\n-------------\nepoch 182 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.8560\ntimer:  0.7243 sec.\n-------------\nEpoch 183/200\n-------------\n(train)\n-------------\nepoch 183 || Epoch_D_Loss:0.0191 ||Epoch_G_Loss:1.0310\ntimer:  0.7217 sec.\n-------------\nEpoch 184/200\n-------------\n(train)\n-------------\nepoch 184 || Epoch_D_Loss:0.0085 ||Epoch_G_Loss:0.7468\ntimer:  0.7303 sec.\n-------------\nEpoch 185/200\n-------------\n(train)\n-------------\nepoch 185 || Epoch_D_Loss:0.0032 ||Epoch_G_Loss:0.9024\ntimer:  0.7321 sec.\n-------------\nEpoch 186/200\n-------------\n(train)\n-------------\nepoch 186 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8672\ntimer:  0.7283 sec.\n-------------\nEpoch 187/200\n-------------\n(train)\n-------------\nepoch 187 || Epoch_D_Loss:0.0476 ||Epoch_G_Loss:0.9672\ntimer:  0.7208 sec.\n-------------\nEpoch 188/200\n-------------\n(train)\n-------------\nepoch 188 || Epoch_D_Loss:0.0077 ||Epoch_G_Loss:0.7749\ntimer:  0.7255 sec.\n-------------\nEpoch 189/200\n-------------\n(train)\n-------------\nepoch 189 || Epoch_D_Loss:0.0045 ||Epoch_G_Loss:0.8037\ntimer:  0.7221 sec.\n-------------\nEpoch 190/200\n-------------\n(train)\n-------------\nepoch 190 || Epoch_D_Loss:0.0031 ||Epoch_G_Loss:0.8678\ntimer:  0.7244 sec.\n-------------\nEpoch 191/200\n-------------\n(train)\n-------------\nepoch 191 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.9078\ntimer:  0.7223 sec.\n-------------\nEpoch 192/200\n-------------\n(train)\n-------------\nepoch 192 || Epoch_D_Loss:0.0018 ||Epoch_G_Loss:1.0057\ntimer:  0.7271 sec.\n-------------\nEpoch 193/200\n-------------\n(train)\n-------------\nepoch 193 || Epoch_D_Loss:0.0322 ||Epoch_G_Loss:0.8226\ntimer:  0.7226 sec.\n-------------\nEpoch 194/200\n-------------\n(train)\n-------------\nepoch 194 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8305\ntimer:  0.7231 sec.\n-------------\nEpoch 195/200\n-------------\n(train)\n-------------\nepoch 195 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.9072\ntimer:  0.7225 sec.\n-------------\nEpoch 196/200\n-------------\n(train)\n-------------\nepoch 196 || Epoch_D_Loss:0.0049 ||Epoch_G_Loss:1.0098\ntimer:  0.7306 sec.\n-------------\nEpoch 197/200\n-------------\n(train)\n-------------\nepoch 197 || Epoch_D_Loss:0.0216 ||Epoch_G_Loss:0.9132\ntimer:  0.7397 sec.\n-------------\nEpoch 198/200\n-------------\n(train)\n-------------\nepoch 198 || Epoch_D_Loss:0.0172 ||Epoch_G_Loss:0.8279\ntimer:  0.7386 sec.\n-------------\nEpoch 199/200\n-------------\n(train)\n-------------\nepoch 199 || Epoch_D_Loss:0.0021 ||Epoch_G_Loss:0.8379\ntimer:  0.7345 sec.\n\n\n\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# 입력 난수\nbatch_size = 8\nz_dim = 20\nfixed_z = torch.randn(batch_size, z_dim)\nfixed_z = fixed_z.view(fixed_z.size(0), fixed_z.size(1), 1, 1)\n\n# 화상 생성\nG_update.eval()\nfake_images = G_update(fixed_z.to(device))\n\n# 훈련 데이터\nbatch_iterator = iter(train_dataloader)  # 반복자로 변환\nimges = next(batch_iterator)  # 1번째 요소를 꺼낸다\n\n\n# 출력\nfig = plt.figure(figsize=(15, 6))\nfor i in range(0, 5):\n    # 상단에 훈련 데이터를,\n    plt.subplot(2, 5, i+1)\n    plt.imshow(imges[i][0].cpu().detach().numpy(), 'gray')\n\n    # 하단에 생성 데이터를 표시한다\n    plt.subplot(2, 5, 5+i+1)\n    plt.imshow(fake_images[i][0].cpu().detach().numpy(), 'gray')"
  },
  {
    "objectID": "docs/DL/posts/DCGAN.html#논문-리뷰",
    "href": "docs/DL/posts/DCGAN.html#논문-리뷰",
    "title": "DCGAN paper review",
    "section": "",
    "text": "[DCGAN 장점] - 대부분의 상황에서 안정적으로 학습이 됨 - word2vec과 같이 DCGAN으로 학습된 Generator가 벡터 산술 연산이 가능한 성질을 갖음\n\n\nDCGAN이 학습한 필터를 시각화하여 보여줌 -&gt; 특정 필터들이 이미지의 특정 물체를 학습했음을 보여줌\n\n\n\n\nAlt text\n\n\n\n성능면에서 비지도 학습 알고리즘에 비해 우수함\n\n[APPROACH AND MODEL ARCHITECTURE]\nDCGAN은\n- Max Pooling To Strided Convolution - Fully-Connected Layer 삭제 - BatchNormalization을 추가함 -&gt; deep한 모델이더라도 gradient의 흐름이 잘 전달됨 - ReLU와 Leaky ReLU를 사용\n[DETAILS OF ADVERSARIAL TRAINING]\n\n모델 및 옵티마이저\n\n\nmini-batch Stochastic Gradient Descent(SGD) a with batch size of 128\nAll weight: zero-centered Normal distribution with std 0.02\nLeaky ReLU: slope 0.2\nOptimizer: Adam (GAN에서는 momentum 사용)\nlearning late: 0.0002(0.001은 너무 커서..)\nD’s criterion= \\(\\log(D(x))\\)(real data) + \\(\\log(1 - D(G(x)))\\)(fake data)\nG’s criterion = \\(\\log(D(G(z)))\\)\n\n데이터\n\nLSUN\nFACES\nIMAGENET-1K\n\n\nDCGAN에서 중요한 기준 - NOT MIMICKING TRAIN DATA -&gt; 단순히 학습 데이터를 모방하면 안됨! - “Walking in the latent Space” -&gt; G의 input z의 공간인 latent Space에서 z1에서 z2로 살짝 이동한다 하더라도 급작스러운 변화가 일어나지 않고 물흐르듯 부드러운 변화를 보여줘야 한다."
  },
  {
    "objectID": "docs/DL/posts/DCGAN.html#논문-구현",
    "href": "docs/DL/posts/DCGAN.html#논문-구현",
    "title": "DCGAN paper review",
    "section": "",
    "text": "import random\nimport math\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom PIL import Image\n\nimport torch\nimport torch.utils.data as data\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision.datasets as dsets\nfrom torchvision import transforms\n\ntorch.manual_seed(1234)\nnp.random.seed(1234)\nrandom.seed(1234)\n\n\n\n\ndef conv_dim(i,k,s,p):\n    '''\n    nn.Conv2d 사용할 때 사이즈 계산\n    i: input image size\n    k: kernel size\n    s: stride\n    p: padding\n    '''\n    out = (i - k + 2*p)/s + 1\n    return out  \n\n\ndef convt_dim(i,k,s,p):\n    '''\n    nn.ConvTranspose2d 사용할 때 사이즈 계산\n    i: input image size\n    k: kernel size\n    s: stride\n    p: padding\n    '''\n    out = (i-1) * s - 2 * p + k\n    return out  \n\n\n\n\n\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim = 20, image_size = 64):\n        super(Generator, self).__init__()\n\n        # layer1 -&gt; W(H) * 4\n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, image_size * 8, kernel_size = 4, stride = 1),\n            nn.BatchNorm2d(image_size * 8),\n            nn.ReLU(inplace=True))\n        \n        # layer2 -&gt; W(H) * 2\n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 8, image_size * 4, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size * 4),\n            nn.ReLU(inplace=True))\n        \n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 4, image_size * 2, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size * 2),\n            nn.ReLU(inplace=True))\n        \n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 2, image_size, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size),\n            nn.ReLU(inplace=True))\n        \n        self.last = nn.Sequential(\n            nn.ConvTranspose2d(image_size, 1, kernel_size= 4, stride=2, padding=1), # 흑백 이미지이므로 출력 차원을 1으로 지정한 것\n            nn.Tanh())\n        \n    def forward(self, z):\n        out = self.layer1(z)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        out = out.type(torch.FloatTensor)\n        return out\n\n\n\n\nG = Generator(z_dim=20, image_size=64)\ninput_z = torch.randn(1, 20)\n\n# tensor size -&gt; (1, 20, 1, 1) \n# pytorch: (batch_size, channel, height, width)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n\nfake_img = G(input_z)\n\n\n# fake_img[0][0].size() -&gt; (64, 64)\nimg_transformed = fake_img[0][0].detach().numpy()\nplt.imshow(img_transformed, 'gray')\nplt.show()\n\n\n\n\n\n\n\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, z_dim=20, image_size=64):\n        super(Discriminator, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, image_size, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(image_size, image_size*2, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(image_size*2, image_size*4, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(image_size*4, image_size*8, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.last = nn.Conv2d(image_size*8, 1, kernel_size=4, stride=1)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        out = out.type(torch.FloatTensor)\n        return out\n\n\nD = Discriminator(z_dim=20, image_size=64)\n\nd_out = D(fake_img)\n\n# 출력 d_out에 Sigmoid를 곱해 0에서 1로 변환\nprint(nn.Sigmoid()(d_out))\n\ntensor([[[[0.4980]]]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\n\n\n\nmini_batch_size = 2\n\n# 정답 라벨 생성 -&gt; torch.tensor([1,1])\nlabel_real = torch.full((mini_batch_size,), 1)\n\n# 가짜 라벨 생성 -&gt; torch.tensor([0,0])\nlabel_fake = torch.full((mini_batch_size,), 0)\n\n\n# 가짜 이미지 생성\ninput_z = torch.randn(mini_batch_size, 20)\n# tensor size -&gt; (2,20,1,1)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1) \n# G에 의해 tensor size -&gt; (2,1,64,64)\nfake_images = G(input_z)\n# D에 의해 tensor size -&gt; (2,1,1,1)\nd_out_fake = D(fake_images)\n\n\n\n\\(maximize\\) \\(log(D(x))\\)+\\(log(1 - D(G(z)))\\)\n# loss function 정의\nloss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n\n# 진짜 이미지 판정\nd_out_real = D(x)\n\n# 가짜 이미지 생성\ninput_z = torch.randn(mini_batch_size, 20)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1) \nfake_images = G(input_z)\nd_out_fake = D(fake_images)\n\n# 오차를 계산\nd_loss_real = loss_fn(d_out_real.view(-1), label_real)\nd_loss_fake = loss_fn(d_out_fake.view(-1), label_fake)\nd_loss = d_loss_real + d_loss_fake\n\n\n\n\\(maximize\\) \\(log\\)\\((D(G(z)))\\)\n# 가짜 화상을 생성해 판정\ninput_z = torch.randn(mini_batch_size, 20)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\nfake_images = G(input_z)\nd_out_fake = D(fake_images)\n\n# 오차를 계산\ng_loss = criterion(d_out_fake.view(-1), label_real)\n\n\n\n\n\n\n\nimport os\nimport urllib.request\nimport zipfile\nimport tarfile\n\n\nfrom sklearn.datasets import fetch_openml\n\nmnist_F = fetch_openml('mnist_784', version=1, data_home=\"./data/\")  \n\n\n\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', version=1, data_home=\"./data/\")  \n\n\nX = mnist.data.to_numpy()\ny = mnist.target.to_numpy()\n\n\ndata_dir_path = \"./data/img_78/\"\nif not os.path.exists(data_dir_path):\n    os.mkdir(data_dir_path)\n\n\n# MNIST에서 숫자7, 8의 화상만 \"img_78\" 폴더에 화상으로 저장해 나간다\ncount7=0\ncount8=0\nmax_num=200  # 화상은 200장씩 작성한다\n\nfor i in range(len(X)):\n    \n    # 화상7 작성\n    if (y[i] is \"7\") and (count7&lt;max_num):\n        file_path=\"./data/img_78/img_7_\"+str(count7)+\".jpg\"\n        im_f=(X[i].reshape(28, 28))  # 화상을 28×28의 형태로 변경\n        pil_img_f = Image.fromarray(im_f.astype(np.uint8))  # 화상을 PIL으로\n        pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\n        pil_img_f.save(file_path)  # 저장\n        count7+=1 \n    \n    # 화상8 작성\n    if (y[i] is \"8\") and (count8&lt;max_num):\n        file_path=\"./data/img_78/img_8_\"+str(count8)+\".jpg\"\n        im_f=(X[i].reshape(28, 28))  # 화상을 28×28의 형태로 변경\n        pil_img_f = Image.fromarray(im_f.astype(np.uint8))  # 화상을 PIL으로\n        pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\n        pil_img_f.save(file_path)  # 저장\n        count8+=1 \n\n&lt;&gt;:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n&lt;&gt;:18: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n&lt;&gt;:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n&lt;&gt;:18: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if (y[i] is \"7\") and (count7&lt;max_num):\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:18: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if (y[i] is \"8\") and (count8&lt;max_num):\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:13: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n  pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\nC:\\Users\\default.DESKTOP-HUJV032\\AppData\\Local\\Temp\\ipykernel_12224\\1822497188.py:22: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n  pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64로 확대\n\n\n\ndef make_datapath_list():\n    train_img_list = list() \n    for img_idx in range(200):\n        img_path = \"./data/img_78/img_7_\" + str(img_idx)+'.jpg'\n        train_img_list.append(img_path)\n\n        img_path = \"./data/img_78/img_8_\" + str(img_idx)+'.jpg'\n        train_img_list.append(img_path)\n\n    return train_img_list\n\n\n\n\n\n\nclass ImageTransform():\n    \"\"\"이미지 전처리 클래스\"\"\"\n\n    def __init__(self, mean, std):\n        self.data_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n\n    def __call__(self, img):\n        return self.data_transform(img)\n\n\nclass GAN_Img_Dataset(data.Dataset):\n    \"\"\"Dataset 클래스. PyTorch의 Dataset 클래스를 상속\"\"\"\n\n    def __init__(self, file_list, transform):\n        self.file_list = file_list\n        self.transform = transform\n\n    def __len__(self):\n        '''이미지 개수 반환'''\n        return len(self.file_list)\n\n    def __getitem__(self, index):\n        '''전처리된 이미지를 Tensor 형식 데이터로 변환'''\n\n        img_path = self.file_list[index]\n        img = Image.open(img_path)  # [높이][폭]흑백\n\n        # 이미지 전처리\n        img_transformed = self.transform(img)\n        img_transformed = img_transformed.type(torch.FloatTensor)\n        return img_transformed\n\n\n# DataLoader 작성과 동작 확인\n\n# 파일 리스트를 작성\ntrain_img_list=make_datapath_list()\n\n# Dataset 작성\nmean = (0.5,)\nstd = (0.5,)\ntrain_dataset = GAN_Img_Dataset(file_list=train_img_list, transform=ImageTransform(mean, std))\n\n# DataLoader 작성\nbatch_size = 64\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True)\n\n# 동작 확인\nbatch_iterator = iter(train_dataloader)  # 반복자로 변환\nimges = next(batch_iterator)  # 1번째 요소를 꺼낸다\nprint(imges.size())  # torch.Size([64, 1, 64, 64])\n\ntorch.Size([64, 1, 64, 64])\n\n\n\n\n\n\n\n# 네트워크 초기화\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        # Conv2dとConvTranspose2d 초기화\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n    elif classname.find('BatchNorm') != -1:\n        # BatchNorm2d 초기화\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# 초기화 실시\nG.apply(weights_init)\nD.apply(weights_init)\n\nprint(\"네트워크 초기화 완료\")\n\n네트워크 초기화 완료\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");device\n\ndevice(type='cuda', index=0)\n\n\n\n# 모델을 학습시키는 함수를 작성\ndef train_model(G, D, dataloader, num_epochs):\n\n    # GPU가 사용 가능한지 확인\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"사용 장치: \", device)\n\n    # 최적화 기법 설정\n    g_lr, d_lr = 0.0001, 0.0004\n    beta1, beta2 = 0.0, 0.9\n    g_optimizer = torch.optim.Adam(G.parameters(), g_lr, [beta1, beta2])\n    d_optimizer = torch.optim.Adam(D.parameters(), d_lr, [beta1, beta2])\n\n    # 오차함수 정의\n    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n\n    # 파라미터를 하드코딩\n    z_dim = 20\n    mini_batch_size = 64\n\n    # 네트워크를 GPU로\n    G.to(device)\n    D.to(device)\n\n    G.train()  # 모델을 훈련 모드로\n    D.train()  # 모델을 훈련 모드로\n\n    # 네트워크가 어느 정도 고정되면, 고속화시킨다\n    torch.backends.cudnn.benchmark = True\n\n    num_train_imgs = len(dataloader.dataset)\n    batch_size = dataloader.batch_size\n\n    # 반복 카운터 설정\n    iteration = 1\n    logs = []\n\n    # epoch 루프\n    for epoch in range(num_epochs):\n\n        # 개시 시간을 저장\n        t_epoch_start = time.time()\n        epoch_g_loss = 0.0  # epoch의 손실합\n        epoch_d_loss = 0.0  # epoch의 손실합\n\n        print('-------------')\n        print('Epoch {}/{}'.format(epoch, num_epochs))\n        print('-------------')\n        print('(train)')\n\n        # 데이터 로더에서 minibatch씩 꺼내는 루프\n        for imges in dataloader:\n\n            # --------------------\n            # 1. Discriminator 학습\n            # --------------------\n            # 미니 배치 크기가 1이면, 배치 노멀라이제이션에서 에러가 발생하므로 피한다\n            if imges.size()[0] == 1:\n                continue\n\n            # GPU가 사용 가능하면 GPU로 데이터를 보낸다\n            imges = imges.to(device)\n\n            # 정답 라벨과 가짜 라벨 작성\n            # epoch의 마지막 반복은 미니 배치 수가 줄어든다\n            mini_batch_size = imges.size()[0]\n            label_real = torch.full((mini_batch_size,), 1).to(device)\n            label_fake = torch.full((mini_batch_size,), 0).to(device)\n\n            # 진짜 이미지 판정\n            d_out_real = D(imges)\n\n            # 가짜 이미지 생성해 판정\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1).to(device)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images.to(device))\n\n            # 오차를 계산\n            d_loss_real = criterion(d_out_real.view(-1).to(device), label_real.float())\n            d_loss_fake = criterion(d_out_fake.view(-1).to(device), label_fake.float())\n            d_loss = d_loss_real + d_loss_fake\n\n            # 역전파\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n\n            d_loss.backward()\n            d_optimizer.step()\n\n            # 2. Generator 학습\n            # --------------------\n            # 가짜 이미지 생성해 판정\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1).to(device)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images.to(device))\n\n            # 오차를 계산\n            g_loss = criterion(d_out_fake.view(-1).to(device), label_real.float().to(device))\n\n            # 역전파\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n            g_loss.backward()\n            g_optimizer.step()\n\n            # --------------------\n            # 3. 기록\n            # --------------------\n            epoch_d_loss += d_loss.item()\n            epoch_g_loss += g_loss.item()\n            iteration += 1\n\n        # epoch의 phase별 loss와 정답률\n        t_epoch_finish = time.time()\n        print('-------------')\n        print('epoch {} || Epoch_D_Loss:{:.4f} ||Epoch_G_Loss:{:.4f}'.format(\n            epoch, epoch_d_loss/batch_size, epoch_g_loss/batch_size))\n        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n        t_epoch_start = time.time()\n\n    return G, D\n\n\nnum_epochs = 200\nG_update, D_update = train_model(G, D, dataloader=train_dataloader, num_epochs=num_epochs)\n\n사용 장치:  cuda:0\n-------------\nEpoch 0/200\n-------------\n(train)\n-------------\nepoch 0 || Epoch_D_Loss:0.0147 ||Epoch_G_Loss:0.7212\ntimer:  0.9429 sec.\n-------------\nEpoch 1/200\n-------------\n(train)\n-------------\nepoch 1 || Epoch_D_Loss:0.0108 ||Epoch_G_Loss:0.8667\ntimer:  0.7360 sec.\n-------------\nEpoch 2/200\n-------------\n(train)\n-------------\nepoch 2 || Epoch_D_Loss:0.0266 ||Epoch_G_Loss:0.6338\ntimer:  0.7420 sec.\n-------------\nEpoch 3/200\n-------------\n(train)\n-------------\nepoch 3 || Epoch_D_Loss:0.0029 ||Epoch_G_Loss:0.7435\ntimer:  0.7325 sec.\n-------------\nEpoch 4/200\n-------------\n(train)\n-------------\nepoch 4 || Epoch_D_Loss:0.0038 ||Epoch_G_Loss:0.7857\ntimer:  0.7349 sec.\n-------------\nEpoch 5/200\n-------------\n(train)\n-------------\nepoch 5 || Epoch_D_Loss:0.0128 ||Epoch_G_Loss:0.7397\ntimer:  0.7323 sec.\n-------------\nEpoch 6/200\n-------------\n(train)\n-------------\nepoch 6 || Epoch_D_Loss:0.0058 ||Epoch_G_Loss:0.9182\ntimer:  0.7353 sec.\n-------------\nEpoch 7/200\n-------------\n(train)\n-------------\nepoch 7 || Epoch_D_Loss:0.0561 ||Epoch_G_Loss:0.6317\ntimer:  0.7301 sec.\n-------------\nEpoch 8/200\n-------------\n(train)\n-------------\nepoch 8 || Epoch_D_Loss:0.0046 ||Epoch_G_Loss:0.7038\ntimer:  0.7350 sec.\n-------------\nEpoch 9/200\n-------------\n(train)\n-------------\nepoch 9 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.7608\ntimer:  0.7320 sec.\n-------------\nEpoch 10/200\n-------------\n(train)\n-------------\nepoch 10 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.7988\ntimer:  0.7388 sec.\n-------------\nEpoch 11/200\n-------------\n(train)\n-------------\nepoch 11 || Epoch_D_Loss:0.0035 ||Epoch_G_Loss:0.8410\ntimer:  0.7402 sec.\n-------------\nEpoch 12/200\n-------------\n(train)\n-------------\nepoch 12 || Epoch_D_Loss:0.0554 ||Epoch_G_Loss:0.9072\ntimer:  0.7309 sec.\n-------------\nEpoch 13/200\n-------------\n(train)\n-------------\nepoch 13 || Epoch_D_Loss:0.0054 ||Epoch_G_Loss:0.6871\ntimer:  0.7408 sec.\n-------------\nEpoch 14/200\n-------------\n(train)\n-------------\nepoch 14 || Epoch_D_Loss:0.0027 ||Epoch_G_Loss:0.8104\ntimer:  0.7420 sec.\n-------------\nEpoch 15/200\n-------------\n(train)\n-------------\nepoch 15 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8349\ntimer:  0.7335 sec.\n-------------\nEpoch 16/200\n-------------\n(train)\n-------------\nepoch 16 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8269\ntimer:  0.7387 sec.\n-------------\nEpoch 17/200\n-------------\n(train)\n-------------\nepoch 17 || Epoch_D_Loss:0.0113 ||Epoch_G_Loss:0.9406\ntimer:  0.7346 sec.\n-------------\nEpoch 18/200\n-------------\n(train)\n-------------\nepoch 18 || Epoch_D_Loss:0.0484 ||Epoch_G_Loss:0.7943\ntimer:  0.7341 sec.\n-------------\nEpoch 19/200\n-------------\n(train)\n-------------\nepoch 19 || Epoch_D_Loss:0.0038 ||Epoch_G_Loss:0.7086\ntimer:  0.7367 sec.\n-------------\nEpoch 20/200\n-------------\n(train)\n-------------\nepoch 20 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8174\ntimer:  0.7461 sec.\n-------------\nEpoch 21/200\n-------------\n(train)\n-------------\nepoch 21 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.9420\ntimer:  0.7323 sec.\n-------------\nEpoch 22/200\n-------------\n(train)\n-------------\nepoch 22 || Epoch_D_Loss:0.0253 ||Epoch_G_Loss:0.7721\ntimer:  0.7278 sec.\n-------------\nEpoch 23/200\n-------------\n(train)\n-------------\nepoch 23 || Epoch_D_Loss:0.0093 ||Epoch_G_Loss:0.7429\ntimer:  0.7256 sec.\n-------------\nEpoch 24/200\n-------------\n(train)\n-------------\nepoch 24 || Epoch_D_Loss:0.0123 ||Epoch_G_Loss:0.8130\ntimer:  0.7300 sec.\n-------------\nEpoch 25/200\n-------------\n(train)\n-------------\nepoch 25 || Epoch_D_Loss:0.0103 ||Epoch_G_Loss:0.8479\ntimer:  0.7365 sec.\n-------------\nEpoch 26/200\n-------------\n(train)\n-------------\nepoch 26 || Epoch_D_Loss:0.0015 ||Epoch_G_Loss:0.8154\ntimer:  0.7331 sec.\n-------------\nEpoch 27/200\n-------------\n(train)\n-------------\nepoch 27 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8887\ntimer:  0.7366 sec.\n-------------\nEpoch 28/200\n-------------\n(train)\n-------------\nepoch 28 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:1.0104\ntimer:  0.7301 sec.\n-------------\nEpoch 29/200\n-------------\n(train)\n-------------\nepoch 29 || Epoch_D_Loss:0.0665 ||Epoch_G_Loss:0.6415\ntimer:  0.7291 sec.\n-------------\nEpoch 30/200\n-------------\n(train)\n-------------\nepoch 30 || Epoch_D_Loss:0.0060 ||Epoch_G_Loss:0.7702\ntimer:  0.7330 sec.\n-------------\nEpoch 31/200\n-------------\n(train)\n-------------\nepoch 31 || Epoch_D_Loss:0.0018 ||Epoch_G_Loss:0.8099\ntimer:  0.7369 sec.\n-------------\nEpoch 32/200\n-------------\n(train)\n-------------\nepoch 32 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8387\ntimer:  0.7320 sec.\n-------------\nEpoch 33/200\n-------------\n(train)\n-------------\nepoch 33 || Epoch_D_Loss:0.0116 ||Epoch_G_Loss:0.9780\ntimer:  0.7371 sec.\n-------------\nEpoch 34/200\n-------------\n(train)\n-------------\nepoch 34 || Epoch_D_Loss:0.0895 ||Epoch_G_Loss:0.6259\ntimer:  0.7390 sec.\n-------------\nEpoch 35/200\n-------------\n(train)\n-------------\nepoch 35 || Epoch_D_Loss:0.0040 ||Epoch_G_Loss:0.7130\ntimer:  0.7300 sec.\n-------------\nEpoch 36/200\n-------------\n(train)\n-------------\nepoch 36 || Epoch_D_Loss:0.0033 ||Epoch_G_Loss:0.7479\ntimer:  0.7291 sec.\n-------------\nEpoch 37/200\n-------------\n(train)\n-------------\nepoch 37 || Epoch_D_Loss:0.0019 ||Epoch_G_Loss:0.7853\ntimer:  0.7266 sec.\n-------------\nEpoch 38/200\n-------------\n(train)\n-------------\nepoch 38 || Epoch_D_Loss:0.0034 ||Epoch_G_Loss:0.7967\ntimer:  0.7288 sec.\n-------------\nEpoch 39/200\n-------------\n(train)\n-------------\nepoch 39 || Epoch_D_Loss:0.0376 ||Epoch_G_Loss:0.9572\ntimer:  0.7312 sec.\n-------------\nEpoch 40/200\n-------------\n(train)\n-------------\nepoch 40 || Epoch_D_Loss:0.0112 ||Epoch_G_Loss:0.7412\ntimer:  0.7272 sec.\n-------------\nEpoch 41/200\n-------------\n(train)\n-------------\nepoch 41 || Epoch_D_Loss:0.0022 ||Epoch_G_Loss:0.8254\ntimer:  0.7251 sec.\n-------------\nEpoch 42/200\n-------------\n(train)\n-------------\nepoch 42 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8413\ntimer:  0.7251 sec.\n-------------\nEpoch 43/200\n-------------\n(train)\n-------------\nepoch 43 || Epoch_D_Loss:0.0007 ||Epoch_G_Loss:0.8888\ntimer:  0.7370 sec.\n-------------\nEpoch 44/200\n-------------\n(train)\n-------------\nepoch 44 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.9397\ntimer:  0.7321 sec.\n-------------\nEpoch 45/200\n-------------\n(train)\n-------------\nepoch 45 || Epoch_D_Loss:0.0589 ||Epoch_G_Loss:0.7371\ntimer:  0.7245 sec.\n-------------\nEpoch 46/200\n-------------\n(train)\n-------------\nepoch 46 || Epoch_D_Loss:0.0032 ||Epoch_G_Loss:0.7395\ntimer:  0.7276 sec.\n-------------\nEpoch 47/200\n-------------\n(train)\n-------------\nepoch 47 || Epoch_D_Loss:0.0037 ||Epoch_G_Loss:0.8705\ntimer:  0.7277 sec.\n-------------\nEpoch 48/200\n-------------\n(train)\n-------------\nepoch 48 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.8553\ntimer:  0.7288 sec.\n-------------\nEpoch 49/200\n-------------\n(train)\n-------------\nepoch 49 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.8959\ntimer:  0.7374 sec.\n-------------\nEpoch 50/200\n-------------\n(train)\n-------------\nepoch 50 || Epoch_D_Loss:0.0429 ||Epoch_G_Loss:0.9181\ntimer:  0.7258 sec.\n-------------\nEpoch 51/200\n-------------\n(train)\n-------------\nepoch 51 || Epoch_D_Loss:0.0030 ||Epoch_G_Loss:0.7838\ntimer:  0.7300 sec.\n-------------\nEpoch 52/200\n-------------\n(train)\n-------------\nepoch 52 || Epoch_D_Loss:0.0032 ||Epoch_G_Loss:0.8647\ntimer:  0.7330 sec.\n-------------\nEpoch 53/200\n-------------\n(train)\n-------------\nepoch 53 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.8791\ntimer:  0.7306 sec.\n-------------\nEpoch 54/200\n-------------\n(train)\n-------------\nepoch 54 || Epoch_D_Loss:0.0362 ||Epoch_G_Loss:0.9293\ntimer:  0.7291 sec.\n-------------\nEpoch 55/200\n-------------\n(train)\n-------------\nepoch 55 || Epoch_D_Loss:0.0123 ||Epoch_G_Loss:0.7459\ntimer:  0.7313 sec.\n-------------\nEpoch 56/200\n-------------\n(train)\n-------------\nepoch 56 || Epoch_D_Loss:0.0033 ||Epoch_G_Loss:0.8016\ntimer:  0.7408 sec.\n-------------\nEpoch 57/200\n-------------\n(train)\n-------------\nepoch 57 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8647\ntimer:  0.7294 sec.\n-------------\nEpoch 58/200\n-------------\n(train)\n-------------\nepoch 58 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8626\ntimer:  0.7275 sec.\n-------------\nEpoch 59/200\n-------------\n(train)\n-------------\nepoch 59 || Epoch_D_Loss:0.0625 ||Epoch_G_Loss:0.9511\ntimer:  0.7284 sec.\n-------------\nEpoch 60/200\n-------------\n(train)\n-------------\nepoch 60 || Epoch_D_Loss:0.0097 ||Epoch_G_Loss:0.7070\ntimer:  0.7263 sec.\n-------------\nEpoch 61/200\n-------------\n(train)\n-------------\nepoch 61 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.7738\ntimer:  0.7272 sec.\n-------------\nEpoch 62/200\n-------------\n(train)\n-------------\nepoch 62 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.7819\ntimer:  0.7280 sec.\n-------------\nEpoch 63/200\n-------------\n(train)\n-------------\nepoch 63 || Epoch_D_Loss:0.0015 ||Epoch_G_Loss:0.8568\ntimer:  0.7297 sec.\n-------------\nEpoch 64/200\n-------------\n(train)\n-------------\nepoch 64 || Epoch_D_Loss:0.0375 ||Epoch_G_Loss:0.8728\ntimer:  0.7267 sec.\n-------------\nEpoch 65/200\n-------------\n(train)\n-------------\nepoch 65 || Epoch_D_Loss:0.0063 ||Epoch_G_Loss:0.6889\ntimer:  0.7381 sec.\n-------------\nEpoch 66/200\n-------------\n(train)\n-------------\nepoch 66 || Epoch_D_Loss:0.0038 ||Epoch_G_Loss:0.8209\ntimer:  0.7297 sec.\n-------------\nEpoch 67/200\n-------------\n(train)\n-------------\nepoch 67 || Epoch_D_Loss:0.0022 ||Epoch_G_Loss:0.8709\ntimer:  0.7559 sec.\n-------------\nEpoch 68/200\n-------------\n(train)\n-------------\nepoch 68 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.8871\ntimer:  0.7298 sec.\n-------------\nEpoch 69/200\n-------------\n(train)\n-------------\nepoch 69 || Epoch_D_Loss:0.0414 ||Epoch_G_Loss:0.9295\ntimer:  0.7275 sec.\n-------------\nEpoch 70/200\n-------------\n(train)\n-------------\nepoch 70 || Epoch_D_Loss:0.0088 ||Epoch_G_Loss:0.7318\ntimer:  0.7226 sec.\n-------------\nEpoch 71/200\n-------------\n(train)\n-------------\nepoch 71 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8082\ntimer:  0.7209 sec.\n-------------\nEpoch 72/200\n-------------\n(train)\n-------------\nepoch 72 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.8387\ntimer:  0.7196 sec.\n-------------\nEpoch 73/200\n-------------\n(train)\n-------------\nepoch 73 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.9041\ntimer:  0.7261 sec.\n-------------\nEpoch 74/200\n-------------\n(train)\n-------------\nepoch 74 || Epoch_D_Loss:0.0522 ||Epoch_G_Loss:0.9280\ntimer:  0.7300 sec.\n-------------\nEpoch 75/200\n-------------\n(train)\n-------------\nepoch 75 || Epoch_D_Loss:0.0056 ||Epoch_G_Loss:0.6952\ntimer:  0.7258 sec.\n-------------\nEpoch 76/200\n-------------\n(train)\n-------------\nepoch 76 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.8027\ntimer:  0.7219 sec.\n-------------\nEpoch 77/200\n-------------\n(train)\n-------------\nepoch 77 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.8502\ntimer:  0.7210 sec.\n-------------\nEpoch 78/200\n-------------\n(train)\n-------------\nepoch 78 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.8654\ntimer:  0.7208 sec.\n-------------\nEpoch 79/200\n-------------\n(train)\n-------------\nepoch 79 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.9063\ntimer:  0.7214 sec.\n-------------\nEpoch 80/200\n-------------\n(train)\n-------------\nepoch 80 || Epoch_D_Loss:0.0458 ||Epoch_G_Loss:0.9730\ntimer:  0.7253 sec.\n-------------\nEpoch 81/200\n-------------\n(train)\n-------------\nepoch 81 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.7860\ntimer:  0.7200 sec.\n-------------\nEpoch 82/200\n-------------\n(train)\n-------------\nepoch 82 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8159\ntimer:  0.7201 sec.\n-------------\nEpoch 83/200\n-------------\n(train)\n-------------\nepoch 83 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.8737\ntimer:  0.7207 sec.\n-------------\nEpoch 84/200\n-------------\n(train)\n-------------\nepoch 84 || Epoch_D_Loss:0.0444 ||Epoch_G_Loss:0.8085\ntimer:  0.7164 sec.\n-------------\nEpoch 85/200\n-------------\n(train)\n-------------\nepoch 85 || Epoch_D_Loss:0.0100 ||Epoch_G_Loss:0.7245\ntimer:  0.7192 sec.\n-------------\nEpoch 86/200\n-------------\n(train)\n-------------\nepoch 86 || Epoch_D_Loss:0.0019 ||Epoch_G_Loss:0.7648\ntimer:  0.7202 sec.\n-------------\nEpoch 87/200\n-------------\n(train)\n-------------\nepoch 87 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8417\ntimer:  0.7172 sec.\n-------------\nEpoch 88/200\n-------------\n(train)\n-------------\nepoch 88 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8620\ntimer:  0.7171 sec.\n-------------\nEpoch 89/200\n-------------\n(train)\n-------------\nepoch 89 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.9353\ntimer:  0.7189 sec.\n-------------\nEpoch 90/200\n-------------\n(train)\n-------------\nepoch 90 || Epoch_D_Loss:0.0538 ||Epoch_G_Loss:0.9575\ntimer:  0.7199 sec.\n-------------\nEpoch 91/200\n-------------\n(train)\n-------------\nepoch 91 || Epoch_D_Loss:0.0036 ||Epoch_G_Loss:0.7752\ntimer:  0.7231 sec.\n-------------\nEpoch 92/200\n-------------\n(train)\n-------------\nepoch 92 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.8001\ntimer:  0.7212 sec.\n-------------\nEpoch 93/200\n-------------\n(train)\n-------------\nepoch 93 || Epoch_D_Loss:0.0029 ||Epoch_G_Loss:0.9208\ntimer:  0.7198 sec.\n-------------\nEpoch 94/200\n-------------\n(train)\n-------------\nepoch 94 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.9002\ntimer:  0.7233 sec.\n-------------\nEpoch 95/200\n-------------\n(train)\n-------------\nepoch 95 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.8902\ntimer:  0.7227 sec.\n-------------\nEpoch 96/200\n-------------\n(train)\n-------------\nepoch 96 || Epoch_D_Loss:0.0467 ||Epoch_G_Loss:0.8899\ntimer:  0.7380 sec.\n-------------\nEpoch 97/200\n-------------\n(train)\n-------------\nepoch 97 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.7825\ntimer:  0.7333 sec.\n-------------\nEpoch 98/200\n-------------\n(train)\n-------------\nepoch 98 || Epoch_D_Loss:0.0037 ||Epoch_G_Loss:0.8136\ntimer:  0.7191 sec.\n-------------\nEpoch 99/200\n-------------\n(train)\n-------------\nepoch 99 || Epoch_D_Loss:0.0096 ||Epoch_G_Loss:0.9419\ntimer:  0.7208 sec.\n-------------\nEpoch 100/200\n-------------\n(train)\n-------------\nepoch 100 || Epoch_D_Loss:0.0014 ||Epoch_G_Loss:0.8698\ntimer:  0.7468 sec.\n-------------\nEpoch 101/200\n-------------\n(train)\n-------------\nepoch 101 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.9751\ntimer:  0.7229 sec.\n-------------\nEpoch 102/200\n-------------\n(train)\n-------------\nepoch 102 || Epoch_D_Loss:0.0005 ||Epoch_G_Loss:0.9452\ntimer:  0.7198 sec.\n-------------\nEpoch 103/200\n-------------\n(train)\n-------------\nepoch 103 || Epoch_D_Loss:0.0007 ||Epoch_G_Loss:0.9833\ntimer:  0.7279 sec.\n-------------\nEpoch 104/200\n-------------\n(train)\n-------------\nepoch 104 || Epoch_D_Loss:0.1555 ||Epoch_G_Loss:1.0839\ntimer:  0.7222 sec.\n-------------\nEpoch 105/200\n-------------\n(train)\n-------------\nepoch 105 || Epoch_D_Loss:0.0103 ||Epoch_G_Loss:0.6298\ntimer:  0.7233 sec.\n-------------\nEpoch 106/200\n-------------\n(train)\n-------------\nepoch 106 || Epoch_D_Loss:0.0061 ||Epoch_G_Loss:0.7351\ntimer:  0.7223 sec.\n-------------\nEpoch 107/200\n-------------\n(train)\n-------------\nepoch 107 || Epoch_D_Loss:0.0036 ||Epoch_G_Loss:0.7819\ntimer:  0.7340 sec.\n-------------\nEpoch 108/200\n-------------\n(train)\n-------------\nepoch 108 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.8024\ntimer:  0.7279 sec.\n-------------\nEpoch 109/200\n-------------\n(train)\n-------------\nepoch 109 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.9204\ntimer:  0.7284 sec.\n-------------\nEpoch 110/200\n-------------\n(train)\n-------------\nepoch 110 || Epoch_D_Loss:0.0069 ||Epoch_G_Loss:1.1190\ntimer:  0.7254 sec.\n-------------\nEpoch 111/200\n-------------\n(train)\n-------------\nepoch 111 || Epoch_D_Loss:0.0006 ||Epoch_G_Loss:1.0324\ntimer:  0.7267 sec.\n-------------\nEpoch 112/200\n-------------\n(train)\n-------------\nepoch 112 || Epoch_D_Loss:0.0014 ||Epoch_G_Loss:0.9086\ntimer:  0.7268 sec.\n-------------\nEpoch 113/200\n-------------\n(train)\n-------------\nepoch 113 || Epoch_D_Loss:0.0014 ||Epoch_G_Loss:0.9236\ntimer:  0.7270 sec.\n-------------\nEpoch 114/200\n-------------\n(train)\n-------------\nepoch 114 || Epoch_D_Loss:0.0561 ||Epoch_G_Loss:0.9581\ntimer:  0.7281 sec.\n-------------\nEpoch 115/200\n-------------\n(train)\n-------------\nepoch 115 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.7752\ntimer:  0.7249 sec.\n-------------\nEpoch 116/200\n-------------\n(train)\n-------------\nepoch 116 || Epoch_D_Loss:0.0021 ||Epoch_G_Loss:0.8565\ntimer:  0.7257 sec.\n-------------\nEpoch 117/200\n-------------\n(train)\n-------------\nepoch 117 || Epoch_D_Loss:0.0317 ||Epoch_G_Loss:0.8358\ntimer:  0.7263 sec.\n-------------\nEpoch 118/200\n-------------\n(train)\n-------------\nepoch 118 || Epoch_D_Loss:0.0104 ||Epoch_G_Loss:0.7355\ntimer:  0.7370 sec.\n-------------\nEpoch 119/200\n-------------\n(train)\n-------------\nepoch 119 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.8858\ntimer:  0.7332 sec.\n-------------\nEpoch 120/200\n-------------\n(train)\n-------------\nepoch 120 || Epoch_D_Loss:0.0337 ||Epoch_G_Loss:0.7324\ntimer:  0.7300 sec.\n-------------\nEpoch 121/200\n-------------\n(train)\n-------------\nepoch 121 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.7945\ntimer:  0.7221 sec.\n-------------\nEpoch 122/200\n-------------\n(train)\n-------------\nepoch 122 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.8590\ntimer:  0.7202 sec.\n-------------\nEpoch 123/200\n-------------\n(train)\n-------------\nepoch 123 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8890\ntimer:  0.7228 sec.\n-------------\nEpoch 124/200\n-------------\n(train)\n-------------\nepoch 124 || Epoch_D_Loss:0.0006 ||Epoch_G_Loss:0.8837\ntimer:  0.7254 sec.\n-------------\nEpoch 125/200\n-------------\n(train)\n-------------\nepoch 125 || Epoch_D_Loss:0.0068 ||Epoch_G_Loss:0.9388\ntimer:  0.7226 sec.\n-------------\nEpoch 126/200\n-------------\n(train)\n-------------\nepoch 126 || Epoch_D_Loss:0.0567 ||Epoch_G_Loss:0.6943\ntimer:  0.7223 sec.\n-------------\nEpoch 127/200\n-------------\n(train)\n-------------\nepoch 127 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.7448\ntimer:  0.7190 sec.\n-------------\nEpoch 128/200\n-------------\n(train)\n-------------\nepoch 128 || Epoch_D_Loss:0.0019 ||Epoch_G_Loss:0.8421\ntimer:  0.7241 sec.\n-------------\nEpoch 129/200\n-------------\n(train)\n-------------\nepoch 129 || Epoch_D_Loss:0.0044 ||Epoch_G_Loss:0.9485\ntimer:  0.7198 sec.\n-------------\nEpoch 130/200\n-------------\n(train)\n-------------\nepoch 130 || Epoch_D_Loss:0.0442 ||Epoch_G_Loss:1.0138\ntimer:  0.7290 sec.\n-------------\nEpoch 131/200\n-------------\n(train)\n-------------\nepoch 131 || Epoch_D_Loss:0.0076 ||Epoch_G_Loss:0.7135\ntimer:  0.7240 sec.\n-------------\nEpoch 132/200\n-------------\n(train)\n-------------\nepoch 132 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.7633\ntimer:  0.7216 sec.\n-------------\nEpoch 133/200\n-------------\n(train)\n-------------\nepoch 133 || Epoch_D_Loss:0.0015 ||Epoch_G_Loss:0.8278\ntimer:  0.7209 sec.\n-------------\nEpoch 134/200\n-------------\n(train)\n-------------\nepoch 134 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.9835\ntimer:  0.7225 sec.\n-------------\nEpoch 135/200\n-------------\n(train)\n-------------\nepoch 135 || Epoch_D_Loss:0.0004 ||Epoch_G_Loss:0.9521\ntimer:  0.7191 sec.\n-------------\nEpoch 136/200\n-------------\n(train)\n-------------\nepoch 136 || Epoch_D_Loss:0.0004 ||Epoch_G_Loss:1.0310\ntimer:  0.7201 sec.\n-------------\nEpoch 137/200\n-------------\n(train)\n-------------\nepoch 137 || Epoch_D_Loss:0.0633 ||Epoch_G_Loss:1.0482\ntimer:  0.7257 sec.\n-------------\nEpoch 138/200\n-------------\n(train)\n-------------\nepoch 138 || Epoch_D_Loss:0.0215 ||Epoch_G_Loss:0.9015\ntimer:  0.7233 sec.\n-------------\nEpoch 139/200\n-------------\n(train)\n-------------\nepoch 139 || Epoch_D_Loss:0.0041 ||Epoch_G_Loss:0.8207\ntimer:  0.7242 sec.\n-------------\nEpoch 140/200\n-------------\n(train)\n-------------\nepoch 140 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8500\ntimer:  0.7291 sec.\n-------------\nEpoch 141/200\n-------------\n(train)\n-------------\nepoch 141 || Epoch_D_Loss:0.0023 ||Epoch_G_Loss:0.8946\ntimer:  0.7330 sec.\n-------------\nEpoch 142/200\n-------------\n(train)\n-------------\nepoch 142 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.8823\ntimer:  0.7250 sec.\n-------------\nEpoch 143/200\n-------------\n(train)\n-------------\nepoch 143 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:1.0055\ntimer:  0.7228 sec.\n-------------\nEpoch 144/200\n-------------\n(train)\n-------------\nepoch 144 || Epoch_D_Loss:0.0616 ||Epoch_G_Loss:0.9888\ntimer:  0.7235 sec.\n-------------\nEpoch 145/200\n-------------\n(train)\n-------------\nepoch 145 || Epoch_D_Loss:0.0026 ||Epoch_G_Loss:0.8180\ntimer:  0.7214 sec.\n-------------\nEpoch 146/200\n-------------\n(train)\n-------------\nepoch 146 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8650\ntimer:  0.7242 sec.\n-------------\nEpoch 147/200\n-------------\n(train)\n-------------\nepoch 147 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8336\ntimer:  0.7255 sec.\n-------------\nEpoch 148/200\n-------------\n(train)\n-------------\nepoch 148 || Epoch_D_Loss:0.0011 ||Epoch_G_Loss:0.9114\ntimer:  0.7221 sec.\n-------------\nEpoch 149/200\n-------------\n(train)\n-------------\nepoch 149 || Epoch_D_Loss:0.0314 ||Epoch_G_Loss:0.9916\ntimer:  0.7217 sec.\n-------------\nEpoch 150/200\n-------------\n(train)\n-------------\nepoch 150 || Epoch_D_Loss:0.0159 ||Epoch_G_Loss:0.7072\ntimer:  0.7261 sec.\n-------------\nEpoch 151/200\n-------------\n(train)\n-------------\nepoch 151 || Epoch_D_Loss:0.0018 ||Epoch_G_Loss:0.8146\ntimer:  0.7238 sec.\n-------------\nEpoch 152/200\n-------------\n(train)\n-------------\nepoch 152 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8835\ntimer:  0.7259 sec.\n-------------\nEpoch 153/200\n-------------\n(train)\n-------------\nepoch 153 || Epoch_D_Loss:0.0078 ||Epoch_G_Loss:1.0363\ntimer:  0.7235 sec.\n-------------\nEpoch 154/200\n-------------\n(train)\n-------------\nepoch 154 || Epoch_D_Loss:0.0333 ||Epoch_G_Loss:0.8053\ntimer:  0.7234 sec.\n-------------\nEpoch 155/200\n-------------\n(train)\n-------------\nepoch 155 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.8146\ntimer:  0.7232 sec.\n-------------\nEpoch 156/200\n-------------\n(train)\n-------------\nepoch 156 || Epoch_D_Loss:0.0024 ||Epoch_G_Loss:0.8763\ntimer:  0.7245 sec.\n-------------\nEpoch 157/200\n-------------\n(train)\n-------------\nepoch 157 || Epoch_D_Loss:0.0037 ||Epoch_G_Loss:0.8991\ntimer:  0.7251 sec.\n-------------\nEpoch 158/200\n-------------\n(train)\n-------------\nepoch 158 || Epoch_D_Loss:0.0352 ||Epoch_G_Loss:0.9261\ntimer:  0.7250 sec.\n-------------\nEpoch 159/200\n-------------\n(train)\n-------------\nepoch 159 || Epoch_D_Loss:0.0104 ||Epoch_G_Loss:0.7463\ntimer:  0.7217 sec.\n-------------\nEpoch 160/200\n-------------\n(train)\n-------------\nepoch 160 || Epoch_D_Loss:0.0028 ||Epoch_G_Loss:0.8617\ntimer:  0.7248 sec.\n-------------\nEpoch 161/200\n-------------\n(train)\n-------------\nepoch 161 || Epoch_D_Loss:0.0021 ||Epoch_G_Loss:0.9127\ntimer:  0.7300 sec.\n-------------\nEpoch 162/200\n-------------\n(train)\n-------------\nepoch 162 || Epoch_D_Loss:0.0187 ||Epoch_G_Loss:0.8745\ntimer:  0.7322 sec.\n-------------\nEpoch 163/200\n-------------\n(train)\n-------------\nepoch 163 || Epoch_D_Loss:0.0025 ||Epoch_G_Loss:0.8871\ntimer:  0.7360 sec.\n-------------\nEpoch 164/200\n-------------\n(train)\n-------------\nepoch 164 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.8840\ntimer:  0.7259 sec.\n-------------\nEpoch 165/200\n-------------\n(train)\n-------------\nepoch 165 || Epoch_D_Loss:0.0290 ||Epoch_G_Loss:0.9592\ntimer:  0.7324 sec.\n-------------\nEpoch 166/200\n-------------\n(train)\n-------------\nepoch 166 || Epoch_D_Loss:0.0095 ||Epoch_G_Loss:0.8147\ntimer:  0.7241 sec.\n-------------\nEpoch 167/200\n-------------\n(train)\n-------------\nepoch 167 || Epoch_D_Loss:0.0017 ||Epoch_G_Loss:0.8852\ntimer:  0.7227 sec.\n-------------\nEpoch 168/200\n-------------\n(train)\n-------------\nepoch 168 || Epoch_D_Loss:0.0124 ||Epoch_G_Loss:0.8975\ntimer:  0.7256 sec.\n-------------\nEpoch 169/200\n-------------\n(train)\n-------------\nepoch 169 || Epoch_D_Loss:0.0127 ||Epoch_G_Loss:0.8617\ntimer:  0.7270 sec.\n-------------\nEpoch 170/200\n-------------\n(train)\n-------------\nepoch 170 || Epoch_D_Loss:0.0043 ||Epoch_G_Loss:0.8565\ntimer:  0.7238 sec.\n-------------\nEpoch 171/200\n-------------\n(train)\n-------------\nepoch 171 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.9055\ntimer:  0.7263 sec.\n-------------\nEpoch 172/200\n-------------\n(train)\n-------------\nepoch 172 || Epoch_D_Loss:0.0300 ||Epoch_G_Loss:0.9339\ntimer:  0.7229 sec.\n-------------\nEpoch 173/200\n-------------\n(train)\n-------------\nepoch 173 || Epoch_D_Loss:0.0323 ||Epoch_G_Loss:0.7781\ntimer:  0.7257 sec.\n-------------\nEpoch 174/200\n-------------\n(train)\n-------------\nepoch 174 || Epoch_D_Loss:0.0057 ||Epoch_G_Loss:0.7429\ntimer:  0.7237 sec.\n-------------\nEpoch 175/200\n-------------\n(train)\n-------------\nepoch 175 || Epoch_D_Loss:0.0027 ||Epoch_G_Loss:0.8056\ntimer:  0.7228 sec.\n-------------\nEpoch 176/200\n-------------\n(train)\n-------------\nepoch 176 || Epoch_D_Loss:0.0013 ||Epoch_G_Loss:0.7961\ntimer:  0.7236 sec.\n-------------\nEpoch 177/200\n-------------\n(train)\n-------------\nepoch 177 || Epoch_D_Loss:0.0009 ||Epoch_G_Loss:0.8577\ntimer:  0.7254 sec.\n-------------\nEpoch 178/200\n-------------\n(train)\n-------------\nepoch 178 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:1.0004\ntimer:  0.7239 sec.\n-------------\nEpoch 179/200\n-------------\n(train)\n-------------\nepoch 179 || Epoch_D_Loss:0.0479 ||Epoch_G_Loss:0.8469\ntimer:  0.7232 sec.\n-------------\nEpoch 180/200\n-------------\n(train)\n-------------\nepoch 180 || Epoch_D_Loss:0.0031 ||Epoch_G_Loss:0.6928\ntimer:  0.7236 sec.\n-------------\nEpoch 181/200\n-------------\n(train)\n-------------\nepoch 181 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.8195\ntimer:  0.7271 sec.\n-------------\nEpoch 182/200\n-------------\n(train)\n-------------\nepoch 182 || Epoch_D_Loss:0.0008 ||Epoch_G_Loss:0.8560\ntimer:  0.7243 sec.\n-------------\nEpoch 183/200\n-------------\n(train)\n-------------\nepoch 183 || Epoch_D_Loss:0.0191 ||Epoch_G_Loss:1.0310\ntimer:  0.7217 sec.\n-------------\nEpoch 184/200\n-------------\n(train)\n-------------\nepoch 184 || Epoch_D_Loss:0.0085 ||Epoch_G_Loss:0.7468\ntimer:  0.7303 sec.\n-------------\nEpoch 185/200\n-------------\n(train)\n-------------\nepoch 185 || Epoch_D_Loss:0.0032 ||Epoch_G_Loss:0.9024\ntimer:  0.7321 sec.\n-------------\nEpoch 186/200\n-------------\n(train)\n-------------\nepoch 186 || Epoch_D_Loss:0.0016 ||Epoch_G_Loss:0.8672\ntimer:  0.7283 sec.\n-------------\nEpoch 187/200\n-------------\n(train)\n-------------\nepoch 187 || Epoch_D_Loss:0.0476 ||Epoch_G_Loss:0.9672\ntimer:  0.7208 sec.\n-------------\nEpoch 188/200\n-------------\n(train)\n-------------\nepoch 188 || Epoch_D_Loss:0.0077 ||Epoch_G_Loss:0.7749\ntimer:  0.7255 sec.\n-------------\nEpoch 189/200\n-------------\n(train)\n-------------\nepoch 189 || Epoch_D_Loss:0.0045 ||Epoch_G_Loss:0.8037\ntimer:  0.7221 sec.\n-------------\nEpoch 190/200\n-------------\n(train)\n-------------\nepoch 190 || Epoch_D_Loss:0.0031 ||Epoch_G_Loss:0.8678\ntimer:  0.7244 sec.\n-------------\nEpoch 191/200\n-------------\n(train)\n-------------\nepoch 191 || Epoch_D_Loss:0.0010 ||Epoch_G_Loss:0.9078\ntimer:  0.7223 sec.\n-------------\nEpoch 192/200\n-------------\n(train)\n-------------\nepoch 192 || Epoch_D_Loss:0.0018 ||Epoch_G_Loss:1.0057\ntimer:  0.7271 sec.\n-------------\nEpoch 193/200\n-------------\n(train)\n-------------\nepoch 193 || Epoch_D_Loss:0.0322 ||Epoch_G_Loss:0.8226\ntimer:  0.7226 sec.\n-------------\nEpoch 194/200\n-------------\n(train)\n-------------\nepoch 194 || Epoch_D_Loss:0.0012 ||Epoch_G_Loss:0.8305\ntimer:  0.7231 sec.\n-------------\nEpoch 195/200\n-------------\n(train)\n-------------\nepoch 195 || Epoch_D_Loss:0.0020 ||Epoch_G_Loss:0.9072\ntimer:  0.7225 sec.\n-------------\nEpoch 196/200\n-------------\n(train)\n-------------\nepoch 196 || Epoch_D_Loss:0.0049 ||Epoch_G_Loss:1.0098\ntimer:  0.7306 sec.\n-------------\nEpoch 197/200\n-------------\n(train)\n-------------\nepoch 197 || Epoch_D_Loss:0.0216 ||Epoch_G_Loss:0.9132\ntimer:  0.7397 sec.\n-------------\nEpoch 198/200\n-------------\n(train)\n-------------\nepoch 198 || Epoch_D_Loss:0.0172 ||Epoch_G_Loss:0.8279\ntimer:  0.7386 sec.\n-------------\nEpoch 199/200\n-------------\n(train)\n-------------\nepoch 199 || Epoch_D_Loss:0.0021 ||Epoch_G_Loss:0.8379\ntimer:  0.7345 sec.\n\n\n\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# 입력 난수\nbatch_size = 8\nz_dim = 20\nfixed_z = torch.randn(batch_size, z_dim)\nfixed_z = fixed_z.view(fixed_z.size(0), fixed_z.size(1), 1, 1)\n\n# 화상 생성\nG_update.eval()\nfake_images = G_update(fixed_z.to(device))\n\n# 훈련 데이터\nbatch_iterator = iter(train_dataloader)  # 반복자로 변환\nimges = next(batch_iterator)  # 1번째 요소를 꺼낸다\n\n\n# 출력\nfig = plt.figure(figsize=(15, 6))\nfor i in range(0, 5):\n    # 상단에 훈련 데이터를,\n    plt.subplot(2, 5, i+1)\n    plt.imshow(imges[i][0].cpu().detach().numpy(), 'gray')\n\n    # 하단에 생성 데이터를 표시한다\n    plt.subplot(2, 5, 5+i+1)\n    plt.imshow(fake_images[i][0].cpu().detach().numpy(), 'gray')"
  },
  {
    "objectID": "docs/DL/posts/DL_RNN(1).html",
    "href": "docs/DL/posts/DL_RNN(1).html",
    "title": "RNN part1",
    "section": "",
    "text": "RNN은 이전 결과 혹은 이전 데이터를 사용하여 미래의 값을 예측하는 신경망이지만 이번에 다룰 임베딩 레이어와 선형변환층을 사용한 network는 과거의 데이터는 사용하지 않기 때문에 RNN이라고는 할 수 없음\nRNN과 network의 구조를 비슷하게 만들어 RNN의 필요성과 구조를 눈에 익히기 위해 만듬\n\n\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n\nsig = torch.nn.Sigmoid()\nsoft = torch.nn.Softmax(dim=1)\ntanh = torch.nn.Tanh()\n\n- tanh activation function - tanh는 -1부터 1사이의 값을 출력하는 활성화 함수\n\n_x = torch.linspace(-5,5,100)\nplt.plot(_x,tanh(_x))\n\n\n\n\n- 문자를 mapping을 통해 숫자로 변환\n\n딥러닝을 돌리기 위해서는 input이 문자가 아닌 숫자여야 함\n\nmapping은 key와 value로 구성된 딕셔너리가 들어감\n\ntxt는 키값이 되어 value인 숫자가 출력됨\n\n\ndef f(txt, mapping):\n    return [mapping[key] for key in txt]\n\n\ntxt = ['a', 'b', 'c']\nmapping = {'a':0, 'b':1, 'c':2}\nf(txt,mapping)\n\n[0, 1, 2]\n\n\n\ntxt = list('ab')*100\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'a', 'b', 'a'], ['b', 'a', 'b', 'a', 'b'])\n\n\n-class를 사용해 network를 만드는 방법\n\ndef __init__에서는 사용할 변수(레이어)를 선언\n\ndef forward()에서는 __init__에서 선언한 변수(레이어)를 어떻게 연산하여 yhat을 정의할지를 코드로 구현한 후 yhat을 return\n\nclass Mynet1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.Sigmoid()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n\n\n\n\n\nmapping 단계에서 원핫 인코딩을 진행\n\n\ntxt = list('ab')*100\nmapping = {'a':[1, 0], 'b':[0, 1]}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,2).to('cuda:0')\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,2).to('cuda:0')\nx[:5], y[:5]\n\n(tensor([[1., 0.],\n         [0., 1.],\n         [1., 0.],\n         [0., 1.],\n         [1., 0.]], device='cuda:0'),\n tensor([[0., 1.],\n         [1., 0.],\n         [0., 1.],\n         [1., 0.],\n         [0., 1.]], device='cuda:0'))\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=2,out_features=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=2,bias=False)\n).to('cuda:0')\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\n값이 크면 노란색, 값이 작으면 보라색으로 출력됨\n\n\nfig,ax = plt.subplots(1,2)\nax[0].imshow(y[:5].to('cpu'))\nax[1].imshow(soft(net(x[:5]).to('cpu')).data)\n\n&lt;matplotlib.image.AxesImage at 0x20fe7939c40&gt;\n\n\n\n\n\n\n\n\n\ntorch.nn.Embedding()을 사용하면 자동으로 원핫 인코딩을 한 후 선형변환을 함\n\n\nmapping = {'a':0,'b':1,'c':2}\nx = torch.tensor(f(list('abc')*100,mapping)).to('cuda:0')\ny = torch.tensor(f(list('bca')*100,mapping)).to('cuda:0')\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 0, 1], device='cuda:0'),\n tensor([1, 2, 0, 1, 2], device='cuda:0'))\n\n\n\ntorch.manual_seed(43052)\nebdd = torch.nn.Embedding(num_embeddings=3,embedding_dim=1).to('cuda:0')\nebdd(x)[:5]\n\ntensor([[-0.8178],\n        [-0.7052],\n        [-0.5843],\n        [-0.8178],\n        [-0.7052]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nebdd.weight\n\nParameter containing:\ntensor([[-0.8178],\n        [-0.7052],\n        [-0.5843]], device='cuda:0', requires_grad=True)\n\n\n\nembedding layer는 a, b, c에 적용할 weight가 각각 있어서 weight가 3개임\n\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).to('cuda:0')\ny = torch.tensor(f(txt_y,mapping)).to('cuda:0')\nx[:5], y[:5]\n\n(tensor([0, 1, 0, 1, 0], device='cuda:0'),\n tensor([1, 0, 1, 0, 1], device='cuda:0'))\n\n\n\nembedding layer에서 원핫인코딩이 실행되어야 하므로 실수형이 아닌 정수형으로 데이터를 입력해줌\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=2,embedding_dim=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=2)\n).to('cuda:0')\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.imshow(soft(net(x[:5]).to('cpu')).data)\n\n&lt;matplotlib.image.AxesImage at 0x20f857618b0&gt;\n\n\n\n\n\n\n\n\n\ntxt = list('abcde')*100\nmapping = {'a':0,'b':1,'c':2,'d':3,'e':4}\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\nx = torch.tensor(f(txt_x, mapping)).to('cuda:0')\ny = torch.tensor(f(txt_y, mapping)).to('cuda:0')\n\n\n1개의 은닉노드는 2개의 문자를 표현, 2개의 은닉노드는 4개의 문자를 표현\n-&gt; \\(2^n\\)으로 문자를 표현할 수 있음\n5개의 문자열이므로 \\(n = 3\\)이 적합함\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=5, embedding_dim = 3),\n    torch.nn.Tanh(),\n    torch.nn.Linear(3, 5)\n).to('cuda:0')\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(net.parameters())\n\n\nfor epoch in range(5000):\n    #\n    yhat = net(x)\n    #\n    loss = loss_fn(yhat, y)\n    #\n    loss.backward()\n    #\n    optim.step()\n    optim.zero_grad()\n\n\nnet[-1]\n\nLinear(in_features=3, out_features=5, bias=True)\n\n\n\nhidden = net[:-1](x).to('cpu').data\nyhat = soft(net(x)).to('cpu').data\n\ncombined  = torch.concat([hidden, net(x).to('cpu').data, yhat],axis=1)\n\n\nplt.matshow(combined[:15],vmin=-5,vmax=5,cmap='bwr')\nplt.xticks(range(13), labels=[r'$h$',r'$h$',r'$h$',\n                              r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$y=e?$',\n                              r'$P(y=A)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$',r'$P(y=e)$'],size=13)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')\n\n\n\n\n\n\n\n\n\n\n위와 같은 문제는 직전 하나의 문자만 보는 것이 아닌 2개의 문자를 보아야 예측을 할 수 있음\n\n평범한 임베딩 레이어(선형 변환 레이어)를 사용하면 이와 같은 문제를 해결할 수 없음\n\n\ntxt = list('AbAcAd')*100\nmapping = {'A':0, 'b':1,'c':2, 'd':3}\n\n\nx = torch.tensor(f(txt[:-1],mapping))\ny = torch.tensor(f(txt[1:],mapping))\n\n\nx = torch.nn.functional.one_hot(x).float()\ny = torch.nn.functional.one_hot(y).float()\n\n\nx = x.cuda()\ny = y.cuda()\n\n\nclass Hnet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(in_features=4,out_features=2)\n        self.tanh = torch.nn.Tanh()\n    def forward(self,x):\n        hidden = self.tanh(self.i2h(x))\n        return hidden\n\n\ntorch.manual_seed(43052)\n\nhnet = Hnet().cuda()\nlinr = torch.nn.Linear(in_features=2,out_features=4).cuda()\n\nloss_fn = torch.nn.CrossEntropyLoss() \n\noptim = torch.optim.Adam(list(hnet.parameters())+list(linr.parameters()))\n\n\nT = len(x) \nfor epoc in range(20): \n    ## 1~2\n    loss = 0 \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = hnet(xt) \n        ot = linr(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optim.step()\n    optim.zero_grad()\n\n\nlinr(hnet(x))\n\ntensor([[-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.2912,  0.8140, -0.2032,  0.0178],\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        ...,\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.1065,  0.6307, -0.0874,  0.1821],\n        [-0.3589,  0.7921, -0.1970, -0.0302]], device='cuda:0',\n       grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nplt.matshow(linr(hnet(x)).cpu().data[:8],vmin=-1,vmax=1,cmap='bwr')\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x2102411f610&gt;"
  },
  {
    "objectID": "docs/DL/posts/DL_RNN(1).html#준비",
    "href": "docs/DL/posts/DL_RNN(1).html#준비",
    "title": "RNN part1",
    "section": "",
    "text": "sig = torch.nn.Sigmoid()\nsoft = torch.nn.Softmax(dim=1)\ntanh = torch.nn.Tanh()\n\n- tanh activation function - tanh는 -1부터 1사이의 값을 출력하는 활성화 함수\n\n_x = torch.linspace(-5,5,100)\nplt.plot(_x,tanh(_x))\n\n\n\n\n- 문자를 mapping을 통해 숫자로 변환\n\n딥러닝을 돌리기 위해서는 input이 문자가 아닌 숫자여야 함\n\nmapping은 key와 value로 구성된 딕셔너리가 들어감\n\ntxt는 키값이 되어 value인 숫자가 출력됨\n\n\ndef f(txt, mapping):\n    return [mapping[key] for key in txt]\n\n\ntxt = ['a', 'b', 'c']\nmapping = {'a':0, 'b':1, 'c':2}\nf(txt,mapping)\n\n[0, 1, 2]\n\n\n\ntxt = list('ab')*100\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'a', 'b', 'a'], ['b', 'a', 'b', 'a', 'b'])\n\n\n-class를 사용해 network를 만드는 방법\n\ndef __init__에서는 사용할 변수(레이어)를 선언\n\ndef forward()에서는 __init__에서 선언한 변수(레이어)를 어떻게 연산하여 yhat을 정의할지를 코드로 구현한 후 yhat을 return\n\nclass Mynet1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.Sigmoid()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat"
  },
  {
    "objectID": "docs/DL/posts/DL_RNN(1).html#원핫-인코딩",
    "href": "docs/DL/posts/DL_RNN(1).html#원핫-인코딩",
    "title": "RNN part1",
    "section": "",
    "text": "mapping 단계에서 원핫 인코딩을 진행\n\n\ntxt = list('ab')*100\nmapping = {'a':[1, 0], 'b':[0, 1]}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,2).to('cuda:0')\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,2).to('cuda:0')\nx[:5], y[:5]\n\n(tensor([[1., 0.],\n         [0., 1.],\n         [1., 0.],\n         [0., 1.],\n         [1., 0.]], device='cuda:0'),\n tensor([[0., 1.],\n         [1., 0.],\n         [0., 1.],\n         [1., 0.],\n         [0., 1.]], device='cuda:0'))\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=2,out_features=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=2,bias=False)\n).to('cuda:0')\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\n값이 크면 노란색, 값이 작으면 보라색으로 출력됨\n\n\nfig,ax = plt.subplots(1,2)\nax[0].imshow(y[:5].to('cpu'))\nax[1].imshow(soft(net(x[:5]).to('cpu')).data)\n\n&lt;matplotlib.image.AxesImage at 0x20fe7939c40&gt;\n\n\n\n\n\n\n\n\n\ntorch.nn.Embedding()을 사용하면 자동으로 원핫 인코딩을 한 후 선형변환을 함\n\n\nmapping = {'a':0,'b':1,'c':2}\nx = torch.tensor(f(list('abc')*100,mapping)).to('cuda:0')\ny = torch.tensor(f(list('bca')*100,mapping)).to('cuda:0')\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 0, 1], device='cuda:0'),\n tensor([1, 2, 0, 1, 2], device='cuda:0'))\n\n\n\ntorch.manual_seed(43052)\nebdd = torch.nn.Embedding(num_embeddings=3,embedding_dim=1).to('cuda:0')\nebdd(x)[:5]\n\ntensor([[-0.8178],\n        [-0.7052],\n        [-0.5843],\n        [-0.8178],\n        [-0.7052]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nebdd.weight\n\nParameter containing:\ntensor([[-0.8178],\n        [-0.7052],\n        [-0.5843]], device='cuda:0', requires_grad=True)\n\n\n\nembedding layer는 a, b, c에 적용할 weight가 각각 있어서 weight가 3개임\n\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).to('cuda:0')\ny = torch.tensor(f(txt_y,mapping)).to('cuda:0')\nx[:5], y[:5]\n\n(tensor([0, 1, 0, 1, 0], device='cuda:0'),\n tensor([1, 0, 1, 0, 1], device='cuda:0'))\n\n\n\nembedding layer에서 원핫인코딩이 실행되어야 하므로 실수형이 아닌 정수형으로 데이터를 입력해줌\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=2,embedding_dim=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=2)\n).to('cuda:0')\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.imshow(soft(net(x[:5]).to('cpu')).data)\n\n&lt;matplotlib.image.AxesImage at 0x20f857618b0&gt;\n\n\n\n\n\n\n\n\n\ntxt = list('abcde')*100\nmapping = {'a':0,'b':1,'c':2,'d':3,'e':4}\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\nx = torch.tensor(f(txt_x, mapping)).to('cuda:0')\ny = torch.tensor(f(txt_y, mapping)).to('cuda:0')\n\n\n1개의 은닉노드는 2개의 문자를 표현, 2개의 은닉노드는 4개의 문자를 표현\n-&gt; \\(2^n\\)으로 문자를 표현할 수 있음\n5개의 문자열이므로 \\(n = 3\\)이 적합함\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=5, embedding_dim = 3),\n    torch.nn.Tanh(),\n    torch.nn.Linear(3, 5)\n).to('cuda:0')\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(net.parameters())\n\n\nfor epoch in range(5000):\n    #\n    yhat = net(x)\n    #\n    loss = loss_fn(yhat, y)\n    #\n    loss.backward()\n    #\n    optim.step()\n    optim.zero_grad()\n\n\nnet[-1]\n\nLinear(in_features=3, out_features=5, bias=True)\n\n\n\nhidden = net[:-1](x).to('cpu').data\nyhat = soft(net(x)).to('cpu').data\n\ncombined  = torch.concat([hidden, net(x).to('cpu').data, yhat],axis=1)\n\n\nplt.matshow(combined[:15],vmin=-5,vmax=5,cmap='bwr')\nplt.xticks(range(13), labels=[r'$h$',r'$h$',r'$h$',\n                              r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$y=e?$',\n                              r'$P(y=A)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$',r'$P(y=e)$'],size=13)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')"
  },
  {
    "objectID": "docs/DL/posts/DL_RNN(1).html#한계-abacad",
    "href": "docs/DL/posts/DL_RNN(1).html#한계-abacad",
    "title": "RNN part1",
    "section": "",
    "text": "위와 같은 문제는 직전 하나의 문자만 보는 것이 아닌 2개의 문자를 보아야 예측을 할 수 있음\n\n평범한 임베딩 레이어(선형 변환 레이어)를 사용하면 이와 같은 문제를 해결할 수 없음\n\n\ntxt = list('AbAcAd')*100\nmapping = {'A':0, 'b':1,'c':2, 'd':3}\n\n\nx = torch.tensor(f(txt[:-1],mapping))\ny = torch.tensor(f(txt[1:],mapping))\n\n\nx = torch.nn.functional.one_hot(x).float()\ny = torch.nn.functional.one_hot(y).float()\n\n\nx = x.cuda()\ny = y.cuda()\n\n\nclass Hnet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(in_features=4,out_features=2)\n        self.tanh = torch.nn.Tanh()\n    def forward(self,x):\n        hidden = self.tanh(self.i2h(x))\n        return hidden\n\n\ntorch.manual_seed(43052)\n\nhnet = Hnet().cuda()\nlinr = torch.nn.Linear(in_features=2,out_features=4).cuda()\n\nloss_fn = torch.nn.CrossEntropyLoss() \n\noptim = torch.optim.Adam(list(hnet.parameters())+list(linr.parameters()))\n\n\nT = len(x) \nfor epoc in range(20): \n    ## 1~2\n    loss = 0 \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = hnet(xt) \n        ot = linr(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optim.step()\n    optim.zero_grad()\n\n\nlinr(hnet(x))\n\ntensor([[-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.2912,  0.8140, -0.2032,  0.0178],\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        ...,\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.1065,  0.6307, -0.0874,  0.1821],\n        [-0.3589,  0.7921, -0.1970, -0.0302]], device='cuda:0',\n       grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nplt.matshow(linr(hnet(x)).cpu().data[:8],vmin=-1,vmax=1,cmap='bwr')\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x2102411f610&gt;"
  },
  {
    "objectID": "docs/DL/posts/DL_RNN(2).html",
    "href": "docs/DL/posts/DL_RNN(2).html",
    "title": "RNN part2",
    "section": "",
    "text": "기존의 선형변환(임베딩 레이어, Linear 레이어)는 과거의 데이터를 사용하지 못해서 AbAcAd와 같은 예제를 해결하지 못함\n\nRNN이나 LSTM과 같은 레이어를 사용하면 위와 같은 예제를 해결할 수 있음\n\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nfrom tqdm import tqdm\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \nsoft = torch.nn.Softmax(dim=1)\n\n- optimizer에 입력하는 network.parameters()\n\nnetwork.parameters()자리에는 iterable object를 입력해야 함\n\n대표적인 iterable object는 리스트가 있음\n\n만약 optimizer를 사용해 2개 이상의 값들을 업데이트하고 싶다면 이들을 리스트로 묶어서 network.parameters() 자리에 입력을 해주면 됨\n\n\n\n\ntxt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\nx = torch.tensor(f(txt_x,{'A':0,'b':1,'c':2,'d':3}))\ny = torch.tensor(f(txt_y,{'A':0,'b':1,'c':2,'d':3}))\n\n\nx= torch.nn.functional.one_hot(x).float()\ny= torch.nn.functional.one_hot(y).float()\n\n\nx\n\ntensor([[1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [1., 0., 0., 0.],\n        ...,\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.],\n        [1., 0., 0., 0.]])\n\n\nRNN의 원리를 수식으로 설명하면 아래와 같다.\n\\(h_t = tanh(x_{input}W_{ih} + h_{t-1}W_{hh} + bias_h)\\) (여기에서 \\(h_{t-1}\\)은 과거의 데이터를 의미한다)\n\\(output = W_{ho}h_t + bias_o\\)\n\\(yhat = softmax(output)\\)\n\\(h_1\\)의 경우에는 과거의 데이터가 없으므로 처음 시작할 때는 0으로 만들어진 \\(h_t\\)를 만들어줘야 함\n\n\n\n위의 수식대로 순환신경망 정의하기\n\n\ntorch.manual_seed(43052)\n\nrnncell = torch.nn.RNNCell(4, 2)\n\n\ntorch.manual_seed(43052)\n\nLin = torch.nn.Linear(2, 4)\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(list(rnncell.parameters())+list(Lin.parameters()))\n\n\nT = len(x)\nfor epoch in tqdm(range(5000)):\n    loss = 0\n    ht = torch.zeros(1,2)\n\n    for t in range(T):\n        xt, yt = x[[t]], y[[t]]\n        ht = rnncell(xt, ht)\n        ot = Lin(ht)\n\n        loss = loss + loss_fn(ot, yt)\n\n    loss.backward()\n\n    optim.step()\n    optim.zero_grad()\n\n100%|██████████| 5000/5000 [13:40&lt;00:00,  6.10it/s]\n\n\n\nhidden = torch.zeros(T, 2)\n_water = torch.zeros(1,2)\n\nhidden[[0]] = rnncell(x[[0]], _water)\n\nfor t in range(1, T):\n    hidden[[t]] = rnncell(x[[t]], hidden[[t-1]])\n\n\nhidden\n\ntensor([[-0.9912, -0.9117],\n        [ 0.0698, -1.0000],\n        [-0.9927, -0.9682],\n        ...,\n        [-0.9935, -0.9315],\n        [ 0.5777, -1.0000],\n        [-0.9960, -0.0109]], grad_fn=&lt;IndexPutBackward0&gt;)\n\n\n\nyhat = soft(Lin(hidden))\n\nplt.matshow(yhat[-15:].data, cmap ='bwr')\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x13d2bdfcbb0&gt;\n\n\n\n\n\n\n\n\n\n\nrnn = torch.nn.RNN(4, 2)\n\n\nrnn.weight_hh_l0.data = rnncell.weight_hh.data \nrnn.weight_ih_l0.data = rnncell.weight_ih.data\nrnn.bias_hh_l0.data = rnncell.bias_hh.data\nrnn.bias_ih_l0.data = rnncell.bias_ih.data\n\n\n_water = torch.zeros(1, 2)\nrnncell(x,_water)\n\nRuntimeError: Input batch size 599 doesn't match hidden0 batch size 1\n\n\n\nRNNcell은 위와 같이 입력을 주면 에러가 발생함\n\nfor t in range(T):\n    xt, yt = x[[t]], y[[t]]\n    ht = rnncell(xt, ht)\n    ot = Lin(ht)\n\n그렇기 때문에 위와 같은 방식으로 학습을 시켜야 함\n\n\n_water = torch.zeros(1, 2)\nrnn(x,_water)\n\n(tensor([[-0.9912, -0.9117],\n         [ 0.0698, -1.0000],\n         [-0.9927, -0.9682],\n         ...,\n         [-0.9935, -0.9315],\n         [ 0.5777, -1.0000],\n         [-0.9960, -0.0109]], grad_fn=&lt;SqueezeBackward1&gt;),\n tensor([[-0.9960, -0.0109]], grad_fn=&lt;SqueezeBackward1&gt;))\n\n\n\n하지만 RNN은 for문 안에 넣을 필요없이 한번에 연산이 가능함\n\n같은 weight와 bias를 사용하지만 편의성(?)측면에서 RNN이 좋음\n\n\n\n\n\nrnn = torch.nn.RNN(4,2).to('cuda:0')\n\n\ntorch.manual_seed(43052)\n_rnncell = torch.nn.RNNCell(4, 2).to('cuda:0')\n\nrnn.weight_hh_l0.data = _rnncell.weight_hh.data \nrnn.weight_ih_l0.data = _rnncell.weight_ih.data\nrnn.bias_hh_l0.data = _rnncell.bias_hh.data\nrnn.bias_ih_l0.data = _rnncell.bias_ih.data\n\n\ntorch.manual_seed(43052)\nLin = torch.nn.Linear(2, 4).to('cuda:0')\n\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(list(rnn.parameters()) + list(Lin.parameters()))\n\n\n_water = torch.zeros(1,2).to('cuda:0')\n\nfor epoch in tqdm(range(5000)):\n    hidden, ht = rnn(x.to('cuda:0'), _water)\n    output = Lin(hidden)\n    \n    loss = loss_fn(output, y.to('cuda:0'))\n\n    loss.backward()\n\n    optim.step()\n    optim.zero_grad()\n\n  0%|          | 0/5000 [00:00&lt;?, ?it/s]c:\\Users\\default.DESKTOP-HUJV032\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:476: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:968.)\n  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,\n100%|██████████| 5000/5000 [00:23&lt;00:00, 215.98it/s]\n\n\n- rnn을 실행하면 2개가 출력되는데 이는 hidden과 ht이다\n\nhidden은 x를 계산한 모든 값들이 출력된다\n\nht는 마지막의 값만 저장되어있는데 이는 ht값들이 쌓여서 hidden을 만든다고 생각하면 된다.\n\n그렇기 때문에 hidden을 사용해 loss를 계산하여야 한다.\n\n\nyhat = soft(output)\n\nplt.matshow(yhat.data[:15].to('cpu'), cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x13d29080130&gt;\n\n\n\n\n\n\nplt.matshow(yhat.data[-15:].to('cpu'), cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x13d29060e50&gt;\n\n\n\n\n\n\n초반에는 예측을 틀리지만 마지막에서는 완벽한 성능을 보여줌\n\n\ncombined = torch.concat([hidden, yhat], axis = 1)\nplt.matshow(combined[-15:].data.to('cpu'),cmap='bwr')\n\n&lt;matplotlib.image.AxesImage at 0x13d310f79d0&gt;\n\n\n\n\n\n\n\n\n\nx = x.to('cuda:0')\ny = y.to('cuda:0')\n\n\ntorch.manual_seed(43052)\nrnn = torch.nn.RNN(4,3).to('cuda:0')\nLin = torch.nn.Linear(3, 4).to('cuda:0')\n\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(list(rnn.parameters()) + list(Lin.parameters()))\n\n\n_water = torch.zeros(1,3).to('cuda:0')\n\nfor epoch in tqdm(range(5000)):\n    hidden, ht = rnn(x, _water)\n    output = Lin(hidden)\n    \n    loss = loss_fn(output, y)\n\n    loss.backward()\n\n    optim.step()\n    optim.zero_grad()\n\n100%|██████████| 5000/5000 [00:20&lt;00:00, 245.12it/s]\n\n\n- rnn을 실행하면 2개가 출력되는데 이는 hidden과 ht이다\n\nhidden은 x를 계산한 모든 값들이 출력된다.\n\nht는 마지막의 값만 저장되어있는데 이는 ht값들이 쌓여서 hidden을 만든다고 생각하면 된다.\n\n그렇기 때문에 hidden을 사용해 loss를 계산하여야 한다.\n\n\nyhat = soft(output)\n\nplt.matshow(yhat.data[:15].to('cpu'), cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x13d3d08e100&gt;\n\n\n\n\n\n\nplt.matshow(yhat.data[-15:].to('cpu'), cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x13d443a88b0&gt;\n\n\n\n\n\n\n히든 노드가 2개 였을 때는 초반에 틀렸지만 히든 노드가 3개 일때는 초반에도 다 맞추는 것을 확인할 수 있음\n\n\ncombined = torch.concat([hidden, yhat], axis = 1)\nplt.matshow(combined[-15:].data.to('cpu'),cmap='bwr')\n\n&lt;matplotlib.image.AxesImage at 0x13d44422f40&gt;\n\n\n\n\n\n히든 층과 yhat을 시각화 한 것들을 보면 같은 A이지만 hidden에서는 다른 방식으로 설명을 하고 있는 것을 알 수 있다.\n-&gt; 즉 겉보기로는 같은 A이지만 사실은 다른 뜻의 A이므로 A1,b,A2,c,A3,d로 생각을 해야 함"
  },
  {
    "objectID": "docs/DL/posts/DL_RNN(2).html#rnncell을-사용해-abacad-예제-해결하기",
    "href": "docs/DL/posts/DL_RNN(2).html#rnncell을-사용해-abacad-예제-해결하기",
    "title": "RNN part2",
    "section": "",
    "text": "txt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\nx = torch.tensor(f(txt_x,{'A':0,'b':1,'c':2,'d':3}))\ny = torch.tensor(f(txt_y,{'A':0,'b':1,'c':2,'d':3}))\n\n\nx= torch.nn.functional.one_hot(x).float()\ny= torch.nn.functional.one_hot(y).float()\n\n\nx\n\ntensor([[1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [1., 0., 0., 0.],\n        ...,\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.],\n        [1., 0., 0., 0.]])\n\n\nRNN의 원리를 수식으로 설명하면 아래와 같다.\n\\(h_t = tanh(x_{input}W_{ih} + h_{t-1}W_{hh} + bias_h)\\) (여기에서 \\(h_{t-1}\\)은 과거의 데이터를 의미한다)\n\\(output = W_{ho}h_t + bias_o\\)\n\\(yhat = softmax(output)\\)\n\\(h_1\\)의 경우에는 과거의 데이터가 없으므로 처음 시작할 때는 0으로 만들어진 \\(h_t\\)를 만들어줘야 함\n\n\n\n위의 수식대로 순환신경망 정의하기\n\n\ntorch.manual_seed(43052)\n\nrnncell = torch.nn.RNNCell(4, 2)\n\n\ntorch.manual_seed(43052)\n\nLin = torch.nn.Linear(2, 4)\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(list(rnncell.parameters())+list(Lin.parameters()))\n\n\nT = len(x)\nfor epoch in tqdm(range(5000)):\n    loss = 0\n    ht = torch.zeros(1,2)\n\n    for t in range(T):\n        xt, yt = x[[t]], y[[t]]\n        ht = rnncell(xt, ht)\n        ot = Lin(ht)\n\n        loss = loss + loss_fn(ot, yt)\n\n    loss.backward()\n\n    optim.step()\n    optim.zero_grad()\n\n100%|██████████| 5000/5000 [13:40&lt;00:00,  6.10it/s]\n\n\n\nhidden = torch.zeros(T, 2)\n_water = torch.zeros(1,2)\n\nhidden[[0]] = rnncell(x[[0]], _water)\n\nfor t in range(1, T):\n    hidden[[t]] = rnncell(x[[t]], hidden[[t-1]])\n\n\nhidden\n\ntensor([[-0.9912, -0.9117],\n        [ 0.0698, -1.0000],\n        [-0.9927, -0.9682],\n        ...,\n        [-0.9935, -0.9315],\n        [ 0.5777, -1.0000],\n        [-0.9960, -0.0109]], grad_fn=&lt;IndexPutBackward0&gt;)\n\n\n\nyhat = soft(Lin(hidden))\n\nplt.matshow(yhat[-15:].data, cmap ='bwr')\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x13d2bdfcbb0&gt;"
  },
  {
    "objectID": "docs/DL/posts/DL_RNN(2).html#rnn과-rnncell의-차이점",
    "href": "docs/DL/posts/DL_RNN(2).html#rnn과-rnncell의-차이점",
    "title": "RNN part2",
    "section": "",
    "text": "rnn = torch.nn.RNN(4, 2)\n\n\nrnn.weight_hh_l0.data = rnncell.weight_hh.data \nrnn.weight_ih_l0.data = rnncell.weight_ih.data\nrnn.bias_hh_l0.data = rnncell.bias_hh.data\nrnn.bias_ih_l0.data = rnncell.bias_ih.data\n\n\n_water = torch.zeros(1, 2)\nrnncell(x,_water)\n\nRuntimeError: Input batch size 599 doesn't match hidden0 batch size 1\n\n\n\nRNNcell은 위와 같이 입력을 주면 에러가 발생함\n\nfor t in range(T):\n    xt, yt = x[[t]], y[[t]]\n    ht = rnncell(xt, ht)\n    ot = Lin(ht)\n\n그렇기 때문에 위와 같은 방식으로 학습을 시켜야 함\n\n\n_water = torch.zeros(1, 2)\nrnn(x,_water)\n\n(tensor([[-0.9912, -0.9117],\n         [ 0.0698, -1.0000],\n         [-0.9927, -0.9682],\n         ...,\n         [-0.9935, -0.9315],\n         [ 0.5777, -1.0000],\n         [-0.9960, -0.0109]], grad_fn=&lt;SqueezeBackward1&gt;),\n tensor([[-0.9960, -0.0109]], grad_fn=&lt;SqueezeBackward1&gt;))\n\n\n\n하지만 RNN은 for문 안에 넣을 필요없이 한번에 연산이 가능함\n\n같은 weight와 bias를 사용하지만 편의성(?)측면에서 RNN이 좋음"
  },
  {
    "objectID": "docs/DL/posts/DL_RNN(2).html#rnn을-사용해-abacad-예제-해결하기-히든-노드-2개",
    "href": "docs/DL/posts/DL_RNN(2).html#rnn을-사용해-abacad-예제-해결하기-히든-노드-2개",
    "title": "RNN part2",
    "section": "",
    "text": "rnn = torch.nn.RNN(4,2).to('cuda:0')\n\n\ntorch.manual_seed(43052)\n_rnncell = torch.nn.RNNCell(4, 2).to('cuda:0')\n\nrnn.weight_hh_l0.data = _rnncell.weight_hh.data \nrnn.weight_ih_l0.data = _rnncell.weight_ih.data\nrnn.bias_hh_l0.data = _rnncell.bias_hh.data\nrnn.bias_ih_l0.data = _rnncell.bias_ih.data\n\n\ntorch.manual_seed(43052)\nLin = torch.nn.Linear(2, 4).to('cuda:0')\n\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(list(rnn.parameters()) + list(Lin.parameters()))\n\n\n_water = torch.zeros(1,2).to('cuda:0')\n\nfor epoch in tqdm(range(5000)):\n    hidden, ht = rnn(x.to('cuda:0'), _water)\n    output = Lin(hidden)\n    \n    loss = loss_fn(output, y.to('cuda:0'))\n\n    loss.backward()\n\n    optim.step()\n    optim.zero_grad()\n\n  0%|          | 0/5000 [00:00&lt;?, ?it/s]c:\\Users\\default.DESKTOP-HUJV032\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:476: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:968.)\n  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,\n100%|██████████| 5000/5000 [00:23&lt;00:00, 215.98it/s]\n\n\n- rnn을 실행하면 2개가 출력되는데 이는 hidden과 ht이다\n\nhidden은 x를 계산한 모든 값들이 출력된다\n\nht는 마지막의 값만 저장되어있는데 이는 ht값들이 쌓여서 hidden을 만든다고 생각하면 된다.\n\n그렇기 때문에 hidden을 사용해 loss를 계산하여야 한다.\n\n\nyhat = soft(output)\n\nplt.matshow(yhat.data[:15].to('cpu'), cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x13d29080130&gt;\n\n\n\n\n\n\nplt.matshow(yhat.data[-15:].to('cpu'), cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x13d29060e50&gt;\n\n\n\n\n\n\n초반에는 예측을 틀리지만 마지막에서는 완벽한 성능을 보여줌\n\n\ncombined = torch.concat([hidden, yhat], axis = 1)\nplt.matshow(combined[-15:].data.to('cpu'),cmap='bwr')\n\n&lt;matplotlib.image.AxesImage at 0x13d310f79d0&gt;"
  },
  {
    "objectID": "docs/DL/posts/DL_RNN(2).html#rnn을-사용해-abacad-예제-해결하기-히든-노드-3개",
    "href": "docs/DL/posts/DL_RNN(2).html#rnn을-사용해-abacad-예제-해결하기-히든-노드-3개",
    "title": "RNN part2",
    "section": "",
    "text": "x = x.to('cuda:0')\ny = y.to('cuda:0')\n\n\ntorch.manual_seed(43052)\nrnn = torch.nn.RNN(4,3).to('cuda:0')\nLin = torch.nn.Linear(3, 4).to('cuda:0')\n\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(list(rnn.parameters()) + list(Lin.parameters()))\n\n\n_water = torch.zeros(1,3).to('cuda:0')\n\nfor epoch in tqdm(range(5000)):\n    hidden, ht = rnn(x, _water)\n    output = Lin(hidden)\n    \n    loss = loss_fn(output, y)\n\n    loss.backward()\n\n    optim.step()\n    optim.zero_grad()\n\n100%|██████████| 5000/5000 [00:20&lt;00:00, 245.12it/s]\n\n\n- rnn을 실행하면 2개가 출력되는데 이는 hidden과 ht이다\n\nhidden은 x를 계산한 모든 값들이 출력된다.\n\nht는 마지막의 값만 저장되어있는데 이는 ht값들이 쌓여서 hidden을 만든다고 생각하면 된다.\n\n그렇기 때문에 hidden을 사용해 loss를 계산하여야 한다.\n\n\nyhat = soft(output)\n\nplt.matshow(yhat.data[:15].to('cpu'), cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x13d3d08e100&gt;\n\n\n\n\n\n\nplt.matshow(yhat.data[-15:].to('cpu'), cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x13d443a88b0&gt;\n\n\n\n\n\n\n히든 노드가 2개 였을 때는 초반에 틀렸지만 히든 노드가 3개 일때는 초반에도 다 맞추는 것을 확인할 수 있음\n\n\ncombined = torch.concat([hidden, yhat], axis = 1)\nplt.matshow(combined[-15:].data.to('cpu'),cmap='bwr')\n\n&lt;matplotlib.image.AxesImage at 0x13d44422f40&gt;\n\n\n\n\n\n히든 층과 yhat을 시각화 한 것들을 보면 같은 A이지만 hidden에서는 다른 방식으로 설명을 하고 있는 것을 알 수 있다.\n-&gt; 즉 겉보기로는 같은 A이지만 사실은 다른 뜻의 A이므로 A1,b,A2,c,A3,d로 생각을 해야 함"
  },
  {
    "objectID": "docs/DL/posts/DL_RNN(3).html",
    "href": "docs/DL/posts/DL_RNN(3).html",
    "title": "RNN part3",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nfrom tqdm import tqdm\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt]\n    \nsoft = torch.nn.Softmax(dim=1)\n\n\n\n\n\n\ntxt = list('abcabC')*100\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\nmapping = {'a':0, 'b':1, 'c':2, 'C':3}\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\nx = x.to('cuda:0')\ny = y.to('cuda:0')\n\n\ntorch.manual_seed(43052)\n\nrnn = torch.nn.RNN(4,3).to('cuda:0')\nlin = torch.nn.Linear(3,4).to('cuda:0')\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(list(rnn.parameters())+ list(lin.parameters()))\n\n\nfor epoch in tqdm(range(12000)):\n    _water = torch.zeros(1,3).to('cuda:0')\n    hidden, ht = rnn(x,_water)\n    output = lin(hidden)\n\n    loss = loss_fn(output, y)\n\n    loss.backward()\n\n    optim.step()\n    optim.zero_grad()\n\n100%|██████████| 12000/12000 [00:53&lt;00:00, 225.40it/s]\n\n\n\nyhat = soft(output)\n\ncombined = torch.concat([hidden, yhat], axis = 1).data.to('cpu')\n\n\nplt.matshow(combined[-15:], cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x21d5a8fa130&gt;\n\n\n\n\n\n\nc와 C를 잘 구별하지 못하는 것을 알 수 있음\n둘이 비슷한 확률로 출력됨\n\n\n\n추가로 3000번 더 학습을 시켜보자!\n\n\nfor epoch in tqdm(range(3000)):\n    _water = torch.zeros(1,3).to('cuda:0')\n    hidden, ht = rnn(x,_water)\n    output = lin(hidden)\n\n    loss = loss_fn(output, y)\n\n    loss.backward()\n\n    optim.step()\n    optim.zero_grad()\n\n100%|██████████| 3000/3000 [00:13&lt;00:00, 222.97it/s]\n\n\n\nyhat = soft(output)\n\ncombined = torch.concat([hidden, yhat], axis = 1).data.to('cpu')\n\n\nplt.matshow(combined[-15:], cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x21d5b15f700&gt;\n\n\n\n\n\n\nRNN은 15000번 정도 실행을 해야 c와 C를 구별할 수 있음\n\n\n\n\n\ntorch.manual_seed(43052)\n\nlstm = torch.nn.LSTM(4,3).to('cuda:0')\nlin = torch.nn.Linear(3,4).to('cuda:0')\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(list(lstm.parameters())+ list(lin.parameters()))\n\n\nfor epoch in tqdm(range(6000)):\n    _water = torch.zeros(1,3).to('cuda:0')\n    hidden, (ht, ct) = lstm(x,(_water,_water))\n    output = lin(hidden)\n\n    loss = loss_fn(output, y)\n\n    loss.backward()\n\n    optim.step()\n    optim.zero_grad()\n\n100%|██████████| 6000/6000 [00:26&lt;00:00, 223.70it/s]\n\n\n\nyhat = soft(output)\n\ncombined = torch.concat([hidden, yhat], axis = 1).data.to('cpu')\n\n\nplt.matshow(combined[-15:], cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x21d56bd3790&gt;\n\n\n\n\n\n\nLSTM은 6000번의 학습으로도 c와 C를 구별할 수 있음\n\n\n\n\n\n\nsig = torch.nn.Sigmoid()\ntanh = torch.nn.Tanh()\nsoft = torch.nn.Softmax(dim=1)\n\n\ntxt = list('abaB') * 100\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\nmapping = {'a':0, 'b':1, 'B':2}\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\ntorch.manual_seed(43052)\n\nlstm_cell = torch.nn.LSTMCell(3, 2)\nlin = torch.nn.Linear(2,3)\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(list(lstm_cell.parameters())+list(lin.parameters()))\n\n\nlstm_cell.weight_ih.T.shape\n\ntorch.Size([3, 8])\n\n\n\nT = len(x)\n\nfor epoch in range(1):\n    ht = torch.zeros(1,2)\n    ct = torch.zeros(1,2)\n    loss = 0\n\n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht, ct = lstm_cell(xt, (ht, ct))\n        _ifgo = xt@lstm_cell.weight_ih.T + ht@lstm_cell.weight_hh.T + lstm_cell.bias_hh + lstm_cell.bias_ih\n\n        # gate calculate\n        input_gate = sig(_ifgo[:,0:2]) \n        forget_gate = sig(_ifgo[:,2:4]) \n        output_gate = sig(_ifgo[:,6:8])\n\n        gt = tanh(_ifgo[:,4:6])\n\n        ct = forget_gate*ct + input_gate*gt\n        ht = output_gate*ct + tanh(ct) \n\n        ot = lin(ht)\n#        loss = loss + loss_fn(ot, yt)\n#    loss = loss/T\n\n#    loss.backward()\n\n#    optim.step()\n#    optim.zero_grad()\n\nLMTM은 아래와 같은 방식으로 output을 산출함\n\\((x_t, h_{t-1}) \\overset{lin}{\\to} {a, b, c, d} \\to [\\sigma(a), \\sigma(b), \\tanh(c), \\sigma(d)] = [i_t, f_t, g_t, o_t]\\)\ngate\n\ninput_gate: \\(i_t\\) -&gt; \\(g_t\\)의 값을 얼마나 통과시킬지를 0~1 사이의 값으로 보여줌\n\nforget_gate: \\(f_t\\) -&gt; \\(C_{t-1}\\)의 값을 얼마나 통과시킬지를 0~1 사이의 값으로 보여줌\n\noutput_gate: \\(o_t\\) -&gt; \\(tanh(C_{t})\\)의 값을 얼마나 통과시킬지를 0~1 사이의 값으로 보여줌\n\nanother value(?)\n\ngt: \\(g_t\\) -&gt; \\(\\sigma((W_{o1} X_t) + (W_{o2} h_{t-1}) + bias)\\) -&gt; 현재와 과거의 값을 선형결합 (w와 x또는 h를 곱함) 후 tanh를 통과시켜 -1에서 1사이의 값으로 보여줌\nct: \\(c_t\\) -&gt; \\(forgetgate * C_{t-1} + inputgate * g_t\\) -&gt; *는 아다마르 곱으로 요소별로 선택한 후 곱하는 연산으로 \\(g_t\\)연산과는 다름\nht: \\(h_t\\) -&gt; \\(tanh(C_t)*O_t\\)\n\n\nht, ct\n\n(tensor([[-0.0406,  0.2505]], grad_fn=&lt;MulBackward0&gt;),\n tensor([[-0.0975,  0.7134]], grad_fn=&lt;AddBackward0&gt;))"
  },
  {
    "objectID": "docs/DL/posts/DL_RNN(3).html#rnn과-lstm-성능-비교-data-abcabc",
    "href": "docs/DL/posts/DL_RNN(3).html#rnn과-lstm-성능-비교-data-abcabc",
    "title": "RNN part3",
    "section": "",
    "text": "txt = list('abcabC')*100\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\nmapping = {'a':0, 'b':1, 'c':2, 'C':3}\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\nx = x.to('cuda:0')\ny = y.to('cuda:0')\n\n\ntorch.manual_seed(43052)\n\nrnn = torch.nn.RNN(4,3).to('cuda:0')\nlin = torch.nn.Linear(3,4).to('cuda:0')\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(list(rnn.parameters())+ list(lin.parameters()))\n\n\nfor epoch in tqdm(range(12000)):\n    _water = torch.zeros(1,3).to('cuda:0')\n    hidden, ht = rnn(x,_water)\n    output = lin(hidden)\n\n    loss = loss_fn(output, y)\n\n    loss.backward()\n\n    optim.step()\n    optim.zero_grad()\n\n100%|██████████| 12000/12000 [00:53&lt;00:00, 225.40it/s]\n\n\n\nyhat = soft(output)\n\ncombined = torch.concat([hidden, yhat], axis = 1).data.to('cpu')\n\n\nplt.matshow(combined[-15:], cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x21d5a8fa130&gt;\n\n\n\n\n\n\nc와 C를 잘 구별하지 못하는 것을 알 수 있음\n둘이 비슷한 확률로 출력됨\n\n\n\n추가로 3000번 더 학습을 시켜보자!\n\n\nfor epoch in tqdm(range(3000)):\n    _water = torch.zeros(1,3).to('cuda:0')\n    hidden, ht = rnn(x,_water)\n    output = lin(hidden)\n\n    loss = loss_fn(output, y)\n\n    loss.backward()\n\n    optim.step()\n    optim.zero_grad()\n\n100%|██████████| 3000/3000 [00:13&lt;00:00, 222.97it/s]\n\n\n\nyhat = soft(output)\n\ncombined = torch.concat([hidden, yhat], axis = 1).data.to('cpu')\n\n\nplt.matshow(combined[-15:], cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x21d5b15f700&gt;\n\n\n\n\n\n\nRNN은 15000번 정도 실행을 해야 c와 C를 구별할 수 있음\n\n\n\n\n\ntorch.manual_seed(43052)\n\nlstm = torch.nn.LSTM(4,3).to('cuda:0')\nlin = torch.nn.Linear(3,4).to('cuda:0')\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(list(lstm.parameters())+ list(lin.parameters()))\n\n\nfor epoch in tqdm(range(6000)):\n    _water = torch.zeros(1,3).to('cuda:0')\n    hidden, (ht, ct) = lstm(x,(_water,_water))\n    output = lin(hidden)\n\n    loss = loss_fn(output, y)\n\n    loss.backward()\n\n    optim.step()\n    optim.zero_grad()\n\n100%|██████████| 6000/6000 [00:26&lt;00:00, 223.70it/s]\n\n\n\nyhat = soft(output)\n\ncombined = torch.concat([hidden, yhat], axis = 1).data.to('cpu')\n\n\nplt.matshow(combined[-15:], cmap = 'bwr')\n\n&lt;matplotlib.image.AxesImage at 0x21d56bd3790&gt;\n\n\n\n\n\n\nLSTM은 6000번의 학습으로도 c와 C를 구별할 수 있음"
  },
  {
    "objectID": "docs/DL/posts/DL_RNN(3).html#lstm의-구조",
    "href": "docs/DL/posts/DL_RNN(3).html#lstm의-구조",
    "title": "RNN part3",
    "section": "",
    "text": "sig = torch.nn.Sigmoid()\ntanh = torch.nn.Tanh()\nsoft = torch.nn.Softmax(dim=1)\n\n\ntxt = list('abaB') * 100\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\nmapping = {'a':0, 'b':1, 'B':2}\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\ntorch.manual_seed(43052)\n\nlstm_cell = torch.nn.LSTMCell(3, 2)\nlin = torch.nn.Linear(2,3)\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(list(lstm_cell.parameters())+list(lin.parameters()))\n\n\nlstm_cell.weight_ih.T.shape\n\ntorch.Size([3, 8])\n\n\n\nT = len(x)\n\nfor epoch in range(1):\n    ht = torch.zeros(1,2)\n    ct = torch.zeros(1,2)\n    loss = 0\n\n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht, ct = lstm_cell(xt, (ht, ct))\n        _ifgo = xt@lstm_cell.weight_ih.T + ht@lstm_cell.weight_hh.T + lstm_cell.bias_hh + lstm_cell.bias_ih\n\n        # gate calculate\n        input_gate = sig(_ifgo[:,0:2]) \n        forget_gate = sig(_ifgo[:,2:4]) \n        output_gate = sig(_ifgo[:,6:8])\n\n        gt = tanh(_ifgo[:,4:6])\n\n        ct = forget_gate*ct + input_gate*gt\n        ht = output_gate*ct + tanh(ct) \n\n        ot = lin(ht)\n#        loss = loss + loss_fn(ot, yt)\n#    loss = loss/T\n\n#    loss.backward()\n\n#    optim.step()\n#    optim.zero_grad()\n\nLMTM은 아래와 같은 방식으로 output을 산출함\n\\((x_t, h_{t-1}) \\overset{lin}{\\to} {a, b, c, d} \\to [\\sigma(a), \\sigma(b), \\tanh(c), \\sigma(d)] = [i_t, f_t, g_t, o_t]\\)\ngate\n\ninput_gate: \\(i_t\\) -&gt; \\(g_t\\)의 값을 얼마나 통과시킬지를 0~1 사이의 값으로 보여줌\n\nforget_gate: \\(f_t\\) -&gt; \\(C_{t-1}\\)의 값을 얼마나 통과시킬지를 0~1 사이의 값으로 보여줌\n\noutput_gate: \\(o_t\\) -&gt; \\(tanh(C_{t})\\)의 값을 얼마나 통과시킬지를 0~1 사이의 값으로 보여줌\n\nanother value(?)\n\ngt: \\(g_t\\) -&gt; \\(\\sigma((W_{o1} X_t) + (W_{o2} h_{t-1}) + bias)\\) -&gt; 현재와 과거의 값을 선형결합 (w와 x또는 h를 곱함) 후 tanh를 통과시켜 -1에서 1사이의 값으로 보여줌\nct: \\(c_t\\) -&gt; \\(forgetgate * C_{t-1} + inputgate * g_t\\) -&gt; *는 아다마르 곱으로 요소별로 선택한 후 곱하는 연산으로 \\(g_t\\)연산과는 다름\nht: \\(h_t\\) -&gt; \\(tanh(C_t)*O_t\\)\n\n\nht, ct\n\n(tensor([[-0.0406,  0.2505]], grad_fn=&lt;MulBackward0&gt;),\n tensor([[-0.0975,  0.7134]], grad_fn=&lt;AddBackward0&gt;))"
  },
  {
    "objectID": "docs/DL/posts/DL_logistic_regression.html",
    "href": "docs/DL/posts/DL_logistic_regression.html",
    "title": "Regression part2",
    "section": "",
    "text": "toc:true\n\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n\n회귀분석과 달리 로지스틱 회귀는 0 또는 1과 같이 분류를 하는 모델임\n- \\(x\\)가 커질수록 \\(y=1\\)이 잘나오는 모형은 아래와 같이 설계할 수 있음\n\n$y_i Ber(_i),$ where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)\n\\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-(\\hat{w}_0+\\hat{w}_1x_i))}\\)\n베르누이 분포상에 존재한다는 것은 0 또는 1이라는 것\n\n\n로지스틱에서는 loss를 위에 있는 식을 사용해야 함\n우도함수에 의해서 저렇게 되는거 같음…\n\n\n_df = pd.DataFrame({'x':range(-6,7),'y':[0,0,0,0,0,0,1,0,1,1,1,1,1]})\n\n\nxx = torch.linspace(-6,6,100)\ndef f(x):\n    return torch.exp(x)/(1+torch.exp(x))\n\n\nplt.plot(_df.x,_df.y,'o')\nplt.plot(xx,f(xx))\n\n\n\n\n\nx = torch.linspace(-1,1,2000).reshape(2000,1) # -1에서 1사이의 값 2천개 준비\n# net 학습을 통해서 아래에 있는 w0(= -1)과 w1(= 5)을 맞춰야 함\nw0 = - 1 \nw1 = 5 \nu = w0+x*w1 \nv = torch.exp(u)/(1+torch.exp(u)) # v=πi, 즉 확률을 의미함 -&gt; 0 ~ 1사이에 값이 존재 # 시그모이드\ny = torch.bernoulli(v) # 0.5이하의 값은 0으로, 0.5 초과하는값은 1로 치환 # y는 정답\n\n\nplt.scatter(x,y,alpha=0.05)\nplt.plot(x,v,'--r')\n\n\n\n\n\n\n\n\n\\(loss= - \\sum_{i=1}^{n} \\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\)\n회귀에서는 MSE를 사용하여 loss를 계산하였지만 이진분류에서는 BCELoss를 사용하여 loss를 계산한다.\n\nBCELoss는 정답이 0일때와 정답이 1일때의 경우를 나눠서 loss계산한다. (위의 수식 확인)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1, 1),\n    torch.nn.Sigmoid()\n)\n\nloss_fn = torch.nn.BCELoss()\n\noptim = torch.optim.SGD(net.parameters(), lr = 0.05)\n\nfor epoc in range(3000): \n    ## 1 \n    yhat= net(x) \n    ## 2 \n    loss = loss_fn(yhat, y)\n    ## 3 \n    loss.backward() \n    ## 4 \n    optim.step()\n    optim.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.05)\nplt.plot(x,v,'--')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nnet[0].weight, net[0].bias # 학습을 할수록 weight는 5에 가까워지고 bias는 -1에 가까워짐\n\n(Parameter containing:\n tensor([[4.6119]], requires_grad=True), Parameter containing:\n tensor([-0.8733], requires_grad=True))\n\n\n\n\n\n아래와 같은 underlying을 갖는 곡선은 로지스틱 회귀로 적합시키는 것이 어려움\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv')\nplt.plot(df.x,df.y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\n\n\n\n\n\nx= torch.tensor(df.x).float().reshape(-1,1)\ny= torch.tensor(df.y).float().reshape(-1,1)\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\n\nloss_fn = torch.nn.BCELoss() \n\noptimizr = torch.optim.Adam(net.parameters()) \n\n\nfor epoc in range(6000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'--b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n아무리 많이 돌려도 위와 같이 증가하다가 감소하는 underlying은 표현이 안됨 \\(\\to\\) DNN등장"
  },
  {
    "objectID": "docs/DL/posts/DL_logistic_regression.html#시그모이드",
    "href": "docs/DL/posts/DL_logistic_regression.html#시그모이드",
    "title": "Regression part2",
    "section": "",
    "text": "회귀분석과 달리 로지스틱 회귀는 0 또는 1과 같이 분류를 하는 모델임\n- \\(x\\)가 커질수록 \\(y=1\\)이 잘나오는 모형은 아래와 같이 설계할 수 있음\n\n$y_i Ber(_i),$ where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)\n\\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-(\\hat{w}_0+\\hat{w}_1x_i))}\\)\n베르누이 분포상에 존재한다는 것은 0 또는 1이라는 것\n\n\n로지스틱에서는 loss를 위에 있는 식을 사용해야 함\n우도함수에 의해서 저렇게 되는거 같음…\n\n\n_df = pd.DataFrame({'x':range(-6,7),'y':[0,0,0,0,0,0,1,0,1,1,1,1,1]})\n\n\nxx = torch.linspace(-6,6,100)\ndef f(x):\n    return torch.exp(x)/(1+torch.exp(x))\n\n\nplt.plot(_df.x,_df.y,'o')\nplt.plot(xx,f(xx))\n\n\n\n\n\nx = torch.linspace(-1,1,2000).reshape(2000,1) # -1에서 1사이의 값 2천개 준비\n# net 학습을 통해서 아래에 있는 w0(= -1)과 w1(= 5)을 맞춰야 함\nw0 = - 1 \nw1 = 5 \nu = w0+x*w1 \nv = torch.exp(u)/(1+torch.exp(u)) # v=πi, 즉 확률을 의미함 -&gt; 0 ~ 1사이에 값이 존재 # 시그모이드\ny = torch.bernoulli(v) # 0.5이하의 값은 0으로, 0.5 초과하는값은 1로 치환 # y는 정답\n\n\nplt.scatter(x,y,alpha=0.05)\nplt.plot(x,v,'--r')"
  },
  {
    "objectID": "docs/DL/posts/DL_logistic_regression.html#bceloss",
    "href": "docs/DL/posts/DL_logistic_regression.html#bceloss",
    "title": "Regression part2",
    "section": "",
    "text": "\\(loss= - \\sum_{i=1}^{n} \\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\)\n회귀에서는 MSE를 사용하여 loss를 계산하였지만 이진분류에서는 BCELoss를 사용하여 loss를 계산한다.\n\nBCELoss는 정답이 0일때와 정답이 1일때의 경우를 나눠서 loss계산한다. (위의 수식 확인)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1, 1),\n    torch.nn.Sigmoid()\n)\n\nloss_fn = torch.nn.BCELoss()\n\noptim = torch.optim.SGD(net.parameters(), lr = 0.05)\n\nfor epoc in range(3000): \n    ## 1 \n    yhat= net(x) \n    ## 2 \n    loss = loss_fn(yhat, y)\n    ## 3 \n    loss.backward() \n    ## 4 \n    optim.step()\n    optim.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.05)\nplt.plot(x,v,'--')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nnet[0].weight, net[0].bias # 학습을 할수록 weight는 5에 가까워지고 bias는 -1에 가까워짐\n\n(Parameter containing:\n tensor([[4.6119]], requires_grad=True), Parameter containing:\n tensor([-0.8733], requires_grad=True))"
  },
  {
    "objectID": "docs/DL/posts/DL_logistic_regression.html#로지스틱의-한계",
    "href": "docs/DL/posts/DL_logistic_regression.html#로지스틱의-한계",
    "title": "Regression part2",
    "section": "",
    "text": "아래와 같은 underlying을 갖는 곡선은 로지스틱 회귀로 적합시키는 것이 어려움\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv')\nplt.plot(df.x,df.y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\n\n\n\n\n\nx= torch.tensor(df.x).float().reshape(-1,1)\ny= torch.tensor(df.y).float().reshape(-1,1)\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\n\nloss_fn = torch.nn.BCELoss() \n\noptimizr = torch.optim.Adam(net.parameters()) \n\n\nfor epoc in range(6000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'--b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n아무리 많이 돌려도 위와 같이 증가하다가 감소하는 underlying은 표현이 안됨 \\(\\to\\) DNN등장"
  },
  {
    "objectID": "docs/DL/posts/DL_ANN.html",
    "href": "docs/DL/posts/DL_ANN.html",
    "title": "ANN",
    "section": "",
    "text": "toc:true\n\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n\nReLU함수의 등장 배경\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv')\nplt.plot(df.x,df.y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\n\n\n\n\n위와 같이 증가하다가 감소하는 underlying을 표현하기 위해서는 꺽이는 그래프를 만들어야 한다. 꺽인 그래프를 만드는 방법은 ReLU함수 등을 사용하면 된다.\n\\(ReLU(x) = \\max(0,x)\\)\n\nx= torch.tensor(df.x).float().reshape(-1,1)\ny= torch.tensor(df.y).float().reshape(-1,1)\n\n\nrelu=torch.nn.ReLU()\nplt.plot(x,'--r')\nplt.plot(relu(x),'--b')\n\n\n\n\n위의 그래프의 파란선을 보면 음수는 0이 되고 양수의 값은 그대로인 것을 확인할 수 있다. 이것이 렐루함수이다.\n\n\n\n선형 변환 \\(\\to\\) ReLU \\(\\to\\) 선형변환 구조를 사용하여 꺽인 그래프를 표현\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(in_features=1,out_features=2,bias=True) # 입력이 1개인데 출력이 2개 -&gt; 꺽이는 점 최대 1개\na1 = torch.nn.ReLU() # 음수는 0 양수는 그대로\nl2 = torch.nn.Linear(in_features=2,out_features=1,bias=True) # l1에 의해 출력이 2개가 되어 l2의 input은 2이고 이를 다시 하나로 만들기 위해 output을 1로 지정\na2 = torch.nn.Sigmoid() # 출력의 범위는 0 ~ 1 & 0과 1에 가까워질수록 기울기가 완만해짐\n\n\nnet = torch.nn.Sequential(l1,a1,l2,a2)\n\n\nl1.weight.data = torch.tensor([[1.0],[-1.0]])\nl1.bias.data = torch.tensor([0.0, 0.0])\n\nl2.weight.data = torch.tensor([[ -4.5, -9.0]])\nl2.bias.data= torch.tensor([4.5])\n\nl1.weight,l1.bias,l2.weight,l2.bias\n\n(Parameter containing:\n tensor([[ 1.],\n         [-1.]], requires_grad=True), Parameter containing:\n tensor([0., 0.], requires_grad=True), Parameter containing:\n tensor([[-4.5000, -9.0000]], requires_grad=True), Parameter containing:\n tensor([4.5000], requires_grad=True))\n\n\n\n\n\nplt.plot(l1(x).data)\n\n\n\n\n\n\n\n\nplt.plot(a1(l1(x)).data)\n\n\n\n\n\n\n\n\nplt.plot(l2(a1(l1(x))).data,color='C2')\n\n\n\n\n\n\n\n\nplt.plot(a2(l2(a1(l1(x)))).data,color='C2')\n#plt.plot(net(x).data,color='C2') # 위와 똑같은 결과\n\n\n\n\n\n\n\n\n\n다양한 예제를 위와 같은 모델로 적합시킬 수 있다.\n\n선형 변환시 out_feature를 크게 지정하면 지정할수록 꺽이는 점이 많아져 표현력이 증가함\n단, 그만큼 모델이 복잡해지고 파라미터의 수가 커짐\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex2.csv')\ndf\n\n\n  \n    \n      \n\n\n\n\n\n\nx\nunderlying\ny\n\n\n\n\n0\n-1.000000\n14.791438\n14.486265\n\n\n1\n-0.999000\n14.756562\n14.832600\n\n\n2\n-0.997999\n14.721663\n15.473211\n\n\n3\n-0.996999\n14.686739\n14.757734\n\n\n4\n-0.995998\n14.651794\n15.042901\n\n\n...\n...\n...\n...\n\n\n1995\n0.995998\n5.299511\n5.511416\n\n\n1996\n0.996999\n5.322140\n6.022263\n\n\n1997\n0.997999\n5.344736\n4.989637\n\n\n1998\n0.999000\n5.367299\n5.575369\n\n\n1999\n1.000000\n5.389829\n5.466730\n\n\n\n\n\n2000 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\n\n\nplt.plot(x,y,'o',alpha=0.02)\n#plt.plot(df.x,df.underlying,'-b')\n\n\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=32), # x:(n,1) --&gt; u1:(n,32)\n    torch.nn.ReLU(), # u1:(n,32) --&gt; v1:(n,32) \n    torch.nn.Linear(in_features=32,out_features=1) # v1:(n,32) --&gt; u2:(n,1)\n    # 이번에는 시그모이드를 포함하지 않았는데 이는 y값이 0~1사이에 존재하지 않아서...\n)\n\n\n# 학습전\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,lw=4)\n\n\n\n\n\nloss_fn = torch.nn.MSELoss()\n\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(12000): \n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\n# 12,000번 학습 후\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,lw=4)\n\n\n\n\n위 그래프를 확인해보면 충분히 많이 학습했음에도 불구하고 net와 underlying이 일치하지 않는 것을 확인할 수 있다.\n이는 local minimum에 빠져서 제대로 학습되지 못하고 있는 것을 알 수 있다.\n\\(\\to\\) 아래는 랜덤시드를 변경해주어 적절한 초기값을 사용한 결과이다\n\ntorch.manual_seed(5)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=32,out_features=1) \n)\n\n\nloss_fn = torch.nn.MSELoss()\n\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(12000): \n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\n# 12,000번 학습 후\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,lw=4)\n\n\n\n\n\n\n\n시벤코 정리는 선형변환 \\(\\to\\) 시그모이드 \\(\\to\\) 선형변환을 사용하면 모든 연속함수를 원하는 정확도로 근사시킬수 있다.\n- 요즘은 선형변환 \\(\\to\\) 렐루 \\(\\to\\) 선형변환 와 같은 조합을 사용한다.\n- 아래와 같은 적절한 선형변환(적절한 노드의 개수와 적절한 weight와 bias)을 하면 다양한 모양의 underlying을 그릴수 있다.\n\nh = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0\n\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input):\n        return h(input) # activation 의 출력 \n\n\n\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,3),\n            MyActivation(),\n            torch.nn.Linear(3,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,1024),\n            MyActivation(),\n            torch.nn.Linear(1024,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n- 정리: 히든레이어의 노드의 수가 많으면 많을수록 표현력이 증가한다.\n\n\n\n\n- ver 1: 예전: gradient descent, batch gradient descent \\(\\to\\) 요즘: gradient descent\n모든 샘플을 이용해 순간 기울기를 구하고 이를 바탕으로 업데이트\n(epoch1) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n(epoch2) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n…\n+ 앞서 사용한 코드들은 gradient descent임\nyhat = net(x)\n\nloss = torch.sum((y-yhat)**2) # loss = torch.nn.MSELoss(yhat, y)\n\nloss.backward() \n\noptim.step()\noptim.zero_grad()\n\n- ver 2: 예전: stochastic gradient descent \\(\\to\\) 요즘: stochastic gradient descent with batch size = 1\n샘플 하나당 업데이트 한번 (하나의 샘플만을 이용해 slope계산)\n(epoch1)\n\\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\to slope \\to update\\)\n\\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\to slope \\to update\\)\n…\n\\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n(epoch2)\n\\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\to slope \\to update\\)\n\\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\to slope \\to update\\)\n…\n\\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n…\n\n- ver 3: 예전: mini-batch gradient descent, mini-batch stochastic gradient descent \\(\\to\\) 요즘: stochastic gradient descent\n- ver3: \\(m (\\leq n)\\) 개의 샘플을 이용하여 slope 계산\n\\(m=3\\)이라고 하자.\n(epoch1)\n\\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n\\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n\\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n\\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n(epoch2)\n\\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n\\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n\\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n\\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n…\n# X의 tensor size = torch.Size([12665, 784]), \n# y의 tensor size = torch.Size([12665, 1])\n# 12665 / 2048 ≈ 6.18\n\nds = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=2048) \n\nfor epoc in range(10): # epoch = 7 * 10번\n    for xx,yy in dl: # 7번\n        ## 1\n        #yhat = net(xx)\n        ## 2 \n        loss = loss_fn(net(xx),yy) \n        ## 3 \n        loss.backward() \n        ## 4 \n        optimizr.step()\n        optimizr.zero_grad()\n\n+ 정리: stochastic gradient descent를 사용하면 정확도는 비슷하지만 학습속도가 gradient descent보다 빠르다.\n\n\n\n\n모델이 train 데이터에 너무 적합되어서 test 데이터의 추세를 맞추지 못하는 경우\n\n데이터를 underlying + 오차라고 할때 데이터의 추세가 underlying이 아닌 오차를 따라갈 경우\n(정의라기 보다는 오버피팅 용어에 대한 간단한 설명)\n\n\ntorch.manual_seed(5) \nx = torch.linspace(0,1,100).reshape(100,1)\ny = torch.randn(100).reshape(100,1)*0.01\n\nx와 y는 랜덤하게 뽑은 값이므로 x와 y사이에는 어떠한 관계도도 없다.\n\nxtr = x[:80] \nytr = y[:80]\nxtest = x[80:]\nytest = y[80:]\nplt.plot(xtr,ytr)\nplt.plot(xtest,ytest)\nplt.title('train: blue / test: orange');\n\n\n\n\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1, 512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n)\n\nloss_fn = torch.nn.MSELoss()\n\noptim = torch.optim.Adam(net.parameters())\n\nfor epoch in range(1000):\n  yhat = net(xtr)\n  loss = loss_fn(yhat,ytr)\n  loss.backward()\n  optim.step()\n  optim.zero_grad()\n\n\nplt.plot(x,y,alpha=0.5)\nplt.plot(xtr,net(xtr).data,'--') # prediction (train) \nplt.plot(xtest,net(xtest).data,'--') # prediction with unseen data (test) \n\n\n\n\n위의 점선들을 확인하면 train data부분인 주황점선은 데이터의 분포를 잘 따라가지만 test data부분인 초록 점선은 데이터의 분포를 전혀 예측하지 못하고 있다.\n이를 통해 해당 모델은 오버피팅이 되어 있는 것을 알 수 있다.\n\n\n\n\ntorch.manual_seed(1) \nnet=torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512), \n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.8),\n    torch.nn.Linear(in_features=512,out_features=1)) \n\noptimizr= torch.optim.Adam(net.parameters())\n\nloss_fn= torch.nn.MSELoss()\n\nfor epoc in range(1000): \n    ## 1 \n    # net(xtr) \n    ## 2 \n    loss=loss_fn(net(xtr),ytr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n위와 같은 방식으로 드랍아웃층을 활성화 함수 다음에 위치시킨다.\ntorch.nn.Dropout(0.9)\n위 코드는 드랍아웃층의 노드중 90%를 임의로 골라 결과를 0으로 만들고 나머지 10%의 노드는 10배만큼 값이 커지게 만들어 노드를 통과한 값들의 총합을 일정하게 유지시켜준다.\n\n드랍아웃 정리 - 드랍아웃 레이어\n- 구조: 입력 -&gt; 드랍아웃 레이어 -&gt; 출력\n- 역할:\n(1) 입력의 일부를 임의로 0으로 만드는 역할(랜덤 포레스트와 비슷한 알고리즘)\n(2) 0이 안된 것들은 스칼래배하여 드랍아웃을 통과한 모든 숫자들의 총합이 일정하게 되도록 조정\n\n\n효과: 오버피팅을 억제하는 효과가 있음\n\n의미: each iteration(each epoch x)마다 학습에 참여하는 노드가 로테이션으로 랜덤으로 결정됨.\n\n느낌: 모든 노드가 골고루 학습가능 + 한 두개의 특화된 능력치가 개발되기 보다 평균적인 능력치가 전박적으로 개선됨"
  },
  {
    "objectID": "docs/DL/posts/DL_ANN.html#relu함수",
    "href": "docs/DL/posts/DL_ANN.html#relu함수",
    "title": "ANN",
    "section": "",
    "text": "ReLU함수의 등장 배경\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv')\nplt.plot(df.x,df.y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\n\n\n\n\n위와 같이 증가하다가 감소하는 underlying을 표현하기 위해서는 꺽이는 그래프를 만들어야 한다. 꺽인 그래프를 만드는 방법은 ReLU함수 등을 사용하면 된다.\n\\(ReLU(x) = \\max(0,x)\\)\n\nx= torch.tensor(df.x).float().reshape(-1,1)\ny= torch.tensor(df.y).float().reshape(-1,1)\n\n\nrelu=torch.nn.ReLU()\nplt.plot(x,'--r')\nplt.plot(relu(x),'--b')\n\n\n\n\n위의 그래프의 파란선을 보면 음수는 0이 되고 양수의 값은 그대로인 것을 확인할 수 있다. 이것이 렐루함수이다."
  },
  {
    "objectID": "docs/DL/posts/DL_ANN.html#꺽인-그래프-만들기",
    "href": "docs/DL/posts/DL_ANN.html#꺽인-그래프-만들기",
    "title": "ANN",
    "section": "",
    "text": "선형 변환 \\(\\to\\) ReLU \\(\\to\\) 선형변환 구조를 사용하여 꺽인 그래프를 표현\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(in_features=1,out_features=2,bias=True) # 입력이 1개인데 출력이 2개 -&gt; 꺽이는 점 최대 1개\na1 = torch.nn.ReLU() # 음수는 0 양수는 그대로\nl2 = torch.nn.Linear(in_features=2,out_features=1,bias=True) # l1에 의해 출력이 2개가 되어 l2의 input은 2이고 이를 다시 하나로 만들기 위해 output을 1로 지정\na2 = torch.nn.Sigmoid() # 출력의 범위는 0 ~ 1 & 0과 1에 가까워질수록 기울기가 완만해짐\n\n\nnet = torch.nn.Sequential(l1,a1,l2,a2)\n\n\nl1.weight.data = torch.tensor([[1.0],[-1.0]])\nl1.bias.data = torch.tensor([0.0, 0.0])\n\nl2.weight.data = torch.tensor([[ -4.5, -9.0]])\nl2.bias.data= torch.tensor([4.5])\n\nl1.weight,l1.bias,l2.weight,l2.bias\n\n(Parameter containing:\n tensor([[ 1.],\n         [-1.]], requires_grad=True), Parameter containing:\n tensor([0., 0.], requires_grad=True), Parameter containing:\n tensor([[-4.5000, -9.0000]], requires_grad=True), Parameter containing:\n tensor([4.5000], requires_grad=True))\n\n\n\n\n\nplt.plot(l1(x).data)\n\n\n\n\n\n\n\n\nplt.plot(a1(l1(x)).data)\n\n\n\n\n\n\n\n\nplt.plot(l2(a1(l1(x))).data,color='C2')\n\n\n\n\n\n\n\n\nplt.plot(a2(l2(a1(l1(x)))).data,color='C2')\n#plt.plot(net(x).data,color='C2') # 위와 똑같은 결과"
  },
  {
    "objectID": "docs/DL/posts/DL_ANN.html#dnn-적용",
    "href": "docs/DL/posts/DL_ANN.html#dnn-적용",
    "title": "ANN",
    "section": "",
    "text": "다양한 예제를 위와 같은 모델로 적합시킬 수 있다.\n\n선형 변환시 out_feature를 크게 지정하면 지정할수록 꺽이는 점이 많아져 표현력이 증가함\n단, 그만큼 모델이 복잡해지고 파라미터의 수가 커짐\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex2.csv')\ndf\n\n\n  \n    \n      \n\n\n\n\n\n\nx\nunderlying\ny\n\n\n\n\n0\n-1.000000\n14.791438\n14.486265\n\n\n1\n-0.999000\n14.756562\n14.832600\n\n\n2\n-0.997999\n14.721663\n15.473211\n\n\n3\n-0.996999\n14.686739\n14.757734\n\n\n4\n-0.995998\n14.651794\n15.042901\n\n\n...\n...\n...\n...\n\n\n1995\n0.995998\n5.299511\n5.511416\n\n\n1996\n0.996999\n5.322140\n6.022263\n\n\n1997\n0.997999\n5.344736\n4.989637\n\n\n1998\n0.999000\n5.367299\n5.575369\n\n\n1999\n1.000000\n5.389829\n5.466730\n\n\n\n\n\n2000 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\n\n\nplt.plot(x,y,'o',alpha=0.02)\n#plt.plot(df.x,df.underlying,'-b')\n\n\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=32), # x:(n,1) --&gt; u1:(n,32)\n    torch.nn.ReLU(), # u1:(n,32) --&gt; v1:(n,32) \n    torch.nn.Linear(in_features=32,out_features=1) # v1:(n,32) --&gt; u2:(n,1)\n    # 이번에는 시그모이드를 포함하지 않았는데 이는 y값이 0~1사이에 존재하지 않아서...\n)\n\n\n# 학습전\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,lw=4)\n\n\n\n\n\nloss_fn = torch.nn.MSELoss()\n\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(12000): \n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\n# 12,000번 학습 후\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,lw=4)\n\n\n\n\n위 그래프를 확인해보면 충분히 많이 학습했음에도 불구하고 net와 underlying이 일치하지 않는 것을 확인할 수 있다.\n이는 local minimum에 빠져서 제대로 학습되지 못하고 있는 것을 알 수 있다.\n\\(\\to\\) 아래는 랜덤시드를 변경해주어 적절한 초기값을 사용한 결과이다\n\ntorch.manual_seed(5)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=32,out_features=1) \n)\n\n\nloss_fn = torch.nn.MSELoss()\n\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(12000): \n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\n# 12,000번 학습 후\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,lw=4)"
  },
  {
    "objectID": "docs/DL/posts/DL_ANN.html#시벤코-정리universal-approximation-theorem-1989",
    "href": "docs/DL/posts/DL_ANN.html#시벤코-정리universal-approximation-theorem-1989",
    "title": "ANN",
    "section": "",
    "text": "시벤코 정리는 선형변환 \\(\\to\\) 시그모이드 \\(\\to\\) 선형변환을 사용하면 모든 연속함수를 원하는 정확도로 근사시킬수 있다.\n- 요즘은 선형변환 \\(\\to\\) 렐루 \\(\\to\\) 선형변환 와 같은 조합을 사용한다.\n- 아래와 같은 적절한 선형변환(적절한 노드의 개수와 적절한 weight와 bias)을 하면 다양한 모양의 underlying을 그릴수 있다.\n\nh = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0\n\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input):\n        return h(input) # activation 의 출력 \n\n\n\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,3),\n            MyActivation(),\n            torch.nn.Linear(3,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n\n\n\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,1024),\n            MyActivation(),\n            torch.nn.Linear(1024,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n- 정리: 히든레이어의 노드의 수가 많으면 많을수록 표현력이 증가한다."
  },
  {
    "objectID": "docs/DL/posts/DL_ANN.html#경사하강법-용어-정리",
    "href": "docs/DL/posts/DL_ANN.html#경사하강법-용어-정리",
    "title": "ANN",
    "section": "",
    "text": "- ver 1: 예전: gradient descent, batch gradient descent \\(\\to\\) 요즘: gradient descent\n모든 샘플을 이용해 순간 기울기를 구하고 이를 바탕으로 업데이트\n(epoch1) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n(epoch2) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n…\n+ 앞서 사용한 코드들은 gradient descent임\nyhat = net(x)\n\nloss = torch.sum((y-yhat)**2) # loss = torch.nn.MSELoss(yhat, y)\n\nloss.backward() \n\noptim.step()\noptim.zero_grad()\n\n- ver 2: 예전: stochastic gradient descent \\(\\to\\) 요즘: stochastic gradient descent with batch size = 1\n샘플 하나당 업데이트 한번 (하나의 샘플만을 이용해 slope계산)\n(epoch1)\n\\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\to slope \\to update\\)\n\\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\to slope \\to update\\)\n…\n\\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n(epoch2)\n\\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\to slope \\to update\\)\n\\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\to slope \\to update\\)\n…\n\\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n…\n\n- ver 3: 예전: mini-batch gradient descent, mini-batch stochastic gradient descent \\(\\to\\) 요즘: stochastic gradient descent\n- ver3: \\(m (\\leq n)\\) 개의 샘플을 이용하여 slope 계산\n\\(m=3\\)이라고 하자.\n(epoch1)\n\\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n\\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n\\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n\\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n(epoch2)\n\\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n\\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n\\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n\\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n…\n# X의 tensor size = torch.Size([12665, 784]), \n# y의 tensor size = torch.Size([12665, 1])\n# 12665 / 2048 ≈ 6.18\n\nds = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=2048) \n\nfor epoc in range(10): # epoch = 7 * 10번\n    for xx,yy in dl: # 7번\n        ## 1\n        #yhat = net(xx)\n        ## 2 \n        loss = loss_fn(net(xx),yy) \n        ## 3 \n        loss.backward() \n        ## 4 \n        optimizr.step()\n        optimizr.zero_grad()\n\n+ 정리: stochastic gradient descent를 사용하면 정확도는 비슷하지만 학습속도가 gradient descent보다 빠르다."
  },
  {
    "objectID": "docs/DL/posts/DL_ANN.html#오버피팅",
    "href": "docs/DL/posts/DL_ANN.html#오버피팅",
    "title": "ANN",
    "section": "",
    "text": "모델이 train 데이터에 너무 적합되어서 test 데이터의 추세를 맞추지 못하는 경우\n\n데이터를 underlying + 오차라고 할때 데이터의 추세가 underlying이 아닌 오차를 따라갈 경우\n(정의라기 보다는 오버피팅 용어에 대한 간단한 설명)\n\n\ntorch.manual_seed(5) \nx = torch.linspace(0,1,100).reshape(100,1)\ny = torch.randn(100).reshape(100,1)*0.01\n\nx와 y는 랜덤하게 뽑은 값이므로 x와 y사이에는 어떠한 관계도도 없다.\n\nxtr = x[:80] \nytr = y[:80]\nxtest = x[80:]\nytest = y[80:]\nplt.plot(xtr,ytr)\nplt.plot(xtest,ytest)\nplt.title('train: blue / test: orange');\n\n\n\n\n\ntorch.manual_seed(1)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1, 512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n)\n\nloss_fn = torch.nn.MSELoss()\n\noptim = torch.optim.Adam(net.parameters())\n\nfor epoch in range(1000):\n  yhat = net(xtr)\n  loss = loss_fn(yhat,ytr)\n  loss.backward()\n  optim.step()\n  optim.zero_grad()\n\n\nplt.plot(x,y,alpha=0.5)\nplt.plot(xtr,net(xtr).data,'--') # prediction (train) \nplt.plot(xtest,net(xtest).data,'--') # prediction with unseen data (test) \n\n\n\n\n위의 점선들을 확인하면 train data부분인 주황점선은 데이터의 분포를 잘 따라가지만 test data부분인 초록 점선은 데이터의 분포를 전혀 예측하지 못하고 있다.\n이를 통해 해당 모델은 오버피팅이 되어 있는 것을 알 수 있다."
  },
  {
    "objectID": "docs/DL/posts/DL_ANN.html#드랍아웃",
    "href": "docs/DL/posts/DL_ANN.html#드랍아웃",
    "title": "ANN",
    "section": "",
    "text": "torch.manual_seed(1) \nnet=torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512), \n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.8),\n    torch.nn.Linear(in_features=512,out_features=1)) \n\noptimizr= torch.optim.Adam(net.parameters())\n\nloss_fn= torch.nn.MSELoss()\n\nfor epoc in range(1000): \n    ## 1 \n    # net(xtr) \n    ## 2 \n    loss=loss_fn(net(xtr),ytr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n위와 같은 방식으로 드랍아웃층을 활성화 함수 다음에 위치시킨다.\ntorch.nn.Dropout(0.9)\n위 코드는 드랍아웃층의 노드중 90%를 임의로 골라 결과를 0으로 만들고 나머지 10%의 노드는 10배만큼 값이 커지게 만들어 노드를 통과한 값들의 총합을 일정하게 유지시켜준다.\n\n드랍아웃 정리 - 드랍아웃 레이어\n- 구조: 입력 -&gt; 드랍아웃 레이어 -&gt; 출력\n- 역할:\n(1) 입력의 일부를 임의로 0으로 만드는 역할(랜덤 포레스트와 비슷한 알고리즘)\n(2) 0이 안된 것들은 스칼래배하여 드랍아웃을 통과한 모든 숫자들의 총합이 일정하게 되도록 조정\n\n\n효과: 오버피팅을 억제하는 효과가 있음\n\n의미: each iteration(each epoch x)마다 학습에 참여하는 노드가 로테이션으로 랜덤으로 결정됨.\n\n느낌: 모든 노드가 골고루 학습가능 + 한 두개의 특화된 능력치가 개발되기 보다 평균적인 능력치가 전박적으로 개선됨"
  },
  {
    "objectID": "docs/DL/posts/DL_regression.html",
    "href": "docs/DL/posts/DL_regression.html",
    "title": "Regression part1",
    "section": "",
    "text": "toc:true\n\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt \n\n\n\n\ntorch.manual_seed(43052)\n\nones= torch.ones(100)\nx,_ = torch.randn(100).sort()\nX = torch.stack([ones,x]).T # torch.stack([ones,x],axis=1)\n\n\nW = torch.tensor([2.5,4])\n\n\nϵ = torch.randn(100)*0.5 # 입실론은 오차를 의미함\n\n\ny = X @ W + ϵ\n\n\nplt.plot(x, y, 'o')\n\n\n\n\n\n\n\n\nWhat = torch.tensor([-5.0,10.0],requires_grad=True)\nWhat\n\ntensor([-5., 10.], requires_grad=True)\n\n\n\nyhat = X @ What \n\n\nplt.plot(x, y, 'o')\nplt.plot(x, yhat.data, '-')\n\n\n\n\n\n\n\n\\(loss=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2 = ({\\bf y}-{\\bf\\hat{y}})^\\top({\\bf y}-{\\bf\\hat{y}})=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\)\n궁극적으로 조합 \\((\\hat{w}_0,\\hat{w}_1)\\)에 대하여 가장 작은 loss를 찾아내야 함\n- 찾는 방법: loss를 미분하여 기울기를 얻는다 \\(\\to\\) 기울기 부호의 반대 방향으로 이동한다. \\(\\to\\) 움직이는 보폭(크기)은 (loss미분값 * 학습률(lr, α))\n- \\({\\bf W} \\leftarrow {\\bf W} - \\alpha \\times \\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1)\\)\n- \\(\\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1)\\) 이 값은 아래의 코드를 통해 계산할할 수 있다.\nloss.backward()\n- 위에서 계산한 미분값은 W.grad를 통해 확인할 수 있다.\n- 정리하자면 loss를 정의한 후\nW = W - 학습률 * W.grad\n+ W를 업데이트한 후 W.grad = None을 해줘야 함 + 미분을 통해 업데이트를 하기 위해서는 아래와 같은 옵션을 주어야 한다.\ntorch.tensor([-5.0,10.0],requires_grad=True)\n\n\n\n\n\n\nalpha=0.001 \nfor epoc in range(30): ## 30번 반복합니다!! \n    # step 1\n    yhat=X@What # w와 b를 만드는 \n                # type1) torch.nn.Linear(input, ouput, bias=True) X에 1로 구성된 열이 없을때 사용\n                # type2) torch.nn.Linear(input + 1, ouput, bias=False) X에 1로 구성된 열이 있있을때 사용\n    \n    # step 2\n    loss=torch.sum((y-yhat)**2) # loss = torch.nn.MSELoss(yhat, y)\n    \n    # step 3\n    loss.backward() \n\n    # step 4 -&gt; optimizer를 사용 # optim = torch.optim.SGD(net.parameters(),lr=1/10) 이런 식으로 선언 # net.parameters()는 optimizer에 업데이트할 변수를 지정 # lr은 학습률 지정\n    What.data = What.data-alpha * What.grad # optim.step()\n    What.grad=None # optim.zero_grad()\n\n\nplt.plot(x, y, 'o')\nplt.plot(x, yhat.data, '-')\n\n\n\n\n\n\n\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\n\nalpha=0.001 \n\nfor epoc in range(30): \n    #\n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    #\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    #\n    loss.backward() \n    #\n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nplt.plot(loss_history)\n\n\n\n\n\n\n\n\n- 학습률이 너무 크면 스텝이 너무 커서 loss의 순간기울기가 0이 되는 점을 찾지 못할 수 있다. (학습률이 너무 작으면 loss의 순간기울기가 0이 되는 점을 찾을 수는 있지만 학습시간이 오래 걸린다는 단점이 있음)\n- local minimum과 global minimum이 있는 loss 곡선은 초기 W가 중요하다."
  },
  {
    "objectID": "docs/DL/posts/DL_regression.html#데이터-준비",
    "href": "docs/DL/posts/DL_regression.html#데이터-준비",
    "title": "Regression part1",
    "section": "",
    "text": "torch.manual_seed(43052)\n\nones= torch.ones(100)\nx,_ = torch.randn(100).sort()\nX = torch.stack([ones,x]).T # torch.stack([ones,x],axis=1)\n\n\nW = torch.tensor([2.5,4])\n\n\nϵ = torch.randn(100)*0.5 # 입실론은 오차를 의미함\n\n\ny = X @ W + ϵ\n\n\nplt.plot(x, y, 'o')"
  },
  {
    "objectID": "docs/DL/posts/DL_regression.html#가중치와-bias-준비",
    "href": "docs/DL/posts/DL_regression.html#가중치와-bias-준비",
    "title": "Regression part1",
    "section": "",
    "text": "What = torch.tensor([-5.0,10.0],requires_grad=True)\nWhat\n\ntensor([-5., 10.], requires_grad=True)\n\n\n\nyhat = X @ What \n\n\nplt.plot(x, y, 'o')\nplt.plot(x, yhat.data, '-')"
  },
  {
    "objectID": "docs/DL/posts/DL_regression.html#노란선으로-파란선을-맞춰야-함-맞추기-위해서는-얼마나-틀렸는지를-알아야-함---loss-function",
    "href": "docs/DL/posts/DL_regression.html#노란선으로-파란선을-맞춰야-함-맞추기-위해서는-얼마나-틀렸는지를-알아야-함---loss-function",
    "title": "Regression part1",
    "section": "",
    "text": "\\(loss=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2 = ({\\bf y}-{\\bf\\hat{y}})^\\top({\\bf y}-{\\bf\\hat{y}})=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\)\n궁극적으로 조합 \\((\\hat{w}_0,\\hat{w}_1)\\)에 대하여 가장 작은 loss를 찾아내야 함\n- 찾는 방법: loss를 미분하여 기울기를 얻는다 \\(\\to\\) 기울기 부호의 반대 방향으로 이동한다. \\(\\to\\) 움직이는 보폭(크기)은 (loss미분값 * 학습률(lr, α))\n- \\({\\bf W} \\leftarrow {\\bf W} - \\alpha \\times \\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1)\\)\n- \\(\\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1)\\) 이 값은 아래의 코드를 통해 계산할할 수 있다.\nloss.backward()\n- 위에서 계산한 미분값은 W.grad를 통해 확인할 수 있다.\n- 정리하자면 loss를 정의한 후\nW = W - 학습률 * W.grad\n+ W를 업데이트한 후 W.grad = None을 해줘야 함 + 미분을 통해 업데이트를 하기 위해서는 아래와 같은 옵션을 주어야 한다.\ntorch.tensor([-5.0,10.0],requires_grad=True)"
  },
  {
    "objectID": "docs/DL/posts/DL_regression.html#훈련-방법",
    "href": "docs/DL/posts/DL_regression.html#훈련-방법",
    "title": "Regression part1",
    "section": "",
    "text": "alpha=0.001 \nfor epoc in range(30): ## 30번 반복합니다!! \n    # step 1\n    yhat=X@What # w와 b를 만드는 \n                # type1) torch.nn.Linear(input, ouput, bias=True) X에 1로 구성된 열이 없을때 사용\n                # type2) torch.nn.Linear(input + 1, ouput, bias=False) X에 1로 구성된 열이 있있을때 사용\n    \n    # step 2\n    loss=torch.sum((y-yhat)**2) # loss = torch.nn.MSELoss(yhat, y)\n    \n    # step 3\n    loss.backward() \n\n    # step 4 -&gt; optimizer를 사용 # optim = torch.optim.SGD(net.parameters(),lr=1/10) 이런 식으로 선언 # net.parameters()는 optimizer에 업데이트할 변수를 지정 # lr은 학습률 지정\n    What.data = What.data-alpha * What.grad # optim.step()\n    What.grad=None # optim.zero_grad()\n\n\nplt.plot(x, y, 'o')\nplt.plot(x, yhat.data, '-')\n\n\n\n\n\n\n\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\n\nalpha=0.001 \n\nfor epoc in range(30): \n    #\n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    #\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    #\n    loss.backward() \n    #\n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nplt.plot(loss_history)"
  },
  {
    "objectID": "docs/DL/posts/DL_regression.html#정리",
    "href": "docs/DL/posts/DL_regression.html#정리",
    "title": "Regression part1",
    "section": "",
    "text": "- 학습률이 너무 크면 스텝이 너무 커서 loss의 순간기울기가 0이 되는 점을 찾지 못할 수 있다. (학습률이 너무 작으면 loss의 순간기울기가 0이 되는 점을 찾을 수는 있지만 학습시간이 오래 걸린다는 단점이 있음)\n- local minimum과 global minimum이 있는 loss 곡선은 초기 W가 중요하다."
  },
  {
    "objectID": "docs/BME/index.html",
    "href": "docs/BME/index.html",
    "title": "BME summary",
    "section": "",
    "text": "초음파 물리\n\n\n\n\n\n\n\nUltrasound\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nJuWon\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html",
    "href": "docs/BME/posts/DL_Object_Detection.html",
    "title": "test",
    "section": "",
    "text": "참조 : https://github.com/wikibook/dl-vision"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#applications",
    "href": "docs/BME/posts/DL_Object_Detection.html#applications",
    "title": "test",
    "section": "Applications",
    "text": "Applications\n\n자율 주행 자동차에서 다른 자동차와 보행자를 찾을 때\n의료 분야에서 방사선 사진을 사용해 종양이나 위험한 조직을 찾을 때\n제조업에서 조립 로봇이 제품을 조립하거나 수리할 때\n보안 산업에서 위협을 탐지하거나 사람을 셀 때"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#용어-설명",
    "href": "docs/BME/posts/DL_Object_Detection.html#용어-설명",
    "title": "test",
    "section": "용어 설명",
    "text": "용어 설명\n\nBoudning Box\n\n이미지에서 하나의 객체 전체를 포함하는 가장 작은 직사각형\n\n[이미지 출처] https://medium.com/anolytics/how-bounding-box-annotation-helps-object-detection-in-machine-learning-use-cases-431d93e7b25b\n\n\n\nIOU(Intersection Over Union)\n\n실측값(Ground Truth)과 모델이 예측한 값이 얼마나 겹치는지를 나타내는 지표\n\nIOU가 높을수록 잘 예측한 모델\n\n\n\n\n예시 \n[이미지 출처] https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n\n\n\nNMS(Non-Maximum Suppression, 비최댓값 억제)\n\n확률이 가장 높은 상자와 겹치는 상자들을 제거하는 과정\n최댓값을 갖지 않는 상자들을 제거\n과정\n\n확률 기준으로 모든 상자를 정렬하고 먼저 가장 확률이 높은 상자를 취함\n각 상자에 대해 다른 모든 상자와의 IOU를 계산\n특정 임곗값을 넘는 상자는 제거\n\n\n[이미지 출처] https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#모델-성능-평가",
    "href": "docs/BME/posts/DL_Object_Detection.html#모델-성능-평가",
    "title": "test",
    "section": "모델 성능 평가",
    "text": "모델 성능 평가\n\n정밀도와 재현율\n\n일반적으로 객체 탐지 모델 평가에 사용되지는 않지만, 다른 지표를 계산하는 기본 지표 역할을 함\n\nTP\n\nTrue Positives\n예측이 동일 클래스의 실제 상자와 일치하는지 측정\n\nFP\n\nFalse Positives\n예측이 실제 상자와 일치하지 않는지 측정\n\nFN\n\nFalse Negatives\n실제 분류값이 그와 일치하는 예측을 갖지 못하는지 측정"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#qquad-precision-fractptp-fp",
    "href": "docs/BME/posts/DL_Object_Detection.html#qquad-precision-fractptp-fp",
    "title": "test",
    "section": "\\(\\qquad precision = \\frac{TP}{TP \\ + \\ FP}\\)",
    "text": "\\(\\qquad precision = \\frac{TP}{TP \\ + \\ FP}\\)"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#qquad-recall-fractptp-fn",
    "href": "docs/BME/posts/DL_Object_Detection.html#qquad-recall-fractptp-fn",
    "title": "test",
    "section": "\\(\\qquad recall = \\frac{TP}{TP \\ + \\ FN}\\)",
    "text": "\\(\\qquad recall = \\frac{TP}{TP \\ + \\ FN}\\)\n\n모델이 안정적이지 않은 특징을 기반으로 객체 존재를 예측하면 거짓긍정(FP)이 많아져서 정밀도가 낮아짐\n모델이 너무 엄격해서 정확한 조건을 만족할 때만 객체가 탐지된 것으로 간주하면 거짓부정(FN)이 많아져서 재현율이 낮아짐\n\n\n정밀도-재현율 곡선(precision-recall curve)\n\n신뢰도 임곗값마다 모델의 정밀도와 재현율을 시각화\n모든 bounding box와 함께 모델이 예측의 정확성을 얼마나 확실하는지 0 ~ 1사이의 숫자로 나타내는 신뢰도를 출력\n임계값 T에 따라 정밀도와 재현율이 달라짐\n\n임곗값 T 이하의 예측은 제거함\nT가 1에 가까우면 정밀도는 높지만 재현율은 낮음\n놓치는 객체가 많아져서 재현율이 낮아짐. 즉, 신뢰도가 높은 예측만 유지하기때문에 정밀도는 높아짐\nT가 0에 가까우면 정밀도는 낮지만 재현율은 높음\n대부분의 예측을 유지하기때문에 재현율은 높아지고, 거짓긍정(FP)이 많아져서 정밀도가 낮아짐\n\n예를 들어, 모델이 보행자를 탐지하고 있으면 특별한 이유없이 차를 세우더라도 어떤 보행자도 놓치지 않도록 재현율을 높여야 함 모델이 투자 기회를 탐지하고 있다면 일부 기회를 놓치게 되더라도 잘못된 기회에 돈을 거는 일을 피하기 위해 정밀도를 높여야 함\n\n\n[이미지 출처] https://www.researchgate.net/figure/a-Example-of-Precision-Recall-curve-with-the-precision-score-on-the-y-axis-and-the_fig1_321672019\n\n\nAP (Average Precision, 평균 정밀도) 와 mAP(mean Average Precision)\n\n곡선의 아래 영역에 해당\n항상 1x1 정사각형으로 구성되어 있음\n즉, 항상 0 ~ 1 사이의 값을 가짐\n단일 클래스에 대한 모델 성능 정보를 제공\n전역 점수를 얻기위해서 mAP를 사용\n예를 들어, 데이터셋이 10개의 클래스로 구성된다면 각 클래스에 대한 AP를 계산하고, 그 숫자들의 평균을 다시 구함\n(참고)\n\n최소 2개 이상의 객체를 탐지하는 대회인 PASCAL Visual Object Classes와 Common Objects in Context(COCO)에서 mAP가 사용됨\nCOCO 데이터셋이 더 많은 클래스를 포함하고 있기 때문에 보통 Pascal VOC보다 점수가 더 낮게 나옴\n예시\n\n[이미지 출처] https://www.researchgate.net/figure/Evaluation-on-PASCAL-VOC-2007-and-MS-COCO-test-dev_tbl2_328939155"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#dataset",
    "href": "docs/BME/posts/DL_Object_Detection.html#dataset",
    "title": "test",
    "section": "Dataset",
    "text": "Dataset\n\nVOC\n\n2005년부터 2012년까지 진행\nObject Detection 기술의 benchmark로 간주\n데이터셋에는 20개의 클래스가 존재\nbackground\naeroplane\nbicycle\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ndiningtable\ndog\nhorse\nmotorbike\nperson\npottedplant\nsheep\nsofa\ntrain\ntvmonitor\n훈련 및 검증 데이터 : 11,530개\nROI에 대한 27,450개의 Annotation이 존재\n이미지당 2.4개의 객체 존재\n\n[이미지 출처] https://paperswithcode.github.io/sotabench-eval/pascalvoc/\n\n\n\nCOCO Dataset\n\nCommon Objects in Context\n200,000개의 이미지\n80개의 카테고리에 500,000개 이상의 객체 Annotation이 존재 person bicycle car motorbike aeroplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe backpack umbrella handbag tie suitcase frisbee skis snowboard sports ball kite baseball bat baseball glove skateboard surfboard tennis racket bottle wine glass cup fork knife spoon bowl banana apple sandwich orange broccoli carrot hot dog pizza donut cake chair sofa pottedplant bed diningtable toilet tvmonitor laptop mouse remote keyboard cell phone microwave oven toaster sink refrigerator book clock vase scissors teddy bear hair drier toothbrush\nhttps://cocodataset.org/#home"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#yolo-backbone",
    "href": "docs/BME/posts/DL_Object_Detection.html#yolo-backbone",
    "title": "test",
    "section": "YOLO Backbone",
    "text": "YOLO Backbone\n\n백본 모델(backbone model) 기반\n특징 추출기(Feature Extractor)라고도 불림\nYOLO는 자체 맞춤 아키텍쳐 사용\n어떤 특징 추출기 아키텍쳐를 사용했는지에 따라 성능 달라짐\n\n[이미지 출처] https://www.researchgate.net/figure/Structure-detail-of-YOLOv3It-uses-Darknet-53-as-the-backbone-network-and-uses-three_fig1_335865923\n마지막 계층은 크기가 \\(w \\times h \\times D\\)인 특징 볼륨 출력\n$w h $는 그리드의 크기이고, \\(D\\)는 특징 볼륨 깊이"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#yolo의-계층-출력",
    "href": "docs/BME/posts/DL_Object_Detection.html#yolo의-계층-출력",
    "title": "test",
    "section": "YOLO의 계층 출력",
    "text": "YOLO의 계층 출력\n\n마지막 계층 출력은 \\(w \\times h \\times M\\) 행렬\n\n\\(M = B \\times (C + 5)\\)\n\nB : 그리드 셀당 경계 상자 개수\nC : 클래스 개수\n\n클래스 개수에 5를 더한 이유는 해당 값 만큼의 숫자를 예측해야함\n\n\\(t_x\\), \\(t_y\\)는 경계상자의 중심 좌표를 계산\n\\(t_w\\), \\(t_h\\)는 경계상자의 너비와 높이를 계산\n\\(c\\)는 객체가 경계 상자 안에 있다고 확신하는 신뢰도\n\\(p1, p2, ..., pC\\)는 경계상자가 클래스 1, 2, …, C의 객체를 포함할 확률\n\n\n\n\n[이미지 출처] https://www.researchgate.net/figure/Structure-of-one-output-cell-in-YOLO_fig3_337705605"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#앵커-박스anchor-box",
    "href": "docs/BME/posts/DL_Object_Detection.html#앵커-박스anchor-box",
    "title": "test",
    "section": "앵커 박스(Anchor Box)",
    "text": "앵커 박스(Anchor Box)\n\nYOLOv2에서 도입\n사전 정의된 상자(prior box)\n객체에 가장 근접한 앵커 박스를 맞추고 신경망을 사용해 앵커 박스의 크기를 조정하는 과정때문에 \\(t_x, t_y, t_w, t_h\\)이 필요\n\n[이미지 출처] https://kr.mathworks.com/help/vision/ug/getting-started-with-yolo-v2.html"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#clone-and-install-dependencies",
    "href": "docs/BME/posts/DL_Object_Detection.html#clone-and-install-dependencies",
    "title": "test",
    "section": "Clone and install dependencies",
    "text": "Clone and install dependencies"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#check-tensorflow2-version",
    "href": "docs/BME/posts/DL_Object_Detection.html#check-tensorflow2-version",
    "title": "test",
    "section": "Check Tensorflow2 version",
    "text": "Check Tensorflow2 version"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#convert-pretrained-darknet-weight",
    "href": "docs/BME/posts/DL_Object_Detection.html#convert-pretrained-darknet-weight",
    "title": "test",
    "section": "Convert Pretrained Darknet Weight",
    "text": "Convert Pretrained Darknet Weight\n\nhttps://pjreddie.com/media/files/yolov3.weights\nyolov3.weights를 /yolov3-tf2/data/로 넣기"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#initial-detector",
    "href": "docs/BME/posts/DL_Object_Detection.html#initial-detector",
    "title": "test",
    "section": "Initial Detector",
    "text": "Initial Detector"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#detect-image",
    "href": "docs/BME/posts/DL_Object_Detection.html#detect-image",
    "title": "test",
    "section": "Detect Image",
    "text": "Detect Image"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#prepare-yolov3-pytorch",
    "href": "docs/BME/posts/DL_Object_Detection.html#prepare-yolov3-pytorch",
    "title": "test",
    "section": "Prepare YOLOv3-pytorch",
    "text": "Prepare YOLOv3-pytorch"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#git-clone-to-get-short-videos",
    "href": "docs/BME/posts/DL_Object_Detection.html#git-clone-to-get-short-videos",
    "title": "test",
    "section": "Git clone to get short videos",
    "text": "Git clone to get short videos\n\nhttps://github.com/vindruid/yolov3-in-colab.git"
  },
  {
    "objectID": "docs/BME/posts/DL_Object_Detection.html#process-video",
    "href": "docs/BME/posts/DL_Object_Detection.html#process-video",
    "title": "test",
    "section": "Process Video",
    "text": "Process Video\n\nCustom Data Train 참고\n\nhttps://github.com/AntonMu/TrainYourOwnYOLO"
  },
  {
    "objectID": "docs/DL/posts/DCGAN_new.html",
    "href": "docs/DL/posts/DCGAN_new.html",
    "title": "DCGAN paper review",
    "section": "",
    "text": "[DCGAN 장점]\n- 대부분의 상황에서 안정적으로 학습이 됨 - word2vec과 같이 DCGAN으로 학습된 Generator가 벡터 산술 연산이 가능한 성질을 갖음\n\n\n\nAlt text\n\n\n\nDCGAN이 학습한 필터를 시각화하여 보여줌 -&gt; 특정 필터들이 이미지의 특정 물체를 학습했음을 보여줌\n\n\n\n\nAlt text\n\n\n\n성능면에서 비지도 학습 알고리즘에 비해 우수함\n\n[APPROACH AND MODEL ARCHITECTURE]\nDCGAN은\n- Max Pooling To Strided Convolution - Fully-Connected Layer 삭제 - BatchNormalization을 추가함 -&gt; deep한 모델이더라도 gradient의 흐름이 잘 전달됨 - ReLU와 Leaky ReLU를 사용\n[DETAILS OF ADVERSARIAL TRAINING]\n\n모델 및 옵티마이저\n\n\nmini-batch Stochastic Gradient Descent(SGD) a with batch size of 128\nAll weight: zero-centered Normal distribution with std 0.02\nLeaky ReLU: slope 0.2\nOptimizer: Adam (GAN에서는 momentum 사용)\nlearning late: 0.0002(0.001은 너무 커서..)\nD’s criterion= \\(\\log(D(x))\\)(real data) + \\(\\log(1 - D(G(x)))\\)(fake data)\nG’s criterion = \\(\\log(D(G(z)))\\)\n\n데이터\n\nLSUN\nFACES\nIMAGENET-1K\n\n\nDCGAN에서 중요한 기준 - NOT MIMICKING TRAIN DATA -&gt; 단순히 학습 데이터를 모방하면 안됨! - “Walking in the latent Space” -&gt; G의 input z의 공간인 latent Space에서 z1에서 z2로 살짝 이동한다 하더라도 급작스러운 변화가 일어나지 않고 물흐르듯 부드러운 변화를 보여줘야 한다.\n\n\n\n\n\n\n\nimport torch\nimport torch.utils.data as data\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom fastai.vision.all import * \nimport torchvision.datasets as dsets\nfrom torchvision import transforms\n\n\n\n데이터 다운로드\n.tgz 파일 압출 풀기\n\npath = untar_data(URLs.PETS)\n\nimport tarfile\n\nfname = \"C:/Users/default.DESKTOP-HUJV032/.fastai/archive/oxford-iiit-pet.tgz\"  # 압축 파일을 지정\nap = tarfile.open(fname)      \nap.extractall(\"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet\") # 압축 풀기\n \nap.close()  \n\n\nimg = PILImage.create(\"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/images/havanese_1.jpg\")\nimg\n\n\n\n\n\n\n강아지 고양이 폴더 분류 코드\n\n\n#파일을 폴더 분류에 맞게 이동\ndef moveFile(src, dst):\n    filelist = os.listdir(src)\n    for file in filelist:\n        if file[0].isupper():\n            shutil.copy(os.path.join(src, file), os.path.join(dst, 'cat'))\n        else:\n            shutil.copy(os.path.join(src, file), os.path.join(dst, 'dog'))\n\nsrc = \"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/images\"\ndst = \"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/\"\n\nmoveFile(src, dst)\n\n\n\n\n모든 이미지의 채널이 3이어야 에러가 발생하지 않아서 이를 탐색해주어야 함\n\n\ntest = ImageTransform(mean, std)\ntemp = []\nfor i in range(4988):\n    temp.append(test(Image.open(d_list[i])).shape[0])\n\n\nntemp = pd.DataFrame(temp)\nntemp.columns=['val']\nntemp[ntemp['val']==1]\n\n\n\n\n\n\ndog_path = \"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/dog\"\ndog_list = os.listdir(dog_path)\n\ncat_path = \"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/cat\"\ncat_list = os.listdir(cat_path)\n\n\nd_list = []\nfor dog in dog_list:\n    d_list.append(\"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/dog/\" + str(dog))\n\nc_list = []\nfor cat in cat_list:\n    c_list.append(\"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/cat/\" + str(cat))\n\n\nclass ImageTransform():\n    \"\"\"이미지 전처리 클래스\"\"\"\n    def __init__(self, mean, std):\n        self.data_transform = transforms.Compose([\n            transforms.Resize((64,64)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n\n    def __call__(self, img):\n        return self.data_transform(img)\n\n\nclass GAN_Img_Dataset(data.Dataset):\n    \"\"\"Dataset 클래스. PyTorch의 Dataset 클래스를 상속\"\"\"\n\n    def __init__(self, file_list, transform):\n        self.file_list = file_list\n        self.transform = transform\n\n    def __len__(self):\n        '''이미지 개수 반환'''\n        return len(self.file_list)\n\n    def __getitem__(self, index):\n        '''전처리된 이미지를 Tensor 형식 데이터로 변환'''\n        img_path = self.file_list[index]\n        img = Image.open(img_path)  # [높이][폭]흑백\n\n        # 이미지 전처리\n        img_transformed = self.transform(img)\n        img_transformed = img_transformed.type(torch.FloatTensor)\n        return img_transformed\n\n\n# Dataset 작성\nmean = (0.5,)\nstd = (0.5,)\ntrain_dataset = GAN_Img_Dataset(file_list=d_list, transform=ImageTransform(mean, std))\n\nbatch_size = 64\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n\nbatch_iterator = iter(train_dataloader)  # 반복자로 변환\nimges = next(batch_iterator)  # 1번째 요소를 꺼낸다\nprint(imges.size())  # torch.Size([64, 1, 64, 64])\n\ntorch.Size([64, 3, 64, 64])\n\n\n\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");device\n\ndevice(type='cuda', index=0)\n\n\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim = 20, image_size = 64):\n        super(Generator, self).__init__()\n\n        # layer1 -&gt; W(H) * 4\n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, image_size * 8, kernel_size = 4, stride = 1),\n            nn.BatchNorm2d(image_size * 8),\n            nn.ReLU(inplace=True))\n        \n        # layer2 -&gt; W(H) * 2\n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 8, image_size * 4, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size * 4),\n            nn.ReLU(inplace=True))\n        \n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 4, image_size * 2, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size * 2),\n            nn.ReLU(inplace=True))\n        \n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 2, image_size, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size),\n            nn.ReLU(inplace=True))\n        \n        self.last = nn.Sequential(\n            nn.ConvTranspose2d(image_size, 3, kernel_size= 4, stride=2, padding=1), # 흑백 이미지이므로 출력 차원을 1으로 지정한 것\n            nn.Tanh())\n        \n    def forward(self, z):\n        out = self.layer1(z)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        out = out.type(torch.FloatTensor)\n        return out\n\n\nG = Generator(z_dim=20, image_size=64).to(device)\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, z_dim=20, image_size=64):\n        super(Discriminator, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, image_size, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(image_size, image_size*2, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(image_size*2, image_size*4, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(image_size*4, image_size*8, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.last = nn.Conv2d(image_size*8, 1, kernel_size=4, stride=1)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        out = out.type(torch.FloatTensor)\n        return out\n\n\nD = Discriminator(z_dim=20, image_size=64).to(device)\n\n\n# 네트워크 초기화\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        # Conv2dとConvTranspose2d 초기화\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n    elif classname.find('BatchNorm') != -1:\n        # BatchNorm2d 초기화\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# 초기화 실시\nG.apply(weights_init)\nD.apply(weights_init)\n\nprint(\"네트워크 초기화 완료\")\n\n네트워크 초기화 완료\n\n\n\n\n\n\n# 모델을 학습시키는 함수를 작성\ndef train_model(G, D, dataloader, num_epochs):\n\n    # GPU가 사용 가능한지 확인\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"사용 장치: \", device)\n\n    # 최적화 기법 설정\n    g_lr, d_lr = 0.0001, 0.0004\n    beta1, beta2 = 0.0, 0.9\n    g_optimizer = torch.optim.Adam(G.parameters(), g_lr, [beta1, beta2])\n    d_optimizer = torch.optim.Adam(D.parameters(), d_lr, [beta1, beta2])\n\n    # 오차함수 정의\n    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n\n    # 파라미터를 하드코딩\n    z_dim = 20\n    mini_batch_size = 64\n\n    # 네트워크를 GPU로\n    G.to(device)\n    D.to(device)\n\n    G.train()  # 모델을 훈련 모드로\n    D.train()  # 모델을 훈련 모드로\n\n    # 네트워크가 어느 정도 고정되면, 고속화시킨다\n    torch.backends.cudnn.benchmark = True\n\n    num_train_imgs = len(dataloader.dataset)\n    batch_size = 64\n\n    # 반복 카운터 설정\n    iteration = 1\n    logs = []\n\n    # epoch 루프\n    for epoch in range(num_epochs):\n\n        # 개시 시간을 저장\n        t_epoch_start = time.time()\n        epoch_g_loss = 0.0  # epoch의 손실합\n        epoch_d_loss = 0.0  # epoch의 손실합\n\n        print('-------------')\n        print('Epoch {}/{}'.format(epoch, num_epochs))\n        print('-------------')\n        print('(train)')\n\n        # 데이터 로더에서 minibatch씩 꺼내는 루프\n        for imges in dataloader:\n\n            # --------------------\n            # 1. Discriminator 학습\n            # GPU가 사용 가능하면 GPU로 데이터를 보낸다\n            imges = imges.to(device)\n            \n            \n            # 정답 라벨과 가짜 라벨 작성\n            # epoch의 마지막 반복은 미니 배치 수가 줄어든다\n            mini_batch_size = imges.size()[0]\n            label_real = torch.full((mini_batch_size,), 1).to(device)\n            label_fake = torch.full((mini_batch_size,), 0).to(device)\n\n            # 진짜 이미지 판정\n            d_out_real = D(imges)\n\n            # 가짜 이미지 생성해 판정\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1).to(device)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images.to(device))\n\n            # 오차를 계산\n            d_loss_real = criterion(d_out_real.view(-1).to(device), label_real.float())\n            d_loss_fake = criterion(d_out_fake.view(-1).to(device), label_fake.float())\n            d_loss = d_loss_real + d_loss_fake\n\n            # 역전파\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n\n            d_loss.backward()\n            d_optimizer.step()\n\n            # 2. Generator 학습\n            # --------------------\n            # 가짜 이미지 생성해 판정\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1).to(device)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images.to(device))\n\n            # 오차를 계산\n            g_loss = criterion(d_out_fake.view(-1).to(device), label_real.float().to(device))\n\n            # 역전파\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n            g_loss.backward()\n            g_optimizer.step()\n\n            # --------------------\n            # 3. 기록\n            # --------------------\n            epoch_d_loss += d_loss.item()\n            epoch_g_loss += g_loss.item()\n            iteration += 1\n\n        # epoch의 phase별 loss와 정답률\n        t_epoch_finish = time.time()\n        print('-------------')\n        print('epoch {} || Epoch_D_Loss:{:.4f} ||Epoch_G_Loss:{:.4f}'.format(\n            epoch, epoch_d_loss/batch_size, epoch_g_loss/batch_size))\n        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n        t_epoch_start = time.time()\n\n    return G, D\n\n\nnum_epochs = 1000\nG_update, D_update = train_model(G, D, dataloader=train_dataloader, num_epochs=num_epochs)\n\n\n\n\n\nbatch_size = 8\nz_dim = 20\nfixed_z = torch.randn(batch_size, z_dim)\nfixed_z = fixed_z.view(fixed_z.size(0), fixed_z.size(1), 1, 1)\n\nG_update.eval()\nfake_images = G_update(fixed_z.to(device))\nbatch_iterator = iter(train_dataloader)  # 반복자로 변환\nimges = next(batch_iterator)  \n\n# 출력\nfig = plt.figure(figsize=(15, 6))\nfor i in range(0, 5):\n    # 상단에 훈련 데이터를,\n    plt.subplot(2, 5, i+1)\n    plt.imshow(torch.einsum('cij -&gt; ijc', imges[i]).cpu().detach().numpy())\n\n    # 하단에 생성 데이터를 표시한다\n    plt.subplot(2, 5, 5+i+1)\n    plt.imshow(torch.einsum('cij -&gt; ijc', fake_images[i]).cpu().detach().numpy())\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."
  },
  {
    "objectID": "docs/DL/posts/DCGAN_new.html#논문-리뷰",
    "href": "docs/DL/posts/DCGAN_new.html#논문-리뷰",
    "title": "DCGAN paper review",
    "section": "",
    "text": "[DCGAN 장점]\n- 대부분의 상황에서 안정적으로 학습이 됨 - word2vec과 같이 DCGAN으로 학습된 Generator가 벡터 산술 연산이 가능한 성질을 갖음\n\n\n\nAlt text\n\n\n\nDCGAN이 학습한 필터를 시각화하여 보여줌 -&gt; 특정 필터들이 이미지의 특정 물체를 학습했음을 보여줌\n\n\n\n\nAlt text\n\n\n\n성능면에서 비지도 학습 알고리즘에 비해 우수함\n\n[APPROACH AND MODEL ARCHITECTURE]\nDCGAN은\n- Max Pooling To Strided Convolution - Fully-Connected Layer 삭제 - BatchNormalization을 추가함 -&gt; deep한 모델이더라도 gradient의 흐름이 잘 전달됨 - ReLU와 Leaky ReLU를 사용\n[DETAILS OF ADVERSARIAL TRAINING]\n\n모델 및 옵티마이저\n\n\nmini-batch Stochastic Gradient Descent(SGD) a with batch size of 128\nAll weight: zero-centered Normal distribution with std 0.02\nLeaky ReLU: slope 0.2\nOptimizer: Adam (GAN에서는 momentum 사용)\nlearning late: 0.0002(0.001은 너무 커서..)\nD’s criterion= \\(\\log(D(x))\\)(real data) + \\(\\log(1 - D(G(x)))\\)(fake data)\nG’s criterion = \\(\\log(D(G(z)))\\)\n\n데이터\n\nLSUN\nFACES\nIMAGENET-1K\n\n\nDCGAN에서 중요한 기준 - NOT MIMICKING TRAIN DATA -&gt; 단순히 학습 데이터를 모방하면 안됨! - “Walking in the latent Space” -&gt; G의 input z의 공간인 latent Space에서 z1에서 z2로 살짝 이동한다 하더라도 급작스러운 변화가 일어나지 않고 물흐르듯 부드러운 변화를 보여줘야 한다."
  },
  {
    "objectID": "docs/DL/posts/DCGAN_new.html#논문-구현",
    "href": "docs/DL/posts/DCGAN_new.html#논문-구현",
    "title": "DCGAN paper review",
    "section": "",
    "text": "import torch\nimport torch.utils.data as data\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom fastai.vision.all import * \nimport torchvision.datasets as dsets\nfrom torchvision import transforms\n\n\n\n데이터 다운로드\n.tgz 파일 압출 풀기\n\npath = untar_data(URLs.PETS)\n\nimport tarfile\n\nfname = \"C:/Users/default.DESKTOP-HUJV032/.fastai/archive/oxford-iiit-pet.tgz\"  # 압축 파일을 지정\nap = tarfile.open(fname)      \nap.extractall(\"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet\") # 압축 풀기\n \nap.close()  \n\n\nimg = PILImage.create(\"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/images/havanese_1.jpg\")\nimg\n\n\n\n\n\n\n강아지 고양이 폴더 분류 코드\n\n\n#파일을 폴더 분류에 맞게 이동\ndef moveFile(src, dst):\n    filelist = os.listdir(src)\n    for file in filelist:\n        if file[0].isupper():\n            shutil.copy(os.path.join(src, file), os.path.join(dst, 'cat'))\n        else:\n            shutil.copy(os.path.join(src, file), os.path.join(dst, 'dog'))\n\nsrc = \"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/images\"\ndst = \"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/\"\n\nmoveFile(src, dst)\n\n\n\n\n모든 이미지의 채널이 3이어야 에러가 발생하지 않아서 이를 탐색해주어야 함\n\n\ntest = ImageTransform(mean, std)\ntemp = []\nfor i in range(4988):\n    temp.append(test(Image.open(d_list[i])).shape[0])\n\n\nntemp = pd.DataFrame(temp)\nntemp.columns=['val']\nntemp[ntemp['val']==1]\n\n\n\n\n\n\ndog_path = \"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/dog\"\ndog_list = os.listdir(dog_path)\n\ncat_path = \"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/cat\"\ncat_list = os.listdir(cat_path)\n\n\nd_list = []\nfor dog in dog_list:\n    d_list.append(\"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/dog/\" + str(dog))\n\nc_list = []\nfor cat in cat_list:\n    c_list.append(\"C:/Users/default.DESKTOP-HUJV032/.fastai/data/oxford-iiit-pet/cat/\" + str(cat))\n\n\nclass ImageTransform():\n    \"\"\"이미지 전처리 클래스\"\"\"\n    def __init__(self, mean, std):\n        self.data_transform = transforms.Compose([\n            transforms.Resize((64,64)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n\n    def __call__(self, img):\n        return self.data_transform(img)\n\n\nclass GAN_Img_Dataset(data.Dataset):\n    \"\"\"Dataset 클래스. PyTorch의 Dataset 클래스를 상속\"\"\"\n\n    def __init__(self, file_list, transform):\n        self.file_list = file_list\n        self.transform = transform\n\n    def __len__(self):\n        '''이미지 개수 반환'''\n        return len(self.file_list)\n\n    def __getitem__(self, index):\n        '''전처리된 이미지를 Tensor 형식 데이터로 변환'''\n        img_path = self.file_list[index]\n        img = Image.open(img_path)  # [높이][폭]흑백\n\n        # 이미지 전처리\n        img_transformed = self.transform(img)\n        img_transformed = img_transformed.type(torch.FloatTensor)\n        return img_transformed\n\n\n# Dataset 작성\nmean = (0.5,)\nstd = (0.5,)\ntrain_dataset = GAN_Img_Dataset(file_list=d_list, transform=ImageTransform(mean, std))\n\nbatch_size = 64\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n\nbatch_iterator = iter(train_dataloader)  # 반복자로 변환\nimges = next(batch_iterator)  # 1번째 요소를 꺼낸다\nprint(imges.size())  # torch.Size([64, 1, 64, 64])\n\ntorch.Size([64, 3, 64, 64])\n\n\n\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");device\n\ndevice(type='cuda', index=0)\n\n\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim = 20, image_size = 64):\n        super(Generator, self).__init__()\n\n        # layer1 -&gt; W(H) * 4\n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, image_size * 8, kernel_size = 4, stride = 1),\n            nn.BatchNorm2d(image_size * 8),\n            nn.ReLU(inplace=True))\n        \n        # layer2 -&gt; W(H) * 2\n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 8, image_size * 4, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size * 4),\n            nn.ReLU(inplace=True))\n        \n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 4, image_size * 2, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size * 2),\n            nn.ReLU(inplace=True))\n        \n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 2, image_size, kernel_size = 4, stride = 2, padding=1),\n            nn.BatchNorm2d(image_size),\n            nn.ReLU(inplace=True))\n        \n        self.last = nn.Sequential(\n            nn.ConvTranspose2d(image_size, 3, kernel_size= 4, stride=2, padding=1), # 흑백 이미지이므로 출력 차원을 1으로 지정한 것\n            nn.Tanh())\n        \n    def forward(self, z):\n        out = self.layer1(z)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        out = out.type(torch.FloatTensor)\n        return out\n\n\nG = Generator(z_dim=20, image_size=64).to(device)\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, z_dim=20, image_size=64):\n        super(Discriminator, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, image_size, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(image_size, image_size*2, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(image_size*2, image_size*4, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(image_size*4, image_size*8, kernel_size=4,stride=2, padding=1),\n            nn.LeakyReLU(0.1, inplace=True))\n\n        self.last = nn.Conv2d(image_size*8, 1, kernel_size=4, stride=1)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        out = out.type(torch.FloatTensor)\n        return out\n\n\nD = Discriminator(z_dim=20, image_size=64).to(device)\n\n\n# 네트워크 초기화\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        # Conv2dとConvTranspose2d 초기화\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n    elif classname.find('BatchNorm') != -1:\n        # BatchNorm2d 초기화\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# 초기화 실시\nG.apply(weights_init)\nD.apply(weights_init)\n\nprint(\"네트워크 초기화 완료\")\n\n네트워크 초기화 완료\n\n\n\n\n\n\n# 모델을 학습시키는 함수를 작성\ndef train_model(G, D, dataloader, num_epochs):\n\n    # GPU가 사용 가능한지 확인\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"사용 장치: \", device)\n\n    # 최적화 기법 설정\n    g_lr, d_lr = 0.0001, 0.0004\n    beta1, beta2 = 0.0, 0.9\n    g_optimizer = torch.optim.Adam(G.parameters(), g_lr, [beta1, beta2])\n    d_optimizer = torch.optim.Adam(D.parameters(), d_lr, [beta1, beta2])\n\n    # 오차함수 정의\n    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n\n    # 파라미터를 하드코딩\n    z_dim = 20\n    mini_batch_size = 64\n\n    # 네트워크를 GPU로\n    G.to(device)\n    D.to(device)\n\n    G.train()  # 모델을 훈련 모드로\n    D.train()  # 모델을 훈련 모드로\n\n    # 네트워크가 어느 정도 고정되면, 고속화시킨다\n    torch.backends.cudnn.benchmark = True\n\n    num_train_imgs = len(dataloader.dataset)\n    batch_size = 64\n\n    # 반복 카운터 설정\n    iteration = 1\n    logs = []\n\n    # epoch 루프\n    for epoch in range(num_epochs):\n\n        # 개시 시간을 저장\n        t_epoch_start = time.time()\n        epoch_g_loss = 0.0  # epoch의 손실합\n        epoch_d_loss = 0.0  # epoch의 손실합\n\n        print('-------------')\n        print('Epoch {}/{}'.format(epoch, num_epochs))\n        print('-------------')\n        print('(train)')\n\n        # 데이터 로더에서 minibatch씩 꺼내는 루프\n        for imges in dataloader:\n\n            # --------------------\n            # 1. Discriminator 학습\n            # GPU가 사용 가능하면 GPU로 데이터를 보낸다\n            imges = imges.to(device)\n            \n            \n            # 정답 라벨과 가짜 라벨 작성\n            # epoch의 마지막 반복은 미니 배치 수가 줄어든다\n            mini_batch_size = imges.size()[0]\n            label_real = torch.full((mini_batch_size,), 1).to(device)\n            label_fake = torch.full((mini_batch_size,), 0).to(device)\n\n            # 진짜 이미지 판정\n            d_out_real = D(imges)\n\n            # 가짜 이미지 생성해 판정\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1).to(device)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images.to(device))\n\n            # 오차를 계산\n            d_loss_real = criterion(d_out_real.view(-1).to(device), label_real.float())\n            d_loss_fake = criterion(d_out_fake.view(-1).to(device), label_fake.float())\n            d_loss = d_loss_real + d_loss_fake\n\n            # 역전파\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n\n            d_loss.backward()\n            d_optimizer.step()\n\n            # 2. Generator 학습\n            # --------------------\n            # 가짜 이미지 생성해 판정\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1).to(device)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images.to(device))\n\n            # 오차를 계산\n            g_loss = criterion(d_out_fake.view(-1).to(device), label_real.float().to(device))\n\n            # 역전파\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n            g_loss.backward()\n            g_optimizer.step()\n\n            # --------------------\n            # 3. 기록\n            # --------------------\n            epoch_d_loss += d_loss.item()\n            epoch_g_loss += g_loss.item()\n            iteration += 1\n\n        # epoch의 phase별 loss와 정답률\n        t_epoch_finish = time.time()\n        print('-------------')\n        print('epoch {} || Epoch_D_Loss:{:.4f} ||Epoch_G_Loss:{:.4f}'.format(\n            epoch, epoch_d_loss/batch_size, epoch_g_loss/batch_size))\n        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n        t_epoch_start = time.time()\n\n    return G, D\n\n\nnum_epochs = 1000\nG_update, D_update = train_model(G, D, dataloader=train_dataloader, num_epochs=num_epochs)\n\n\n\n\n\nbatch_size = 8\nz_dim = 20\nfixed_z = torch.randn(batch_size, z_dim)\nfixed_z = fixed_z.view(fixed_z.size(0), fixed_z.size(1), 1, 1)\n\nG_update.eval()\nfake_images = G_update(fixed_z.to(device))\nbatch_iterator = iter(train_dataloader)  # 반복자로 변환\nimges = next(batch_iterator)  \n\n# 출력\nfig = plt.figure(figsize=(15, 6))\nfor i in range(0, 5):\n    # 상단에 훈련 데이터를,\n    plt.subplot(2, 5, i+1)\n    plt.imshow(torch.einsum('cij -&gt; ijc', imges[i]).cpu().detach().numpy())\n\n    # 하단에 생성 데이터를 표시한다\n    plt.subplot(2, 5, 5+i+1)\n    plt.imshow(torch.einsum('cij -&gt; ijc', fake_images[i]).cpu().detach().numpy())\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."
  },
  {
    "objectID": "docs/DL/posts/KnowledgeDistillation.html",
    "href": "docs/DL/posts/KnowledgeDistillation.html",
    "title": "Knowledge Distillation paper review - Knowledge",
    "section": "",
    "text": "Knowledge Distillation에 대해 정리한 survey논문 리뷰\nKnowledge Distillation model (KD model)의 목적을 간략히 설명하면 large scale model(teacher model)을 KD model을 통해 small scale model(student model)로 압축(compression)하여 파라미터를 최적화 시키는 것\n이번 페이지에서는 large sclae model의 Knowledge를 small scale model에게 옮길 때 어떠한 형태(form)로 모델의 knowledge를 추출한 후 옮기는지에 대해 설명할 것\n\n\n\n\n\n\nimg\n\n\n\n\n\n\n\nimg\n\n\nmain idea 해당 모델은 large scale model의 last ouput layer를 사용하여 small scale model이 이를 모방(mimic)하게 하는 것\n\n\n\nDistillation Loss for soft logits\n\\(L_{ResD} (p(z_{t}, T), p(z{s}, T)) = L_{R} (p(z_{t}, T), p(z_{S}, T))\\) 우항은 Kullback-Leibler divergence Loss를 따름\n\\(where\\) \\(logits = ln(odds) = ln (\\frac{p}{1-p})\\), Softmax(T = t) = \\(p(z_{i}, T)\\) =\\(\\frac{e^{a/t}}{e^{a/t} + e^{b/t} + e^{c/t}}\\)\n이를 정리하면 large scale model의 output(soft target)과 small scale model의 output(soft target)의 차이를 최소화 시키는 방향으로 학습을 진행\nStudent Loss 는 small scale model의 output을 정답(label)과 매치시켜 학습을 진행\n\\(L_{CE}(y, p(z_{s}, T = 1))\\)\n\\(where\\) Softmax(T = 1) = \\(\\frac{e^{a}}{e^{a} + e^{b} + e^{c}}\\)\n\n\n\n\n\n\nmain idea feature activation을 직접적으로 매치시켜서 각 layer를 유사하도록 학습을 진행\nFeature-Based Knowledge는 얇고 깊은 network에 적합함\n\n\n\nDistillation Loss for feature-based knowledge transfer\n\\(L_{FeaD}(f_{t}(x), f_{s}(x))\\) = \\(L_{F}(\\phi_{t}(f_{t}(x)), \\phi_{t}(f_{t}(x)))\\)\n\\(where\\) \\(\\phi\\) 는 image shape을 바꿔주는 function,\n\\(f_{t}(x), f_{s}(x)\\)는 중간 층(intermediate layers)의 activation map을 의미,\n\\(L_{F}()\\) 는 feature map간의 유사도를 확인해주는 function\n이를 정리하면 상황에 따라 \\(f_{t}()\\)가 attention map, Feature map이 될 수 는 있지만 궁극적으로는 Teacher Model의 layer와 Student Model의 layer를 매치시켜 최대한 유사하도록 학습을 진행\n\n※ Teacher Model의 hint layer와 Student Model의 guided layer를 어떻게 효율적으로 선택해야 하는지는 지속적인 연구가 필요\n\n\n\n\n\nmain idea 다른 층 간의 상관관계 혹은 데이터 샘플간의 상관관계를 탐색하여 이를 바탕으로 학습을 진행\nexample)\n1. feature map간의 데이터 상관관계를 탐색하고 Student model은 이를 사용해 상호간의 정보 흐름을 모방하며 학습\n2. 각기 다른 Teacher Model의 상관관계와 중요도를 2개의 그래프로 표현하는데 이는 logits과 feature를 사용함\n\n\n\nDistillation Loss of relation-based knowledge based on the relations of feature maps\n\\(L_{RelD}(f_{t}, f_{s})\\) = \\(L_{R^1}(\\psi_{t}(\\hat f_{t}, \\check f_{t}), \\psi_{t}(\\hat f_{s}, \\check f_{s}))\\)\n\\(where\\) \\(\\hat f_{t,s}, \\check f_{t,s}\\) 는 Teacher, Student Model로부터 선택된 feature map 쌍,\n\\(\\psi ()\\) 는 feature map 간의 유사도를 확인해주는 function,\n\\(L_{R^1}()\\) 는 Teacher, Student feature map간의 상관관계를 확인해주는 function\nDistillation Loss of relation-based knowledge based on the relations of instance relationship\n\\(L_{RelD}(f_{t}, f_{s})\\) = \\(L_{R^2}(\\psi_{t}(t_{i}, t_{j}), \\psi_{t}(s_{i}, s_{j}))\\)\n\\(where\\) \\((t_{i},t_{j}) \\in F_{t}, (s_{i},s_{j}) \\in F_{s}\\)\n\\(F_{t,s}\\) 는 Teacher, Student Model로부터의 sets of feature representation,\n\\(\\psi ()\\) 는 feature representation 간의 유사도를 확인해주는 function,\n\\(L_{R^2}()\\) 는 Teacher, Student feature map간의 상관관계를 확인해주는 function"
  },
  {
    "objectID": "docs/DL/posts/KnowledgeDistillation.html#knowledge",
    "href": "docs/DL/posts/KnowledgeDistillation.html#knowledge",
    "title": "Knowledge Distillation paper review - Knowledge",
    "section": "",
    "text": "img\n\n\n\n\n\n\n\nimg\n\n\nmain idea 해당 모델은 large scale model의 last ouput layer를 사용하여 small scale model이 이를 모방(mimic)하게 하는 것\n\n\n\nDistillation Loss for soft logits\n\\(L_{ResD} (p(z_{t}, T), p(z{s}, T)) = L_{R} (p(z_{t}, T), p(z_{S}, T))\\) 우항은 Kullback-Leibler divergence Loss를 따름\n\\(where\\) \\(logits = ln(odds) = ln (\\frac{p}{1-p})\\), Softmax(T = t) = \\(p(z_{i}, T)\\) =\\(\\frac{e^{a/t}}{e^{a/t} + e^{b/t} + e^{c/t}}\\)\n이를 정리하면 large scale model의 output(soft target)과 small scale model의 output(soft target)의 차이를 최소화 시키는 방향으로 학습을 진행\nStudent Loss 는 small scale model의 output을 정답(label)과 매치시켜 학습을 진행\n\\(L_{CE}(y, p(z_{s}, T = 1))\\)\n\\(where\\) Softmax(T = 1) = \\(\\frac{e^{a}}{e^{a} + e^{b} + e^{c}}\\)\n\n\n\n\n\n\nmain idea feature activation을 직접적으로 매치시켜서 각 layer를 유사하도록 학습을 진행\nFeature-Based Knowledge는 얇고 깊은 network에 적합함\n\n\n\nDistillation Loss for feature-based knowledge transfer\n\\(L_{FeaD}(f_{t}(x), f_{s}(x))\\) = \\(L_{F}(\\phi_{t}(f_{t}(x)), \\phi_{t}(f_{t}(x)))\\)\n\\(where\\) \\(\\phi\\) 는 image shape을 바꿔주는 function,\n\\(f_{t}(x), f_{s}(x)\\)는 중간 층(intermediate layers)의 activation map을 의미,\n\\(L_{F}()\\) 는 feature map간의 유사도를 확인해주는 function\n이를 정리하면 상황에 따라 \\(f_{t}()\\)가 attention map, Feature map이 될 수 는 있지만 궁극적으로는 Teacher Model의 layer와 Student Model의 layer를 매치시켜 최대한 유사하도록 학습을 진행\n\n※ Teacher Model의 hint layer와 Student Model의 guided layer를 어떻게 효율적으로 선택해야 하는지는 지속적인 연구가 필요\n\n\n\n\n\nmain idea 다른 층 간의 상관관계 혹은 데이터 샘플간의 상관관계를 탐색하여 이를 바탕으로 학습을 진행\nexample)\n1. feature map간의 데이터 상관관계를 탐색하고 Student model은 이를 사용해 상호간의 정보 흐름을 모방하며 학습\n2. 각기 다른 Teacher Model의 상관관계와 중요도를 2개의 그래프로 표현하는데 이는 logits과 feature를 사용함\n\n\n\nDistillation Loss of relation-based knowledge based on the relations of feature maps\n\\(L_{RelD}(f_{t}, f_{s})\\) = \\(L_{R^1}(\\psi_{t}(\\hat f_{t}, \\check f_{t}), \\psi_{t}(\\hat f_{s}, \\check f_{s}))\\)\n\\(where\\) \\(\\hat f_{t,s}, \\check f_{t,s}\\) 는 Teacher, Student Model로부터 선택된 feature map 쌍,\n\\(\\psi ()\\) 는 feature map 간의 유사도를 확인해주는 function,\n\\(L_{R^1}()\\) 는 Teacher, Student feature map간의 상관관계를 확인해주는 function\nDistillation Loss of relation-based knowledge based on the relations of instance relationship\n\\(L_{RelD}(f_{t}, f_{s})\\) = \\(L_{R^2}(\\psi_{t}(t_{i}, t_{j}), \\psi_{t}(s_{i}, s_{j}))\\)\n\\(where\\) \\((t_{i},t_{j}) \\in F_{t}, (s_{i},s_{j}) \\in F_{s}\\)\n\\(F_{t,s}\\) 는 Teacher, Student Model로부터의 sets of feature representation,\n\\(\\psi ()\\) 는 feature representation 간의 유사도를 확인해주는 function,\n\\(L_{R^2}()\\) 는 Teacher, Student feature map간의 상관관계를 확인해주는 function"
  },
  {
    "objectID": "docs/DL/posts/KnowledgeDistillation.html#distillation-schemes",
    "href": "docs/DL/posts/KnowledgeDistillation.html#distillation-schemes",
    "title": "Knowledge Distillation paper review",
    "section": "",
    "text": "※ 이러한 3가지의 Distillation Scheme은 서로를 보완하기 위해서 합쳐서 사용될 때도 있음\n\n\nmain idea 잘 학습된 모델(knowledgeable teacher)가 작은 모델(student)을 특성을 match시키거나 분포를 match시키는 방향으로 학습 진행\nOffline Distillation 진행 순서\n1. large teacher model을 training set를 사용하여 훈련시킴\n2. 학습된 teacher model을 사용해 knowledge를 logits 또는 중간층의 feature 형태로 추출하고 이를 통해 student model 학습\n단점 student model의 성능을 결국 teacher model의 성능을 따르게 되는데 좋은 성능의 teacher model은 복잡하고 용량이 커서 학습에 오랜시간이 걸린다.\n\n\n\nmain idea large-scale model(teacher model)과 small scale model(student model)을 동시에 학습을 시키는 것\n\n\n\nmain idea 작은 모델(student)이 스스로 학습을 하는 것"
  },
  {
    "objectID": "docs/DL/posts/KnowledgeDistillation2.html",
    "href": "docs/DL/posts/KnowledgeDistillation2.html",
    "title": "Knowledge Distillation paper review part2",
    "section": "",
    "text": "Knowledge Distillation에 대해 정리한 survey 논문 리뷰\nKnowledge Distillation model (KD model)의 목적을 간략히 설명하면 large scale model(teacher model)을 KD model을 통해 small scale model(student model)로 압축(compression)하여 파라미터를 최적화 시키는 것\n이번 페이지에서는 아래와 같은 내용을 정리할 것\n\nsmall scale model에게 knowledge를 옮길 때 어떠한 방식(Distillation Scheme)으로 student model을 학습시키는지\n전이되는 knowledge의 퀄리티를 결정하는 Teacher and Student networks의 구조\n\n\n\n\n\n※ 이러한 3가지의 Distillation Scheme은 서로를 보완하기 위해서 합쳐서 사용될 때도 있음\n\n\nmain idea 잘 학습된 모델(knowledgeable teacher)가 작은 모델(student)을 특성을 match시키거나 분포를 match시키는 방향으로 학습 진행\nOffline Distillation 진행 순서\n\nlarge teacher model을 training set를 사용하여 훈련시킴\n\n학습된 teacher model을 사용해 knowledge를 logits 또는 중간층의 feature 형태로 추출하고 이를 통해 student model 학습\n\n단점 student model의 성능을 결국 teacher model의 성능을 따르게 되는데 좋은 성능의 teacher model은 복잡하고 용량이 커서 학습에 오랜시간이 걸린다.\n\n\n\nmain idea large-scale model(teacher model)과 small scale model(student model)을 동시에 학습을 시키는 것으로 Offline Distillation에서 complex high-capacity teacher model이 필요하다는 단점을 극복하기 위해 만들어진 기법\n\nOnline Distillation은 one-phase end-to-end 구조를 띄고 있다는 장점이 있음\n\nexample of Online Distillation\n\n일반화 성능을 높이기 위해서는 ensemble of soft logits을 사용하여 deep mutual learning을 확장시킴\n\n연산량을 줄이기 위해서는 각 branch는 student model을 나타내고 different branch는 같은 backbone network를 공유하는 multibracnh architecture를 제안함\n\n※ 고용량의 teacher model을 Online 환경에서 다루지 못하여 Online 환경에서 teacher model과 student model의 관계를 파악하는 것이 흥미로운 주제임\n\n\n\nmain idea 작은 모델(student)이 스스로 학습을 하는 것으로 같은 network를 teacher model과 student model이 사용한다.\n+ Online Distillation의 특별한 case로 여겨질 수 있음\nexample of Self-Distillation\n\nSnapshot distillation은 self-distillation의 특별한 이형으로 초기 에포크의 네트워크 knowledge가 teacher model이 되어 마지막 부분의 에포크의 네트워크에게 knowledge를 전달하는 구조\n\nfeature embedding space에 있는 데이터 유사도를 반영하는 기법\n\nclass-wise self-knowledge distillation은 학습된 모델의 intra-class samples의 결과 분포와 augmented samples의 결과 분포를 match시키는 기법\n\n\n\n\n\n\nstructure of student model\n\nteacher network를 layer와 channel의 수를 줄임으로 간략하게 표현한 구조\n\n구조가 보존되는 teacher network의 quantized version\n양자화(quantized)란, 모델에 사용되는 파라미터를 특정 비트로 줄여서 모델의 크기를 줄여 inference time을 줄이는 효과를 냄\n\n효율적인 기본 과정을 낼 수 있는 small network\n\nsmall network with optimized global network structure ?\n\nteacher model과 같은 구조\n\nexample of T-S Architecture\n\ntraining gap을 줄이기 위해 보조 teacher model, residual learning 제안됨\n구조적 차이를 줄이기 위해 student model의 구조를 teacher 모델의 small or qunatized version으로 구축\n\n여러 개의 layer를 하나의 layer로 압축\nblock-wise knowledge transfer ?\n요즘은 Depth-wise separable convolution를 통해 모델 경량화를 진행"
  },
  {
    "objectID": "docs/DL/posts/KnowledgeDistillation2.html#distillation-schemes",
    "href": "docs/DL/posts/KnowledgeDistillation2.html#distillation-schemes",
    "title": "Knowledge Distillation paper review part2",
    "section": "",
    "text": "※ 이러한 3가지의 Distillation Scheme은 서로를 보완하기 위해서 합쳐서 사용될 때도 있음\n\n\nmain idea 잘 학습된 모델(knowledgeable teacher)가 작은 모델(student)을 특성을 match시키거나 분포를 match시키는 방향으로 학습 진행\nOffline Distillation 진행 순서\n\nlarge teacher model을 training set를 사용하여 훈련시킴\n\n학습된 teacher model을 사용해 knowledge를 logits 또는 중간층의 feature 형태로 추출하고 이를 통해 student model 학습\n\n단점 student model의 성능을 결국 teacher model의 성능을 따르게 되는데 좋은 성능의 teacher model은 복잡하고 용량이 커서 학습에 오랜시간이 걸린다.\n\n\n\nmain idea large-scale model(teacher model)과 small scale model(student model)을 동시에 학습을 시키는 것으로 Offline Distillation에서 complex high-capacity teacher model이 필요하다는 단점을 극복하기 위해 만들어진 기법\n\nOnline Distillation은 one-phase end-to-end 구조를 띄고 있다는 장점이 있음\n\nexample of Online Distillation\n\n일반화 성능을 높이기 위해서는 ensemble of soft logits을 사용하여 deep mutual learning을 확장시킴\n\n연산량을 줄이기 위해서는 각 branch는 student model을 나타내고 different branch는 같은 backbone network를 공유하는 multibracnh architecture를 제안함\n\n※ 고용량의 teacher model을 Online 환경에서 다루지 못하여 Online 환경에서 teacher model과 student model의 관계를 파악하는 것이 흥미로운 주제임\n\n\n\nmain idea 작은 모델(student)이 스스로 학습을 하는 것으로 같은 network를 teacher model과 student model이 사용한다.\n+ Online Distillation의 특별한 case로 여겨질 수 있음\nexample of Self-Distillation\n\nSnapshot distillation은 self-distillation의 특별한 이형으로 초기 에포크의 네트워크 knowledge가 teacher model이 되어 마지막 부분의 에포크의 네트워크에게 knowledge를 전달하는 구조\n\nfeature embedding space에 있는 데이터 유사도를 반영하는 기법\n\nclass-wise self-knowledge distillation은 학습된 모델의 intra-class samples의 결과 분포와 augmented samples의 결과 분포를 match시키는 기법"
  },
  {
    "objectID": "docs/DL/posts/KnowledgeDistillation1.html",
    "href": "docs/DL/posts/KnowledgeDistillation1.html",
    "title": "Knowledge Distillation paper review part1",
    "section": "",
    "text": "Knowledge Distillation에 대해 정리한 survey논문 리뷰\nKnowledge Distillation model (KD model)의 목적을 간략히 설명하면 large scale model(teacher model)을 KD model을 통해 small scale model(student model)로 압축(compression)하여 파라미터를 최적화 시키는 것\n이번 페이지에서는 large sclae model의 Knowledge를 small scale model에게 옮길 때 어떠한 형태(form)로 모델의 knowledge를 추출한 후 옮기는지에 대해 설명할 것\n\n\n\n\n\n\nimg\n\n\n\n\n\n\n\nimg\n\n\nmain idea 해당 모델은 large scale model의 last ouput layer를 사용하여 small scale model이 이를 모방(mimic)하게 하는 것\n\n\n\nDistillation Loss for soft logits\n\\(L_{ResD} (p(z_{t}, T), p(z{s}, T)) = L_{R} (p(z_{t}, T), p(z_{S}, T))\\)\n\\(where\\) \\(p(z_{i}, T)\\) = Softmax(T = t)= Soft Target =\\(\\frac{e^{a/T}}{e^{a/T} + e^{b/T} + e^{c/T}}\\)\n이를 정리하면 Kullback-Leibler Divergence Loss( \\(L_{R}\\) )를 따르므로 Student, Teacher model의 Output인 Soft Target의 분포를 비교하는 것을 의미\nStudent Loss 는 small scale model의 output을 정답(label)과 매치시켜 학습을 진행\n\\(L_{CE}(y, p(z_{s}, T = 1))\\)\n\\(where\\) Softmax(T = 1) = \\(\\frac{e^{a}}{e^{a} + e^{b} + e^{c}}\\)\nTotal Loss 는 아래와 같이 Distillation Loss와 Student Loss에 적절한 가중치를 주어 산출\n\\(L = \\sum L_{KD}(S(x,\\theta _{S}, T ), T(x, \\theta _{T}, T)) T^2 \\alpha+ L_{CE}(\\hat y_{S}, y)(1-\\alpha)\\)\n\nTemperture가 커진다면 Model이 학습할때 Distillation Loss를 줄이는 방향으로 학습을 하고 prediction 값은 더욱더 Soft하게 됨\n\n\\(\\alpha\\) 가 커진다면 Model이 학습할 때 Distillation Loss를 줄이는 방향으로 학습을 하게 됨\n\n\n이를 코드로 작성한다면 아래와 같다.\ndef Total_Loss_fn(student_pred, teacher_pred, alpha, T):\n    loss = F.kl_div(F.log_softmax(student_pred / T, dim=1), F.softmax(teacher_pred / T, dim=1), reduction='batchmean') * (T ** 2) * alpha + F.cross_entropy(student_pred, y) * (1 - alpha)\n    return loss\n\n\n\n\n\nmain idea feature activation을 직접적으로 매치시켜서 각 layer를 유사하도록 학습을 진행\nFeature-Based Knowledge는 얇고 깊은 network에 적합함\n\n\n\nDistillation Loss for feature-based knowledge transfer\n\\(L_{FeaD}(f_{t}(x), f_{s}(x))\\) = \\(L_{F}(\\phi_{t}(f_{t}(x)), \\phi_{s}(f_{s}(x)))\\)\n\\(where\\) \\(\\phi\\) 는 image shape을 바꿔주는 function,\n\\(f_{t}(x), f_{s}(x)\\)는 중간 층(intermediate layers)의 activation map을 의미,\n\\(L_{F}()\\) 는 feature map간의 유사도를 확인해주는 function\n이를 정리하면 상황에 따라 \\(f_{t}()\\)가 attention map, Feature map이 될 수 는 있지만 궁극적으로는 Teacher Model의 layer와 Student Model의 layer를 매치시켜 최대한 유사하도록 학습을 진행\n\n※ Teacher Model의 hint layer와 Student Model의 guided layer를 어떻게 효율적으로 선택해야 하는지는 지속적인 연구가 필요\n\n\n\n\n\nmain idea 다른 층 간의 상관관계 혹은 데이터 샘플간의 상관관계를 탐색하여 이를 바탕으로 학습을 진행\nexample)\n1. feature map간의 데이터 상관관계를 탐색하고 Student model은 이를 사용해 상호간의 정보 흐름을 모방하며 학습\n2. 각기 다른 Teacher Model의 상관관계와 중요도를 2개의 그래프로 표현하는데 이는 logits과 feature를 사용함\n\n\n\nDistillation Loss of relation-based knowledge based on the relations of feature maps\n\\(L_{RelD}(f_{t}, f_{s})\\) = \\(L_{R^1}(\\psi_{t}(\\hat f_{t}, \\check f_{t}), \\psi_{t}(\\hat f_{s}, \\check f_{s}))\\)\n\\(where\\) \\(\\hat f_{t,s}, \\check f_{t,s}\\) 는 Teacher, Student Model로부터 선택된 feature map 쌍,\n\\(\\psi ()\\) 는 feature map 간의 유사도를 확인해주는 function,\n\\(L_{R^1}()\\) 는 Teacher, Student feature map간의 상관관계를 확인해주는 function\nDistillation Loss of relation-based knowledge based on the relations of instance relationship\n\\(L_{RelD}(f_{t}, f_{s})\\) = \\(L_{R^2}(\\psi_{t}(t_{i}, t_{j}), \\psi_{t}(s_{i}, s_{j}))\\)\n\\(where\\) \\((t_{i},t_{j}) \\in F_{t}, (s_{i},s_{j}) \\in F_{s}\\)\n\\(F_{t,s}\\) 는 Teacher, Student Model로부터의 sets of feature representation,\n\\(\\psi ()\\) 는 feature representation 간의 유사도를 확인해주는 function,\n\\(L_{R^2}()\\) 는 Teacher, Student feature map간의 상관관계를 확인해주는 function"
  },
  {
    "objectID": "docs/DL/posts/KnowledgeDistillation1.html#knowledge",
    "href": "docs/DL/posts/KnowledgeDistillation1.html#knowledge",
    "title": "Knowledge Distillation paper review part1",
    "section": "",
    "text": "img\n\n\n\n\n\n\n\nimg\n\n\nmain idea 해당 모델은 large scale model의 last ouput layer를 사용하여 small scale model이 이를 모방(mimic)하게 하는 것\n\n\n\nDistillation Loss for soft logits\n\\(L_{ResD} (p(z_{t}, T), p(z{s}, T)) = L_{R} (p(z_{t}, T), p(z_{S}, T))\\)\n\\(where\\) \\(p(z_{i}, T)\\) = Softmax(T = t)= Soft Target =\\(\\frac{e^{a/T}}{e^{a/T} + e^{b/T} + e^{c/T}}\\)\n이를 정리하면 Kullback-Leibler Divergence Loss( \\(L_{R}\\) )를 따르므로 Student, Teacher model의 Output인 Soft Target의 분포를 비교하는 것을 의미\nStudent Loss 는 small scale model의 output을 정답(label)과 매치시켜 학습을 진행\n\\(L_{CE}(y, p(z_{s}, T = 1))\\)\n\\(where\\) Softmax(T = 1) = \\(\\frac{e^{a}}{e^{a} + e^{b} + e^{c}}\\)\nTotal Loss 는 아래와 같이 Distillation Loss와 Student Loss에 적절한 가중치를 주어 산출\n\\(L = \\sum L_{KD}(S(x,\\theta _{S}, T ), T(x, \\theta _{T}, T)) T^2 \\alpha+ L_{CE}(\\hat y_{S}, y)(1-\\alpha)\\)\n\nTemperture가 커진다면 Model이 학습할때 Distillation Loss를 줄이는 방향으로 학습을 하고 prediction 값은 더욱더 Soft하게 됨\n\n\\(\\alpha\\) 가 커진다면 Model이 학습할 때 Distillation Loss를 줄이는 방향으로 학습을 하게 됨\n\n\n이를 코드로 작성한다면 아래와 같다.\ndef Total_Loss_fn(student_pred, teacher_pred, alpha, T):\n    loss = F.kl_div(F.log_softmax(student_pred / T, dim=1), F.softmax(teacher_pred / T, dim=1), reduction='batchmean') * (T ** 2) * alpha + F.cross_entropy(student_pred, y) * (1 - alpha)\n    return loss\n\n\n\n\n\nmain idea feature activation을 직접적으로 매치시켜서 각 layer를 유사하도록 학습을 진행\nFeature-Based Knowledge는 얇고 깊은 network에 적합함\n\n\n\nDistillation Loss for feature-based knowledge transfer\n\\(L_{FeaD}(f_{t}(x), f_{s}(x))\\) = \\(L_{F}(\\phi_{t}(f_{t}(x)), \\phi_{s}(f_{s}(x)))\\)\n\\(where\\) \\(\\phi\\) 는 image shape을 바꿔주는 function,\n\\(f_{t}(x), f_{s}(x)\\)는 중간 층(intermediate layers)의 activation map을 의미,\n\\(L_{F}()\\) 는 feature map간의 유사도를 확인해주는 function\n이를 정리하면 상황에 따라 \\(f_{t}()\\)가 attention map, Feature map이 될 수 는 있지만 궁극적으로는 Teacher Model의 layer와 Student Model의 layer를 매치시켜 최대한 유사하도록 학습을 진행\n\n※ Teacher Model의 hint layer와 Student Model의 guided layer를 어떻게 효율적으로 선택해야 하는지는 지속적인 연구가 필요\n\n\n\n\n\nmain idea 다른 층 간의 상관관계 혹은 데이터 샘플간의 상관관계를 탐색하여 이를 바탕으로 학습을 진행\nexample)\n1. feature map간의 데이터 상관관계를 탐색하고 Student model은 이를 사용해 상호간의 정보 흐름을 모방하며 학습\n2. 각기 다른 Teacher Model의 상관관계와 중요도를 2개의 그래프로 표현하는데 이는 logits과 feature를 사용함\n\n\n\nDistillation Loss of relation-based knowledge based on the relations of feature maps\n\\(L_{RelD}(f_{t}, f_{s})\\) = \\(L_{R^1}(\\psi_{t}(\\hat f_{t}, \\check f_{t}), \\psi_{t}(\\hat f_{s}, \\check f_{s}))\\)\n\\(where\\) \\(\\hat f_{t,s}, \\check f_{t,s}\\) 는 Teacher, Student Model로부터 선택된 feature map 쌍,\n\\(\\psi ()\\) 는 feature map 간의 유사도를 확인해주는 function,\n\\(L_{R^1}()\\) 는 Teacher, Student feature map간의 상관관계를 확인해주는 function\nDistillation Loss of relation-based knowledge based on the relations of instance relationship\n\\(L_{RelD}(f_{t}, f_{s})\\) = \\(L_{R^2}(\\psi_{t}(t_{i}, t_{j}), \\psi_{t}(s_{i}, s_{j}))\\)\n\\(where\\) \\((t_{i},t_{j}) \\in F_{t}, (s_{i},s_{j}) \\in F_{s}\\)\n\\(F_{t,s}\\) 는 Teacher, Student Model로부터의 sets of feature representation,\n\\(\\psi ()\\) 는 feature representation 간의 유사도를 확인해주는 function,\n\\(L_{R^2}()\\) 는 Teacher, Student feature map간의 상관관계를 확인해주는 function"
  },
  {
    "objectID": "docs/DL/posts/KnowledgeDistillation2.html#teacherstudent-architecture",
    "href": "docs/DL/posts/KnowledgeDistillation2.html#teacherstudent-architecture",
    "title": "Knowledge Distillation paper review part2",
    "section": "",
    "text": "structure of student model\n\nteacher network를 layer와 channel의 수를 줄임으로 간략하게 표현한 구조\n\n구조가 보존되는 teacher network의 quantized version\n양자화(quantized)란, 모델에 사용되는 파라미터를 특정 비트로 줄여서 모델의 크기를 줄여 inference time을 줄이는 효과를 냄\n\n효율적인 기본 과정을 낼 수 있는 small network\n\nsmall network with optimized global network structure ?\n\nteacher model과 같은 구조\n\nexample of T-S Architecture\n\ntraining gap을 줄이기 위해 보조 teacher model, residual learning 제안됨\n구조적 차이를 줄이기 위해 student model의 구조를 teacher 모델의 small or qunatized version으로 구축\n\n여러 개의 layer를 하나의 layer로 압축\nblock-wise knowledge transfer ?\n요즘은 Depth-wise separable convolution를 통해 모델 경량화를 진행"
  },
  {
    "objectID": "docs/DL/posts/Response_Based_Knowledge Practice.html",
    "href": "docs/DL/posts/Response_Based_Knowledge Practice.html",
    "title": "Response-based Knowledge Practice",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport time\n\n\n\n\nmnist_transform = transforms.Compose([\n    transforms.Resize(size = (28*28)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,),(1.0,))\n])\n\n\ntrain_data = datasets.FashionMNIST('./data', train=True, download=True,transform=transforms.ToTensor())\nvalid_data = datasets.FashionMNIST('./data', train=False, download=True,transform=transforms.ToTensor())\ntest_data = datasets.FashionMNIST('./data', train=False, download=True,transform=transforms.ToTensor())\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\nExtracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\nExtracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\nExtracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\n\n\n100%|██████████| 26421880/26421880 [00:00&lt;00:00, 116761645.70it/s]\n100%|██████████| 29515/29515 [00:00&lt;00:00, 6130887.61it/s]\n100%|██████████| 4422102/4422102 [00:00&lt;00:00, 63049467.35it/s]\n100%|██████████| 5148/5148 [00:00&lt;00:00, 21420909.71it/s]\n\n\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size= 64, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(dataset = valid_data, batch_size= 64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size= 64, shuffle=True)\n\n\nfor batch_idx, (x, target) in enumerate(train_loader):\n    plt.imshow(x[0,0])\n    # print(x.reshape(-1,28*28)[0], target[0])\n    break\n\n\n\n\n\n\n\n\nclass teacher_model(nn.Module):\n    def __init__(self):\n        super(teacher_model, self).__init__()\n        self.layer1 = nn.Linear(784,512)\n        self.layer2 = nn.Linear(512,256)\n        self.layer3 = nn.Linear(256,128)\n        self.layer4 = nn.Linear(128,64)\n        self.layer5 = nn.Linear(64,32)\n        self.layer6 = nn.Linear(32,10)\n\n    def forward(self, x):\n        x = x.view(-1,28*28)\n        output = F.relu(self.layer1(x))\n        output = F.relu(self.layer2(output))\n        output = F.relu(self.layer3(output))\n        output = F.relu(self.layer4(output))\n        output = F.relu(self.layer5(output))\n        output = self.layer6(output)\n        return output\n\n\nclass student_model(nn.Module):\n    def __init__(self):\n        super(student_model, self).__init__()\n        self.layer = nn.Linear(784,10)\n\n    def forward(self, x):\n        x = x.view(-1,28*28)\n        output = self.layer(x)\n        return output\n\n\n\n\n\ndef T_train(T_model, train_loader, valid_loader, epochs):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = 64\n    T_model = T_model.to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizr = optim.Adam(T_model.parameters(), lr=0.001)\n\n    T_train_loss = []\n    T_val_loss = []\n\n    for epch in tqdm(range(epochs+1)):\n        #t_epoch_start = time.time()\n        epch_loss = 0.0\n        val_epch_loss = 0.0\n\n        # 모델 학습\n        T_model.train()\n        for batch_idx, (x, target) in enumerate(train_loader):\n            x, target = x.to(device), target.to(device)\n            target = F.one_hot(target.to(torch.int64), num_classes = 10)\n\n            yhat = T_model(x)\n\n            loss = loss_fn(yhat.float(), target.float())\n\n            optimizr.zero_grad()\n\n            loss.requires_grad_(True)\n            loss.backward()\n\n            optimizr.step()\n\n            epch_loss += loss.item()\n\n        # 모델 검증\n        T_model.eval()\n        with torch.no_grad():\n            for batch_idx, (x,y) in enumerate(valid_loader):\n                x, y = x.to(device), y.to(device)\n                y = F.one_hot(y.to(torch.int64), num_classes = 10)\n                val_epch_loss += loss_fn(T_model(x).float(), y.float()).item()\n\n        #t_epoch_finish = time.time()\n        T_train_loss.append(epch_loss / batch_size)\n        T_val_loss.append(val_epch_loss / batch_size)\n        # if epch % 10 == 0:\n        #     print('Epoch: {}, Loss: {}, Epoch_time: {:.4f}'\n        #     .format(epch, epch_loss, t_epoch_finish - t_epoch_start))\n\n    return T_train_loss, T_val_loss\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\nT_model = teacher_model()\n\nT_tr_loss, T_vl_loss = T_train(T_model, train_loader,valid_loader, epochs = 50)\n\n\n\n\n\nplt.plot(np.arange(51), T_tr_loss)\nplt.plot(np.arange(51), T_vl_loss)\nplt.show()\n\n\n\n\n\ndef S_train(S_model, train_loader, valid_loader, epochs):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = 64\n    S_model = S_model.to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizr = optim.Adam(S_model.parameters(), lr=0.001)\n\n    S_train_loss = []\n    S_val_loss = []\n    for epch in tqdm(range(epochs+1)):\n        #t_epoch_start = time.time()\n        epch_loss = 0.0\n        val_epch_loss = 0.0\n\n        # 모델 학습\n        S_model.train()\n        for batch_idx, (x, target) in enumerate(train_loader):\n            x, target = x.to(device), target.to(device)\n            target = F.one_hot(target.to(torch.int64), num_classes = 10)\n\n            yhat = S_model(x)\n\n            loss = loss_fn(yhat.float(), target.float())\n\n            optimizr.zero_grad()\n\n            loss.requires_grad_(True)\n            loss.backward()\n\n            optimizr.step()\n\n            epch_loss += loss.item()\n\n        # 모델 검증\n        S_model.eval()\n        with torch.no_grad():\n            for batch_idx, (x,y) in enumerate(valid_loader):\n                x, y = x.to(device), y.to(device)\n                y = F.one_hot(y.to(torch.int64), num_classes = 10)\n                val_epch_loss += loss_fn(S_model(x).float(), y.float()).item()\n\n        #모델 결과 확인\n        # t_epoch_finish = time.time()\n        S_train_loss.append(epch_loss / batch_size)\n        S_val_loss.append(val_epch_loss / batch_size)\n        # if epch % 10 == 0:\n        #     print('Epoch: {}, Loss: {}, Epoch_time: {:.4f}'\n        #     .format(epch, epch_loss, t_epoch_finish - t_epoch_start))\n\n    return S_train_loss, S_val_loss\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\nS_model = student_model()\ntr_s_loss, vl_s_loss = S_train(S_model, train_loader, valid_loader, epochs = 50)\n\n\n\n\n\nplt.plot(np.arange(51), tr_s_loss)\nplt.plot(np.arange(51), vl_s_loss)\nplt.show()\n\n\n\n\n\n\n\ntorch.save(obj = T_model.state_dict(), f = 'teacher_model_parm.pth')\ntorch.save(obj = S_model.state_dict(), f = 'student_model_parm.pth')\n\n\n\n\n\nnew_T_model = teacher_model()\nnew_T_model.load_state_dict(torch.load('teacher_model_parm.pth'))\n\n&lt;All keys matched successfully&gt;\n\n\n\nnew_S_model = student_model()\nnew_S_model.load_state_dict(torch.load('student_model_parm.pth'))\n\n&lt;All keys matched successfully&gt;\n\n\n\n\n\n\n\nnew_T_model.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnew_T_model = new_T_model.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = new_T_model(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"Teacher model acc: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nTeacher model acc: 0.8862\n\n\n\nnew_S_model.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnew_S_model = new_S_model.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = new_S_model(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc without distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nstudent model acc without distillation: 0.8424\n\n\n\n\n\n\ndef Total_Loss_fn(student_pred, teacher_pred, alpha, T):\n    loss = F.kl_div(F.log_softmax(student_pred / T, dim=1), F.softmax(teacher_pred / T, dim=1), reduction='batchmean') * (T ** 2) * alpha + F.cross_entropy(student_pred, y) * (1 - alpha)\n    return loss\n\n\n\n\ntest_model = student_model()\ntest_model = test_model.to(device)\n\n\ny2_pred = test_model(x) # 엉망인 결과\n\n\nnew_S_model.to(device) # 어느정도 학습된 모델의 결과\ny1_pred = new_S_model(x)\n\n\nnew_T_model.to(device) # target\ny_target = new_T_model(x)\n\n\nTotal_Loss_fn(y2_pred,y_target, T=3,alpha=0.5)\n\ntensor(9.6223, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\n\n\n\ndef Distillation_train(T_model, S_model, train_loader, valid_loader, epochs, lr, T, alpha):\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = 64\n    T_model = T_model.to(device)\n    S_model = S_model.to(device)\n\n    optimizr = optim.Adam(S_model.parameters(), lr= lr)\n    loss_fn = nn.CrossEntropyLoss()\n    train_loss = []\n    val_loss = []\n    for epch in tqdm(range(epochs+1)):\n        #t_epoch_start = time.time()\n        epch_loss = 0.0\n        val_epch_loss = 0.0\n\n        # 모델 학습\n        T_model.train()\n        S_model.train()\n        for batch_idx, (x, target) in enumerate(train_loader):\n            x, target = x.to(device), target.to(device)\n            target = F.one_hot(target.to(torch.int64), num_classes = 10)\n\n            teacher_pred = T_model(x)\n            student_pred = S_model(x)\n\n            loss = F.kl_div(F.log_softmax(student_pred / T, dim=1), F.softmax(teacher_pred / T, dim=1), reduction='batchmean') * (T ** 2) * alpha + F.cross_entropy(student_pred, target.float()) * (1 - alpha)\n\n            optimizr.zero_grad()\n\n            loss.requires_grad_(True)\n            loss.backward()\n\n            optimizr.step()\n\n            epch_loss += loss.item()\n\n        # 모델 검증\n        S_model.eval()\n        with torch.no_grad():\n            for batch_idx, (x,y) in enumerate(valid_loader):\n                x, y = x.to(device), y.to(device)\n                y = F.one_hot(y.to(torch.int64), num_classes = 10)\n                val_epch_loss += loss_fn(S_model(x).float(), y.float()).item()\n\n        #모델 결과 확인\n        # t_epoch_finish = time.time()\n        train_loss.append(epch_loss / batch_size)\n        val_loss.append(val_epch_loss / batch_size)\n        # if epch % 10 == 0:\n        #     print('Epoch: {}, Loss: {}, Epoch_time: {:.4f}'\n        #     .format(epch, epch_loss, t_epoch_finish - t_epoch_start))\n\n    return train_loss, val_loss\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_2 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_2, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 2.0, alpha = 0.5)\n\n\n\n\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\n\n\n\n\ntest_model_2.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_2 = test_model_2.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_2(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nstudent model acc with distillation: 0.8452\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_3 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_3, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 3.0, alpha = 0.5)\n\n\n\n\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\n\n\n\n\ntest_model_3.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_3 = test_model_3.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_3(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nstudent model acc with distillation: 0.8384\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_5 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_5, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 5.0, alpha = 0.5)\n\n\n\n\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\n\n\n\n\ntest_model_5.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_5 = test_model_5.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_5(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nstudent model acc with distillation: 0.8243\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_7 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_7, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 7.0, alpha = 0.5)\n\n\n\n\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\n\n\n\n\ntest_model_7.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_7 = test_model_7.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_7(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nstudent model acc with distillation: 0.8129\n\n\n\nTemperture 바꾸기 (Distillation 없이 학습 진행하면 정확도는 84.24%)\n\n\n\n\nT value\nacc\n\\(\\alpha\\)\n\n\n\n\n2\n84.52\n0.5\n\n\n3\n83.84\n0.5\n\n\n5\n82.43\n0.5\n\n\n7\n81.29\n0.5\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_01 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_01, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 2.0, alpha = 0.1)\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\ntest_model_01.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_01 = test_model_01.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_01(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\n\n\n\n\n\n\nstudent model acc with distillation: 0.8455\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_03 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_03, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 2.0, alpha = 0.3)\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\ntest_model_03.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_03 = test_model_03.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_03(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\n\n\n\n\n\n\nstudent model acc with distillation: 0.8469\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_07 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_07, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 2.0, alpha = 0.7)\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\ntest_model_07.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_07 = test_model_07.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_07(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\n\n\n\n\n\n\nstudent model acc with distillation: 0.8426\n\n\n\n\\(\\alpha\\) 바꾸기 (Distillation 없이 학습 진행하면 정확도는 84.24%)\n\n\n\n\nT value\nacc\n\\(\\alpha\\)\n\n\n\n\n2\n84.55\n0.1\n\n\n2\n84.69\n0.3\n\n\n2\n84.52\n0.5\n\n\n2\n84.26\n0.7\n\n\n\n▶ T = 2, α = 0.3 일 때 Response-based Knowledge는 최적의 성능을 보임\nDistillation을 하지 않은 Student Model보다 0.5% 높은 정확도를 보임"
  },
  {
    "objectID": "docs/DL/posts/Response_Based_Knowledge Practice.html#데이터-준비",
    "href": "docs/DL/posts/Response_Based_Knowledge Practice.html#데이터-준비",
    "title": "Response-based Knowledge Practice",
    "section": "",
    "text": "mnist_transform = transforms.Compose([\n    transforms.Resize(size = (28*28)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,),(1.0,))\n])\n\n\ntrain_data = datasets.FashionMNIST('./data', train=True, download=True,transform=transforms.ToTensor())\nvalid_data = datasets.FashionMNIST('./data', train=False, download=True,transform=transforms.ToTensor())\ntest_data = datasets.FashionMNIST('./data', train=False, download=True,transform=transforms.ToTensor())\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\nExtracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\nExtracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\nExtracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\n\n\n100%|██████████| 26421880/26421880 [00:00&lt;00:00, 116761645.70it/s]\n100%|██████████| 29515/29515 [00:00&lt;00:00, 6130887.61it/s]\n100%|██████████| 4422102/4422102 [00:00&lt;00:00, 63049467.35it/s]\n100%|██████████| 5148/5148 [00:00&lt;00:00, 21420909.71it/s]\n\n\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size= 64, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(dataset = valid_data, batch_size= 64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size= 64, shuffle=True)\n\n\nfor batch_idx, (x, target) in enumerate(train_loader):\n    plt.imshow(x[0,0])\n    # print(x.reshape(-1,28*28)[0], target[0])\n    break"
  },
  {
    "objectID": "docs/DL/posts/Response_Based_Knowledge Practice.html#네트워크-준비",
    "href": "docs/DL/posts/Response_Based_Knowledge Practice.html#네트워크-준비",
    "title": "Response-based Knowledge Practice",
    "section": "",
    "text": "class teacher_model(nn.Module):\n    def __init__(self):\n        super(teacher_model, self).__init__()\n        self.layer1 = nn.Linear(784,512)\n        self.layer2 = nn.Linear(512,256)\n        self.layer3 = nn.Linear(256,128)\n        self.layer4 = nn.Linear(128,64)\n        self.layer5 = nn.Linear(64,32)\n        self.layer6 = nn.Linear(32,10)\n\n    def forward(self, x):\n        x = x.view(-1,28*28)\n        output = F.relu(self.layer1(x))\n        output = F.relu(self.layer2(output))\n        output = F.relu(self.layer3(output))\n        output = F.relu(self.layer4(output))\n        output = F.relu(self.layer5(output))\n        output = self.layer6(output)\n        return output\n\n\nclass student_model(nn.Module):\n    def __init__(self):\n        super(student_model, self).__init__()\n        self.layer = nn.Linear(784,10)\n\n    def forward(self, x):\n        x = x.view(-1,28*28)\n        output = self.layer(x)\n        return output"
  },
  {
    "objectID": "docs/DL/posts/Response_Based_Knowledge Practice.html#네트워크-학습",
    "href": "docs/DL/posts/Response_Based_Knowledge Practice.html#네트워크-학습",
    "title": "Response-based Knowledge Practice",
    "section": "",
    "text": "def T_train(T_model, train_loader, valid_loader, epochs):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = 64\n    T_model = T_model.to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizr = optim.Adam(T_model.parameters(), lr=0.001)\n\n    T_train_loss = []\n    T_val_loss = []\n\n    for epch in tqdm(range(epochs+1)):\n        #t_epoch_start = time.time()\n        epch_loss = 0.0\n        val_epch_loss = 0.0\n\n        # 모델 학습\n        T_model.train()\n        for batch_idx, (x, target) in enumerate(train_loader):\n            x, target = x.to(device), target.to(device)\n            target = F.one_hot(target.to(torch.int64), num_classes = 10)\n\n            yhat = T_model(x)\n\n            loss = loss_fn(yhat.float(), target.float())\n\n            optimizr.zero_grad()\n\n            loss.requires_grad_(True)\n            loss.backward()\n\n            optimizr.step()\n\n            epch_loss += loss.item()\n\n        # 모델 검증\n        T_model.eval()\n        with torch.no_grad():\n            for batch_idx, (x,y) in enumerate(valid_loader):\n                x, y = x.to(device), y.to(device)\n                y = F.one_hot(y.to(torch.int64), num_classes = 10)\n                val_epch_loss += loss_fn(T_model(x).float(), y.float()).item()\n\n        #t_epoch_finish = time.time()\n        T_train_loss.append(epch_loss / batch_size)\n        T_val_loss.append(val_epch_loss / batch_size)\n        # if epch % 10 == 0:\n        #     print('Epoch: {}, Loss: {}, Epoch_time: {:.4f}'\n        #     .format(epch, epch_loss, t_epoch_finish - t_epoch_start))\n\n    return T_train_loss, T_val_loss\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\nT_model = teacher_model()\n\nT_tr_loss, T_vl_loss = T_train(T_model, train_loader,valid_loader, epochs = 50)\n\n\n\n\n\nplt.plot(np.arange(51), T_tr_loss)\nplt.plot(np.arange(51), T_vl_loss)\nplt.show()\n\n\n\n\n\ndef S_train(S_model, train_loader, valid_loader, epochs):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = 64\n    S_model = S_model.to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizr = optim.Adam(S_model.parameters(), lr=0.001)\n\n    S_train_loss = []\n    S_val_loss = []\n    for epch in tqdm(range(epochs+1)):\n        #t_epoch_start = time.time()\n        epch_loss = 0.0\n        val_epch_loss = 0.0\n\n        # 모델 학습\n        S_model.train()\n        for batch_idx, (x, target) in enumerate(train_loader):\n            x, target = x.to(device), target.to(device)\n            target = F.one_hot(target.to(torch.int64), num_classes = 10)\n\n            yhat = S_model(x)\n\n            loss = loss_fn(yhat.float(), target.float())\n\n            optimizr.zero_grad()\n\n            loss.requires_grad_(True)\n            loss.backward()\n\n            optimizr.step()\n\n            epch_loss += loss.item()\n\n        # 모델 검증\n        S_model.eval()\n        with torch.no_grad():\n            for batch_idx, (x,y) in enumerate(valid_loader):\n                x, y = x.to(device), y.to(device)\n                y = F.one_hot(y.to(torch.int64), num_classes = 10)\n                val_epch_loss += loss_fn(S_model(x).float(), y.float()).item()\n\n        #모델 결과 확인\n        # t_epoch_finish = time.time()\n        S_train_loss.append(epch_loss / batch_size)\n        S_val_loss.append(val_epch_loss / batch_size)\n        # if epch % 10 == 0:\n        #     print('Epoch: {}, Loss: {}, Epoch_time: {:.4f}'\n        #     .format(epch, epch_loss, t_epoch_finish - t_epoch_start))\n\n    return S_train_loss, S_val_loss\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\nS_model = student_model()\ntr_s_loss, vl_s_loss = S_train(S_model, train_loader, valid_loader, epochs = 50)\n\n\n\n\n\nplt.plot(np.arange(51), tr_s_loss)\nplt.plot(np.arange(51), vl_s_loss)\nplt.show()\n\n\n\n\n\n\n\ntorch.save(obj = T_model.state_dict(), f = 'teacher_model_parm.pth')\ntorch.save(obj = S_model.state_dict(), f = 'student_model_parm.pth')\n\n\n\n\n\nnew_T_model = teacher_model()\nnew_T_model.load_state_dict(torch.load('teacher_model_parm.pth'))\n\n&lt;All keys matched successfully&gt;\n\n\n\nnew_S_model = student_model()\nnew_S_model.load_state_dict(torch.load('student_model_parm.pth'))\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "docs/DL/posts/Response_Based_Knowledge Practice.html#네트워크-검증",
    "href": "docs/DL/posts/Response_Based_Knowledge Practice.html#네트워크-검증",
    "title": "Response-based Knowledge Practice",
    "section": "",
    "text": "new_T_model.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnew_T_model = new_T_model.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = new_T_model(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"Teacher model acc: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nTeacher model acc: 0.8862\n\n\n\nnew_S_model.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnew_S_model = new_S_model.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = new_S_model(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc without distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nstudent model acc without distillation: 0.8424"
  },
  {
    "objectID": "docs/DL/posts/Response_Based_Knowledge Practice.html#total-loss-정의",
    "href": "docs/DL/posts/Response_Based_Knowledge Practice.html#total-loss-정의",
    "title": "Response-based Knowledge Practice",
    "section": "",
    "text": "def Total_Loss_fn(student_pred, teacher_pred, alpha, T):\n    loss = F.kl_div(F.log_softmax(student_pred / T, dim=1), F.softmax(teacher_pred / T, dim=1), reduction='batchmean') * (T ** 2) * alpha + F.cross_entropy(student_pred, y) * (1 - alpha)\n    return loss\n\n\n\n\ntest_model = student_model()\ntest_model = test_model.to(device)\n\n\ny2_pred = test_model(x) # 엉망인 결과\n\n\nnew_S_model.to(device) # 어느정도 학습된 모델의 결과\ny1_pred = new_S_model(x)\n\n\nnew_T_model.to(device) # target\ny_target = new_T_model(x)\n\n\nTotal_Loss_fn(y2_pred,y_target, T=3,alpha=0.5)\n\ntensor(9.6223, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)"
  },
  {
    "objectID": "docs/DL/posts/Response_Based_Knowledge Practice.html#knowledge-distillation으로-student-model-학습",
    "href": "docs/DL/posts/Response_Based_Knowledge Practice.html#knowledge-distillation으로-student-model-학습",
    "title": "Response-based Knowledge Practice",
    "section": "",
    "text": "def Distillation_train(T_model, S_model, train_loader, valid_loader, epochs, lr, T, alpha):\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = 64\n    T_model = T_model.to(device)\n    S_model = S_model.to(device)\n\n    optimizr = optim.Adam(S_model.parameters(), lr= lr)\n    loss_fn = nn.CrossEntropyLoss()\n    train_loss = []\n    val_loss = []\n    for epch in tqdm(range(epochs+1)):\n        #t_epoch_start = time.time()\n        epch_loss = 0.0\n        val_epch_loss = 0.0\n\n        # 모델 학습\n        T_model.train()\n        S_model.train()\n        for batch_idx, (x, target) in enumerate(train_loader):\n            x, target = x.to(device), target.to(device)\n            target = F.one_hot(target.to(torch.int64), num_classes = 10)\n\n            teacher_pred = T_model(x)\n            student_pred = S_model(x)\n\n            loss = F.kl_div(F.log_softmax(student_pred / T, dim=1), F.softmax(teacher_pred / T, dim=1), reduction='batchmean') * (T ** 2) * alpha + F.cross_entropy(student_pred, target.float()) * (1 - alpha)\n\n            optimizr.zero_grad()\n\n            loss.requires_grad_(True)\n            loss.backward()\n\n            optimizr.step()\n\n            epch_loss += loss.item()\n\n        # 모델 검증\n        S_model.eval()\n        with torch.no_grad():\n            for batch_idx, (x,y) in enumerate(valid_loader):\n                x, y = x.to(device), y.to(device)\n                y = F.one_hot(y.to(torch.int64), num_classes = 10)\n                val_epch_loss += loss_fn(S_model(x).float(), y.float()).item()\n\n        #모델 결과 확인\n        # t_epoch_finish = time.time()\n        train_loss.append(epch_loss / batch_size)\n        val_loss.append(val_epch_loss / batch_size)\n        # if epch % 10 == 0:\n        #     print('Epoch: {}, Loss: {}, Epoch_time: {:.4f}'\n        #     .format(epch, epch_loss, t_epoch_finish - t_epoch_start))\n\n    return train_loss, val_loss\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_2 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_2, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 2.0, alpha = 0.5)\n\n\n\n\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\n\n\n\n\ntest_model_2.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_2 = test_model_2.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_2(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nstudent model acc with distillation: 0.8452\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_3 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_3, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 3.0, alpha = 0.5)\n\n\n\n\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\n\n\n\n\ntest_model_3.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_3 = test_model_3.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_3(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nstudent model acc with distillation: 0.8384\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_5 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_5, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 5.0, alpha = 0.5)\n\n\n\n\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\n\n\n\n\ntest_model_5.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_5 = test_model_5.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_5(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nstudent model acc with distillation: 0.8243\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_7 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_7, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 7.0, alpha = 0.5)\n\n\n\n\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\n\n\n\n\ntest_model_7.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_7 = test_model_7.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_7(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nstudent model acc with distillation: 0.8129\n\n\n\nTemperture 바꾸기 (Distillation 없이 학습 진행하면 정확도는 84.24%)\n\n\n\n\nT value\nacc\n\\(\\alpha\\)\n\n\n\n\n2\n84.52\n0.5\n\n\n3\n83.84\n0.5\n\n\n5\n82.43\n0.5\n\n\n7\n81.29\n0.5\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_01 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_01, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 2.0, alpha = 0.1)\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\ntest_model_01.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_01 = test_model_01.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_01(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\n\n\n\n\n\n\nstudent model acc with distillation: 0.8455\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_03 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_03, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 2.0, alpha = 0.3)\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\ntest_model_03.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_03 = test_model_03.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_03(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\n\n\n\n\n\n\nstudent model acc with distillation: 0.8469\n\n\n\n\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntemp1 =[]\ntemp2 =[]\n\ntest_model_07 = student_model()\n\ntemp1, temp2 = Distillation_train(T_model=new_T_model, S_model=test_model_07, train_loader= train_loader, valid_loader= valid_loader, epochs = 50, lr = 0.001, T = 2.0, alpha = 0.7)\n\nplt.plot(np.arange(51), temp1)\nplt.plot(np.arange(51), temp2)\nplt.show()\n\ntest_model_07.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntest_model_07 = test_model_07.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = test_model_07(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\n\n\n\n\n\n\nstudent model acc with distillation: 0.8426\n\n\n\n\\(\\alpha\\) 바꾸기 (Distillation 없이 학습 진행하면 정확도는 84.24%)\n\n\n\n\nT value\nacc\n\\(\\alpha\\)\n\n\n\n\n2\n84.55\n0.1\n\n\n2\n84.69\n0.3\n\n\n2\n84.52\n0.5\n\n\n2\n84.26\n0.7\n\n\n\n▶ T = 2, α = 0.3 일 때 Response-based Knowledge는 최적의 성능을 보임\nDistillation을 하지 않은 Student Model보다 0.5% 높은 정확도를 보임"
  },
  {
    "objectID": "docs/DL/posts/Feature_Based_Knowledge Practice.html",
    "href": "docs/DL/posts/Feature_Based_Knowledge Practice.html",
    "title": "Feature-based Knowledge Practice",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nfrom torchvision import datasets, transforms\nimport torch.optim as optim\nfrom torchsummary import summary\n\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport os\nimport numpy as np\nimport time\n\n\ndef conv_dim(i,k,s,p):\n    '''\n    nn.Conv2d 사용할 때 사이즈 계산\n    i: input image size\n    k: kernel size\n    s: stride\n    p: padding\n    '''\n    out = (i - k + 2*p)/s + 1\n    return out\n\n\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n\ntrain_data = datasets.CIFAR10('./data', train=True, download=True,transform=transform_train)\ntest_data = datasets.CIFAR10('./data', train=False, download=True,transform=transform_test)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size= 128, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size= 100, shuffle=True)\n\n\nfor batch_idx, (x, target) in enumerate(train_loader):\n    plt.imshow(torch.einsum('cij -&gt; ijc',x[0]))\n    print(x.shape, target[0])\n    break\n\ntorch.Size([128, 3, 32, 32]) tensor(3)\n\n\n\n\n\n\n\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, planes, stride = 1):\n        super(BasicBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_planes, planes,kernel_size=3,padding=1, stride = stride, bias = False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        \n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,padding=1, stride = 1, bias = False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n\n        if stride != 1: # stride가 1이 아니면 image shape이 변형됨 # stride가 1이면 그냥 패스\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=3, padding=1,stride= stride, bias = False),\n                nn.BatchNorm2d(planes)\n                )\n\n    def forward(self, x):\n        output = F.relu(self.bn1(self.conv1(x)))\n        output = self.bn2(self.conv2(output))\n        # skip connection\n        output += self.shortcut(x)\n        output = F.relu(output)\n        return output\n\n    \nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes = 10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        \n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks - 1) # stride = 1 -&gt; strides = [1,1] # stride = 2 -&gt; strides = [2,1]\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        output = F.relu(self.bn1(self.conv1(x))) # 채널수만 바뀜\n        output = self.layer1(output) # \n        output = self.layer2(output)\n        output = self.layer3(output)\n        output = self.layer4(output)\n        output = F.avg_pool2d(output, 4) # img shape 1/4\n        output = output.view(output.size(0), -1) # FC Layer\n        output = self.linear(output) # img shape = [batch size, num_classes]\n        return output\n    \n    def get_features(self, x):\n        output = F.relu(self.bn1(self.conv1(x)))\n        output = self.layer1(output)\n        output = self.layer2(output)\n        output = self.layer3(output) \n        output = self.layer4(output)\n        output = F.avg_pool2d(output, 4) # img shape 1/4\n        output = output.view(output.size(0), -1) # FC Layer\n        return output\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2,2,2,2])\n\nnet_t = ResNet18()\nnet_t = net_t.to(device)\nsummary(net_t, input_size =(3,32,32), device = device.type)\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 32, 32]           1,728\n       BatchNorm2d-2           [-1, 64, 32, 32]             128\n            Conv2d-3           [-1, 64, 32, 32]          36,864\n       BatchNorm2d-4           [-1, 64, 32, 32]             128\n            Conv2d-5           [-1, 64, 32, 32]          36,864\n       BatchNorm2d-6           [-1, 64, 32, 32]             128\n        BasicBlock-7           [-1, 64, 32, 32]               0\n            Conv2d-8           [-1, 64, 32, 32]          36,864\n       BatchNorm2d-9           [-1, 64, 32, 32]             128\n           Conv2d-10           [-1, 64, 32, 32]          36,864\n      BatchNorm2d-11           [-1, 64, 32, 32]             128\n       BasicBlock-12           [-1, 64, 32, 32]               0\n           Conv2d-13          [-1, 128, 16, 16]          73,728\n      BatchNorm2d-14          [-1, 128, 16, 16]             256\n           Conv2d-15          [-1, 128, 16, 16]         147,456\n      BatchNorm2d-16          [-1, 128, 16, 16]             256\n           Conv2d-17          [-1, 128, 16, 16]          73,728\n      BatchNorm2d-18          [-1, 128, 16, 16]             256\n       BasicBlock-19          [-1, 128, 16, 16]               0\n           Conv2d-20          [-1, 128, 16, 16]         147,456\n      BatchNorm2d-21          [-1, 128, 16, 16]             256\n           Conv2d-22          [-1, 128, 16, 16]         147,456\n      BatchNorm2d-23          [-1, 128, 16, 16]             256\n       BasicBlock-24          [-1, 128, 16, 16]               0\n           Conv2d-25            [-1, 256, 8, 8]         294,912\n      BatchNorm2d-26            [-1, 256, 8, 8]             512\n           Conv2d-27            [-1, 256, 8, 8]         589,824\n      BatchNorm2d-28            [-1, 256, 8, 8]             512\n           Conv2d-29            [-1, 256, 8, 8]         294,912\n      BatchNorm2d-30            [-1, 256, 8, 8]             512\n       BasicBlock-31            [-1, 256, 8, 8]               0\n           Conv2d-32            [-1, 256, 8, 8]         589,824\n      BatchNorm2d-33            [-1, 256, 8, 8]             512\n           Conv2d-34            [-1, 256, 8, 8]         589,824\n      BatchNorm2d-35            [-1, 256, 8, 8]             512\n       BasicBlock-36            [-1, 256, 8, 8]               0\n           Conv2d-37            [-1, 512, 4, 4]       1,179,648\n      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n           Conv2d-39            [-1, 512, 4, 4]       2,359,296\n      BatchNorm2d-40            [-1, 512, 4, 4]           1,024\n           Conv2d-41            [-1, 512, 4, 4]       1,179,648\n      BatchNorm2d-42            [-1, 512, 4, 4]           1,024\n       BasicBlock-43            [-1, 512, 4, 4]               0\n           Conv2d-44            [-1, 512, 4, 4]       2,359,296\n      BatchNorm2d-45            [-1, 512, 4, 4]           1,024\n           Conv2d-46            [-1, 512, 4, 4]       2,359,296\n      BatchNorm2d-47            [-1, 512, 4, 4]           1,024\n       BasicBlock-48            [-1, 512, 4, 4]               0\n           Linear-49                   [-1, 10]           5,130\n================================================================\nTotal params: 12,550,218\nTrainable params: 12,550,218\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 11.25\nParams size (MB): 47.88\nEstimated Total Size (MB): 59.14\n----------------------------------------------------------------\n\n\n\nclass student_CNN(nn.Module):\n    def __init__(self):\n        super(student_CNN, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(32)\n            )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64)\n            )\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(128)\n            )\n        self.linear = nn.Linear(512, 10)\n\n\n    def forward(self, x):\n        output = F.relu(self.layer1(x))\n        output = F.relu(self.layer2(output))\n        output = F.relu(self.layer3(output))\n        output = F.avg_pool2d(output, 4)\n        output = output.view(output.size(0),-1)\n        output = self.linear(output)\n        return output\n    \n    def get_features(self, x):\n        output = F.relu(self.layer1(x))\n        output = F.relu(self.layer2(output))\n        output = F.relu(self.layer3(output))\n        output = F.avg_pool2d(output, 4)\n        output = output.view(output.size(0),-1)\n        return output\n\n\nnet_s =student_CNN()\nnet_s = net_s.to(device)\nsummary(net_s, (3,32,32))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 16, 16]             864\n       BatchNorm2d-2           [-1, 32, 16, 16]              64\n            Conv2d-3             [-1, 64, 8, 8]          18,432\n       BatchNorm2d-4             [-1, 64, 8, 8]             128\n            Conv2d-5            [-1, 128, 8, 8]          73,728\n       BatchNorm2d-6            [-1, 128, 8, 8]             256\n            Linear-7                   [-1, 10]           5,130\n================================================================\nTotal params: 98,602\nTrainable params: 98,602\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.31\nParams size (MB): 0.38\nEstimated Total Size (MB): 0.70\n----------------------------------------------------------------\n\n\n\n\nprint(net_t(torch.rand(128,3,32,32).to(device)).shape == net_s(torch.rand(128,3,32,32).to(device)).shape)\n\nTrue\n\n\n\nnetwork의 출력 shape이 같은 것을 확인\n\nprint(net_t.get_features(torch.rand(128,3,32,32).to(device)).shape == net_s.get_features(torch.rand(128,3,32,32).to(device)).shape)\n\nTrue\n\n\nnetwork의 feature map shape이 같은 것을 확인\n\n\n\n\n\n\n\ndef T_train(T_model, train_loader, valid_loader, epochs):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = 128\n    T_model = T_model.to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizr = optim.Adam(T_model.parameters(), lr=0.001)\n\n    T_train_loss = []\n    T_val_loss = []\n\n    for epch in range(epochs+1):\n        t_epoch_start = time.time()\n        epch_loss = 0.0\n        val_epch_loss = 0.0\n\n        # 모델 학습\n        T_model.train()\n        for batch_idx, (x, target) in tqdm(enumerate(train_loader)):\n            x, target = x.to(device), target.to(device)\n            target = F.one_hot(target.to(torch.int64), num_classes = 10)\n\n            yhat = T_model(x)\n\n            loss = loss_fn(yhat.float(), target.float())\n\n            optimizr.zero_grad()\n\n            loss.requires_grad_(True)\n            loss.backward()\n\n            optimizr.step()\n\n            epch_loss += loss.item()\n\n        # 모델 검증\n        T_model.eval()\n        with torch.no_grad():\n            for batch_idx, (x,y) in enumerate(valid_loader):\n                x, y = x.to(device), y.to(device)\n                y = F.one_hot(y.to(torch.int64), num_classes = 10)\n                val_epch_loss += loss_fn(T_model(x).float(), y.float()).item()\n\n        t_epoch_finish = time.time()\n        \n        T_train_loss.append(epch_loss / batch_size)\n        T_val_loss.append(val_epch_loss / batch_size)\n        \n        if epch % 5 == 0:\n            print('Epoch: {}, Loss: {}, Epoch_time: {:.4f}'\n            .format(epch, epch_loss, t_epoch_finish - t_epoch_start))\n\n    return T_train_loss, T_val_loss\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\nnet_t = ResNet18()\n\nT_tr_loss, T_vl_loss = T_train(net_t, train_loader,test_loader, epochs = 50)\n\n\n\n\nEpoch: 0, Loss: 576.4184775352478, Epoch_time: 39.6003\nEpoch: 5, Loss: 184.98892831802368, Epoch_time: 39.7006\nEpoch: 10, Loss: 114.72835922241211, Epoch_time: 38.6154\nEpoch: 15, Loss: 75.4183604940772, Epoch_time: 39.8180\nEpoch: 20, Loss: 52.922188844531775, Epoch_time: 39.6666\nEpoch: 25, Loss: 37.530922940932214, Epoch_time: 39.7999\nEpoch: 30, Loss: 27.328608073294163, Epoch_time: 39.8334\nEpoch: 35, Loss: 20.078745015896857, Epoch_time: 39.7130\nEpoch: 40, Loss: 17.854032406117767, Epoch_time: 39.8166\nEpoch: 45, Loss: 15.429646169301122, Epoch_time: 39.6000\nEpoch: 50, Loss: 13.586039058631286, Epoch_time: 39.7558\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot(np.arange(51), T_tr_loss)\nplt.plot(np.arange(51), T_vl_loss)\nplt.show()\n\n\n\n\n\n\n\n\ndef S_train(S_model, train_loader, valid_loader, epochs):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = 64\n    S_model = S_model.to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizr = optim.Adam(S_model.parameters(), lr=0.001)\n\n    S_train_loss = []\n    S_val_loss = []\n    for epch in range(epochs+1):\n        t_epoch_start = time.time()\n        \n        epch_loss = 0.0\n        val_epch_loss = 0.0\n\n        # 모델 학습\n        S_model.train()\n        for batch_idx, (x, target) in tqdm(enumerate(train_loader)):\n            x, target = x.to(device), target.to(device)\n            target = F.one_hot(target.to(torch.int64), num_classes = 10)\n\n            yhat = S_model(x)\n\n            loss = loss_fn(yhat.float(), target.float())\n\n            optimizr.zero_grad()\n\n            loss.requires_grad_(True)\n            loss.backward()\n\n            optimizr.step()\n\n            epch_loss += loss.item()\n\n        # 모델 검증\n        S_model.eval()\n        with torch.no_grad():\n            for batch_idx, (x,y) in enumerate(valid_loader):\n                x, y = x.to(device), y.to(device)\n                y = F.one_hot(y.to(torch.int64), num_classes = 10)\n                val_epch_loss += loss_fn(S_model(x).float(), y.float()).item()\n\n        #모델 결과 확인\n        t_epoch_finish = time.time()\n        S_train_loss.append(epch_loss / batch_size)\n        S_val_loss.append(val_epch_loss / batch_size)\n        if epch % 10 == 0:\n            print('Epoch: {}, Loss: {}, Epoch_time: {:.4f}'\n            .format(epch, epch_loss, t_epoch_finish - t_epoch_start))\n\n    return S_train_loss, S_val_loss\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\nnet_s = student_CNN()\ntr_s_loss, vl_s_loss = S_train(net_s, train_loader, test_loader, epochs = 50)\n\n\n\n\nEpoch: 0, Loss: 595.5317752361298, Epoch_time: 14.2222\nEpoch: 10, Loss: 308.24509543180466, Epoch_time: 14.7486\nEpoch: 20, Loss: 258.0080524981022, Epoch_time: 15.1596\nEpoch: 30, Loss: 231.55767691135406, Epoch_time: 13.9361\nEpoch: 40, Loss: 212.09620788693428, Epoch_time: 15.3639\nEpoch: 50, Loss: 198.52148813009262, Epoch_time: 15.3310\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot(np.arange(51), tr_s_loss)\nplt.plot(np.arange(51), vl_s_loss)\nplt.show()\n\n\n\n\n\n\n\n\ntorch.save(obj = net_t.state_dict(), f = 'teacher_CNN_parm.pth')\ntorch.save(obj = net_s.state_dict(), f = 'student_CNN_parm.pth')\n\n\n\n\n\nnew_T_model = ResNet18()\nnew_T_model.load_state_dict(torch.load('teacher_CNN_parm.pth'))\n\n&lt;All keys matched successfully&gt;\n\n\n\nnew_S_model = student_CNN()\nnew_S_model.load_state_dict(torch.load('student_CNN_parm.pth'))\n\n&lt;All keys matched successfully&gt;\n\n\n\nnew_T_model.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnew_T_model = new_T_model.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = new_T_model(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"Teacher model acc: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nTeacher model acc: 0.9152\n\n\n\nnew_S_model.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnew_S_model = new_S_model.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(test_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = new_S_model(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc without distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nstudent model acc without distillation: 0.7611"
  },
  {
    "objectID": "docs/DL/posts/Feature_Based_Knowledge Practice.html#데이터-준비",
    "href": "docs/DL/posts/Feature_Based_Knowledge Practice.html#데이터-준비",
    "title": "Feature-based Knowledge Practice",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nfrom torchvision import datasets, transforms\nimport torch.optim as optim\nfrom torchsummary import summary\n\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport os\nimport numpy as np\nimport time\n\n\ndef conv_dim(i,k,s,p):\n    '''\n    nn.Conv2d 사용할 때 사이즈 계산\n    i: input image size\n    k: kernel size\n    s: stride\n    p: padding\n    '''\n    out = (i - k + 2*p)/s + 1\n    return out\n\n\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n\ntrain_data = datasets.CIFAR10('./data', train=True, download=True,transform=transform_train)\ntest_data = datasets.CIFAR10('./data', train=False, download=True,transform=transform_test)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size= 128, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size= 100, shuffle=True)\n\n\nfor batch_idx, (x, target) in enumerate(train_loader):\n    plt.imshow(torch.einsum('cij -&gt; ijc',x[0]))\n    print(x.shape, target[0])\n    break\n\ntorch.Size([128, 3, 32, 32]) tensor(3)"
  },
  {
    "objectID": "docs/DL/posts/Feature_Based_Knowledge Practice.html#networkresnet-cnn-정의",
    "href": "docs/DL/posts/Feature_Based_Knowledge Practice.html#networkresnet-cnn-정의",
    "title": "Feature-based Knowledge Practice",
    "section": "",
    "text": "class BasicBlock(nn.Module):\n    def __init__(self, in_planes, planes, stride = 1):\n        super(BasicBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_planes, planes,kernel_size=3,padding=1, stride = stride, bias = False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        \n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,padding=1, stride = 1, bias = False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n\n        if stride != 1: # stride가 1이 아니면 image shape이 변형됨 # stride가 1이면 그냥 패스\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=3, padding=1,stride= stride, bias = False),\n                nn.BatchNorm2d(planes)\n                )\n\n    def forward(self, x):\n        output = F.relu(self.bn1(self.conv1(x)))\n        output = self.bn2(self.conv2(output))\n        # skip connection\n        output += self.shortcut(x)\n        output = F.relu(output)\n        return output\n\n    \nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes = 10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        \n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks - 1) # stride = 1 -&gt; strides = [1,1] # stride = 2 -&gt; strides = [2,1]\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        output = F.relu(self.bn1(self.conv1(x))) # 채널수만 바뀜\n        output = self.layer1(output) # \n        output = self.layer2(output)\n        output = self.layer3(output)\n        output = self.layer4(output)\n        output = F.avg_pool2d(output, 4) # img shape 1/4\n        output = output.view(output.size(0), -1) # FC Layer\n        output = self.linear(output) # img shape = [batch size, num_classes]\n        return output\n    \n    def get_features(self, x):\n        output = F.relu(self.bn1(self.conv1(x)))\n        output = self.layer1(output)\n        output = self.layer2(output)\n        output = self.layer3(output) \n        output = self.layer4(output)\n        output = F.avg_pool2d(output, 4) # img shape 1/4\n        output = output.view(output.size(0), -1) # FC Layer\n        return output\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2,2,2,2])\n\nnet_t = ResNet18()\nnet_t = net_t.to(device)\nsummary(net_t, input_size =(3,32,32), device = device.type)\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 32, 32]           1,728\n       BatchNorm2d-2           [-1, 64, 32, 32]             128\n            Conv2d-3           [-1, 64, 32, 32]          36,864\n       BatchNorm2d-4           [-1, 64, 32, 32]             128\n            Conv2d-5           [-1, 64, 32, 32]          36,864\n       BatchNorm2d-6           [-1, 64, 32, 32]             128\n        BasicBlock-7           [-1, 64, 32, 32]               0\n            Conv2d-8           [-1, 64, 32, 32]          36,864\n       BatchNorm2d-9           [-1, 64, 32, 32]             128\n           Conv2d-10           [-1, 64, 32, 32]          36,864\n      BatchNorm2d-11           [-1, 64, 32, 32]             128\n       BasicBlock-12           [-1, 64, 32, 32]               0\n           Conv2d-13          [-1, 128, 16, 16]          73,728\n      BatchNorm2d-14          [-1, 128, 16, 16]             256\n           Conv2d-15          [-1, 128, 16, 16]         147,456\n      BatchNorm2d-16          [-1, 128, 16, 16]             256\n           Conv2d-17          [-1, 128, 16, 16]          73,728\n      BatchNorm2d-18          [-1, 128, 16, 16]             256\n       BasicBlock-19          [-1, 128, 16, 16]               0\n           Conv2d-20          [-1, 128, 16, 16]         147,456\n      BatchNorm2d-21          [-1, 128, 16, 16]             256\n           Conv2d-22          [-1, 128, 16, 16]         147,456\n      BatchNorm2d-23          [-1, 128, 16, 16]             256\n       BasicBlock-24          [-1, 128, 16, 16]               0\n           Conv2d-25            [-1, 256, 8, 8]         294,912\n      BatchNorm2d-26            [-1, 256, 8, 8]             512\n           Conv2d-27            [-1, 256, 8, 8]         589,824\n      BatchNorm2d-28            [-1, 256, 8, 8]             512\n           Conv2d-29            [-1, 256, 8, 8]         294,912\n      BatchNorm2d-30            [-1, 256, 8, 8]             512\n       BasicBlock-31            [-1, 256, 8, 8]               0\n           Conv2d-32            [-1, 256, 8, 8]         589,824\n      BatchNorm2d-33            [-1, 256, 8, 8]             512\n           Conv2d-34            [-1, 256, 8, 8]         589,824\n      BatchNorm2d-35            [-1, 256, 8, 8]             512\n       BasicBlock-36            [-1, 256, 8, 8]               0\n           Conv2d-37            [-1, 512, 4, 4]       1,179,648\n      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n           Conv2d-39            [-1, 512, 4, 4]       2,359,296\n      BatchNorm2d-40            [-1, 512, 4, 4]           1,024\n           Conv2d-41            [-1, 512, 4, 4]       1,179,648\n      BatchNorm2d-42            [-1, 512, 4, 4]           1,024\n       BasicBlock-43            [-1, 512, 4, 4]               0\n           Conv2d-44            [-1, 512, 4, 4]       2,359,296\n      BatchNorm2d-45            [-1, 512, 4, 4]           1,024\n           Conv2d-46            [-1, 512, 4, 4]       2,359,296\n      BatchNorm2d-47            [-1, 512, 4, 4]           1,024\n       BasicBlock-48            [-1, 512, 4, 4]               0\n           Linear-49                   [-1, 10]           5,130\n================================================================\nTotal params: 12,550,218\nTrainable params: 12,550,218\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 11.25\nParams size (MB): 47.88\nEstimated Total Size (MB): 59.14\n----------------------------------------------------------------\n\n\n\nclass student_CNN(nn.Module):\n    def __init__(self):\n        super(student_CNN, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(32)\n            )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64)\n            )\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(128)\n            )\n        self.linear = nn.Linear(512, 10)\n\n\n    def forward(self, x):\n        output = F.relu(self.layer1(x))\n        output = F.relu(self.layer2(output))\n        output = F.relu(self.layer3(output))\n        output = F.avg_pool2d(output, 4)\n        output = output.view(output.size(0),-1)\n        output = self.linear(output)\n        return output\n    \n    def get_features(self, x):\n        output = F.relu(self.layer1(x))\n        output = F.relu(self.layer2(output))\n        output = F.relu(self.layer3(output))\n        output = F.avg_pool2d(output, 4)\n        output = output.view(output.size(0),-1)\n        return output\n\n\nnet_s =student_CNN()\nnet_s = net_s.to(device)\nsummary(net_s, (3,32,32))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 16, 16]             864\n       BatchNorm2d-2           [-1, 32, 16, 16]              64\n            Conv2d-3             [-1, 64, 8, 8]          18,432\n       BatchNorm2d-4             [-1, 64, 8, 8]             128\n            Conv2d-5            [-1, 128, 8, 8]          73,728\n       BatchNorm2d-6            [-1, 128, 8, 8]             256\n            Linear-7                   [-1, 10]           5,130\n================================================================\nTotal params: 98,602\nTrainable params: 98,602\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.31\nParams size (MB): 0.38\nEstimated Total Size (MB): 0.70\n----------------------------------------------------------------\n\n\n\n\nprint(net_t(torch.rand(128,3,32,32).to(device)).shape == net_s(torch.rand(128,3,32,32).to(device)).shape)\n\nTrue\n\n\n\nnetwork의 출력 shape이 같은 것을 확인\n\nprint(net_t.get_features(torch.rand(128,3,32,32).to(device)).shape == net_s.get_features(torch.rand(128,3,32,32).to(device)).shape)\n\nTrue\n\n\nnetwork의 feature map shape이 같은 것을 확인"
  },
  {
    "objectID": "docs/DL/posts/Feature_Based_Knowledge Practice.html#모델학습",
    "href": "docs/DL/posts/Feature_Based_Knowledge Practice.html#모델학습",
    "title": "Feature-based Knowledge Practice",
    "section": "",
    "text": "def T_train(T_model, train_loader, valid_loader, epochs):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = 128\n    T_model = T_model.to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizr = optim.Adam(T_model.parameters(), lr=0.001)\n\n    T_train_loss = []\n    T_val_loss = []\n\n    for epch in range(epochs+1):\n        t_epoch_start = time.time()\n        epch_loss = 0.0\n        val_epch_loss = 0.0\n\n        # 모델 학습\n        T_model.train()\n        for batch_idx, (x, target) in tqdm(enumerate(train_loader)):\n            x, target = x.to(device), target.to(device)\n            target = F.one_hot(target.to(torch.int64), num_classes = 10)\n\n            yhat = T_model(x)\n\n            loss = loss_fn(yhat.float(), target.float())\n\n            optimizr.zero_grad()\n\n            loss.requires_grad_(True)\n            loss.backward()\n\n            optimizr.step()\n\n            epch_loss += loss.item()\n\n        # 모델 검증\n        T_model.eval()\n        with torch.no_grad():\n            for batch_idx, (x,y) in enumerate(valid_loader):\n                x, y = x.to(device), y.to(device)\n                y = F.one_hot(y.to(torch.int64), num_classes = 10)\n                val_epch_loss += loss_fn(T_model(x).float(), y.float()).item()\n\n        t_epoch_finish = time.time()\n        \n        T_train_loss.append(epch_loss / batch_size)\n        T_val_loss.append(val_epch_loss / batch_size)\n        \n        if epch % 5 == 0:\n            print('Epoch: {}, Loss: {}, Epoch_time: {:.4f}'\n            .format(epch, epch_loss, t_epoch_finish - t_epoch_start))\n\n    return T_train_loss, T_val_loss\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\nnet_t = ResNet18()\n\nT_tr_loss, T_vl_loss = T_train(net_t, train_loader,test_loader, epochs = 50)\n\n\n\n\nEpoch: 0, Loss: 576.4184775352478, Epoch_time: 39.6003\nEpoch: 5, Loss: 184.98892831802368, Epoch_time: 39.7006\nEpoch: 10, Loss: 114.72835922241211, Epoch_time: 38.6154\nEpoch: 15, Loss: 75.4183604940772, Epoch_time: 39.8180\nEpoch: 20, Loss: 52.922188844531775, Epoch_time: 39.6666\nEpoch: 25, Loss: 37.530922940932214, Epoch_time: 39.7999\nEpoch: 30, Loss: 27.328608073294163, Epoch_time: 39.8334\nEpoch: 35, Loss: 20.078745015896857, Epoch_time: 39.7130\nEpoch: 40, Loss: 17.854032406117767, Epoch_time: 39.8166\nEpoch: 45, Loss: 15.429646169301122, Epoch_time: 39.6000\nEpoch: 50, Loss: 13.586039058631286, Epoch_time: 39.7558\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot(np.arange(51), T_tr_loss)\nplt.plot(np.arange(51), T_vl_loss)\nplt.show()\n\n\n\n\n\n\n\n\ndef S_train(S_model, train_loader, valid_loader, epochs):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = 64\n    S_model = S_model.to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizr = optim.Adam(S_model.parameters(), lr=0.001)\n\n    S_train_loss = []\n    S_val_loss = []\n    for epch in range(epochs+1):\n        t_epoch_start = time.time()\n        \n        epch_loss = 0.0\n        val_epch_loss = 0.0\n\n        # 모델 학습\n        S_model.train()\n        for batch_idx, (x, target) in tqdm(enumerate(train_loader)):\n            x, target = x.to(device), target.to(device)\n            target = F.one_hot(target.to(torch.int64), num_classes = 10)\n\n            yhat = S_model(x)\n\n            loss = loss_fn(yhat.float(), target.float())\n\n            optimizr.zero_grad()\n\n            loss.requires_grad_(True)\n            loss.backward()\n\n            optimizr.step()\n\n            epch_loss += loss.item()\n\n        # 모델 검증\n        S_model.eval()\n        with torch.no_grad():\n            for batch_idx, (x,y) in enumerate(valid_loader):\n                x, y = x.to(device), y.to(device)\n                y = F.one_hot(y.to(torch.int64), num_classes = 10)\n                val_epch_loss += loss_fn(S_model(x).float(), y.float()).item()\n\n        #모델 결과 확인\n        t_epoch_finish = time.time()\n        S_train_loss.append(epch_loss / batch_size)\n        S_val_loss.append(val_epch_loss / batch_size)\n        if epch % 10 == 0:\n            print('Epoch: {}, Loss: {}, Epoch_time: {:.4f}'\n            .format(epch, epch_loss, t_epoch_finish - t_epoch_start))\n\n    return S_train_loss, S_val_loss\n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\nnet_s = student_CNN()\ntr_s_loss, vl_s_loss = S_train(net_s, train_loader, test_loader, epochs = 50)\n\n\n\n\nEpoch: 0, Loss: 595.5317752361298, Epoch_time: 14.2222\nEpoch: 10, Loss: 308.24509543180466, Epoch_time: 14.7486\nEpoch: 20, Loss: 258.0080524981022, Epoch_time: 15.1596\nEpoch: 30, Loss: 231.55767691135406, Epoch_time: 13.9361\nEpoch: 40, Loss: 212.09620788693428, Epoch_time: 15.3639\nEpoch: 50, Loss: 198.52148813009262, Epoch_time: 15.3310\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot(np.arange(51), tr_s_loss)\nplt.plot(np.arange(51), vl_s_loss)\nplt.show()\n\n\n\n\n\n\n\n\ntorch.save(obj = net_t.state_dict(), f = 'teacher_CNN_parm.pth')\ntorch.save(obj = net_s.state_dict(), f = 'student_CNN_parm.pth')\n\n\n\n\n\nnew_T_model = ResNet18()\nnew_T_model.load_state_dict(torch.load('teacher_CNN_parm.pth'))\n\n&lt;All keys matched successfully&gt;\n\n\n\nnew_S_model = student_CNN()\nnew_S_model.load_state_dict(torch.load('student_CNN_parm.pth'))\n\n&lt;All keys matched successfully&gt;\n\n\n\nnew_T_model.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnew_T_model = new_T_model.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(valid_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = new_T_model(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"Teacher model acc: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nTeacher model acc: 0.9152\n\n\n\nnew_S_model.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnew_S_model = new_S_model.to(device)\nT_acc = []\nfor batch_idx, (x,y) in enumerate(test_loader):\n    x, y = x.to(device), y.to(device)\n    yhat = new_S_model(x)\n    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\nprint(\"student model acc without distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))\n\nstudent model acc without distillation: 0.7611"
  },
  {
    "objectID": "docs/BME/posts/ultrasound1.html",
    "href": "docs/BME/posts/ultrasound1.html",
    "title": "초음파 물리",
    "section": "",
    "text": "초음파에 적용되는 물리 법칙에 대해 간단히 설명하는 글\n※ 초음파 진단기에서 사용되는 초음파가 인체내를 전파되어 갈 경우 액체 속을 전파하는 초음파로 해석하므로 앞으로의 초음파에 대한 글에서는 액체 속의 음파 만을 다룸\n\n\n주파수에 따라 음파는 가청주파수와 초음파로 나뉜다. 가청주파수의 주파수는 20 ~ 20kHz이고 초음파는 20kHz보다 높은 주파수를 의미한다. 초음파 진단기에서는 주로 2 ~ 20MHz를 사용한다.\n음파의 전달 현상: 매질 내의 압력의 변화가 입자의 속도를 변화시키고, 입자의 속도 변화가 매질 내의 새로운 압력 변화를 만들어 내는 과정\n\n\n\n\n\n주파수(frequency, \\(f, Hz\\)): 1초에 몇 회 진동하는지를 나타내는 값\n주기(period, \\(T, s\\)): 한 번 진동하는 동안 걸리는 시간\n\\(f = \\frac{1}{T}[Hz], T = \\frac{1}{f}[s]\\)\n초음파의 전달속도(\\(c, m/s\\)): 초음파가 1초에 몇 m를 퍼져나가는지를 나타내는 값\n초음파의 파장(\\(\\lambda, m\\)): 음파의 인접한 골 사이의 거리를 의미\n\\(\\lambda = c \\cdot T = \\frac{c}{f}[m]\\)\n액체 내에서의 초음파 전달속도는 아래와 같이 근사적으로 나타낼 수 있음\n\\(c = \\sqrt{\\frac{B}{\\rho}}[m/s]\\)\n여기서 \\(B[N/m^2]\\) 는 Bulk Modulus로 매질의 경도(stiffness = incompressibility)를 나타낸다. B의 값이 크면 물질이 딱딱하고 B의 값이 작으면 물질이 물렁하다.\n\\(\\rho [kg/m^3]\\) 는 매질의 밀도를 나타낸다.\n\n\n\n매질에 정현파(sin wave)의 압력을 계속 가해 주면 입자의 속도가 정현파가 된다.\n음향 임피던스(Acoustic Impedence, \\(Z, Rayl = kg/m^2/s\\)): 같은 크기의 정현파 압력을 가해 주어도 매질의 특성에 따라서 입자의 진동의 크기가 달라질 수 있다. 여기서 매질의 특성을 나타내는 것이 음향 임피던스이다.\n\\(Z = \\rho \\cdot c = P/v = \\sqrt{\\rho \\cdot B}\\) \\([kg/m^2/s]\\)\n음압(Sound Pressure): 음파의 진폭으로 압력, 입자의 속도 등으로 표현할 수 있다. 정현파의 경우 첨두치(peak) 압력을 음압으로 본다.\n강도(Intensity): 단위면적을 통과하는 파워로 아래와 같이 나타낼 수 있다.\n\\(I = VP = \\frac{P^2}{Z} = V^2 Z\\)\n\\(V\\)는 입자의 속력, \\(P\\)는 음압, \\(Z\\)는 음향 임피던스를 의미한다.\n단위면적을 통과하는 파워이므로 골고루 퍼져 넓은 면적을 통과한다면 강도는 약해지게 된다.\n\n\n\n전압은 음압, 전류는 음향의 입자 속력, 임피던스는 음향 임피던스, 전력은 음향의 강도에 비유할 수 있다.\n전기회로(교류): \\(V = IZ\\)\n음향: \\(P = UZ\\), \\(P\\)는 음압, \\(U\\)는 매질 입자의 속도, \\(Z\\)는 음향 임피던스를 의미\n전기회로: \\(P\\)(전력, \\(W\\)) = \\(VI\\)\n음향: \\(J\\)(강도, \\(W/m^2\\)) = \\(\\frac{UP}{2}\\)\n\\(\\frac{1}{2}\\) 계수는 음향에서는 P와 U는 첨두치, J는 실효치를 사용했지만 회로에서는 실효치만 사용했기 떄문\n\n\n\n\n다른 성질을 갖는 매질이 서로 접해 있는 경계면에 초음파가 도달하면 반사, 투과, 굴절 등의 현상이 생김\n\n\n\n위의 사진을 통해 초음파의 반사(reflection)와 투과(transmission)를 확인할 수 있다.\n매질이나 경계면에서 에너지의 손실이 없으면 반사되는 음압\\(P_r\\)은 입사된 음압\\(P_i\\)에 대해 아래와 같은 관계를 갖게 됨\n\\(P_r = R \\cdot P_i\\)\n여기서 R은 반사계수로 아래와 같이 표현할 수 있다\n\\(R = \\frac{P_r}{P_i} = \\frac{Z_2 - Z_1}{Z_2 + Z_1}\\) (만약 R &lt; 0이라면 반사파의 위상이 반전됨)\n에너지 보존법칙에 따라 \\(I_i = I_r + I_t\\) 와 같다.\n\\(1 = R^2 + T^2 \\frac{Z_1}{Z_2}\\) \\(\\to\\) \\(T = \\frac{2Z_2}{Z_1 + Z_2} = 1+R\\)\n투과파의 음압은 \\(P_t = T \\cdot P_i\\) 이다.\n\n\n\n굴절(reflection): 초음파가 경계면에 수직으로 입사하지 않는 경우, 입사각과 투과각이 달라지는 현상\n\n스넬의 법칙: \\(\\frac{\\sin{\\theta_1}}{c_1} = \\frac{\\sin{\\theta_2}}{c_2}\\) (\\(\\theta_1\\)은 입사각(반사각), \\(\\theta_2\\)는 굴절각을 의미)\n입사각과 굴절각의 관계는 각각의 매질의 음속에만 직접적으로 관련이 있으며 밀도, 음향 임피던스는 직접적으로 관련이 있지 않음을 의미\n만약 굴절각(\\(\\theta_2\\))가 90\\(\\degree\\)가 넘게 되면 반사파만이 존재하게 되어 전반사가 일어남을 의미\n\n\n\n산란(scattering): 평면 초음파가 만난 경계면이 파장에 비해 길이가 작은 면을 만나면 반사파는 구면파에 가깝게 되어 넓게 퍼져나가게 되는 현상\n산란체(scatter): 위와 같은 경계면을 만든 작은 매질\n레일리 산란(Rayleigh Scattering): 산란체의 크기가 파장에 비해 상당히 작은 경우에 발생하는 산란\n\n\n\n감쇠(attenuation): 매질의 에너지 손실(음향 에너지 \\(\\to\\) 주로 열 에너지) 및 산란으로 인해 음압이 줄어드는 현상\n초음파 공학에서는 감쇠가 주파수에 비례한다고 근사화하여 감쇠계수(\\(\\alpha, dB/(cm \\cdot MHz)\\)) 를 이와 같이 표기한다.\n일반적인 공학에서는 감쇠계수의 단위는 \\(dB/cm\\)이다\n\n\n감쇠계수에 사용되는 \\(dB\\)라는 단위에 대해서 먼저 알아보면\n\\(dB\\): 두 물리량의 크기의 비를 나타내는 방법 중 하나\n신호 B에 대한 신호 A에 대한 \\(dB\\)는 아래와 같이 표현할 수 있음 (신호 A,B는 전류, 전압, 음압 등을 의미)\n\\(x = 20 \\cdot \\log_{10}(\\frac{A}{B})\\)\n전력 또는 음향의 강도에 대한 \\(dB\\)는 아래와 같이 표현할 수 있음\n\\(y = 10 \\cdot \\log_{10}(\\frac{C}{D}) = 10 \\cdot \\log_{10}(\\frac{A^2}{B^2}) = 2 \\cdot 10 \\cdot \\log_{10}(\\frac{A}{B}) = x\\)\n이와 같이 표현할 수 있는 이유는 신호가 작용하는 곳의 임피던스(음향 임피던스)가 같은 경우, 전력(음향의 강도)의 비는 전압(음압)의 제곱의 비와 같기 때문임(\\(P = V \\cdot I = V^2 / Z\\))\n정리하자면 임피던스가 통일되어 있는 경우 전압(음압)비를 나타낸 \\(dB\\)와 전력(강도)비를 나타낸 \\(dB\\)는 동일함\n\n공학에서 \\(dB\\)를 사용하는 이유는 사용자연계에서 많은 일들이 지수함수적으로 발생하는데 이를 log를 이용해 표현하면 지수함수를 대수함수로 바꾸어 표현할 수 있어 사람이 다루기 쉬워진다 장점이 있기 때문\n\n\n\n\n일반적인 공학에서 사용하는 감쇠계수\\(\\alpha\\)는 다음과 같다.\n\\(\\alpha = 감쇠량[dB]/진행거리[cm]\\)\nif) 특정 주파수를 갖는 초음파가 1cm 진행하였더니 음압이 1/2로 줄었다면 해당 주파수에서의 매질의 감쇠계수 \\(6dB/cm\\)이다.\n계산을 하면 \\(\\alpha = 20\\log_{10}\\frac{1}{2}[dB] / 1[cm] = 20 \\cdot -0.301[dB] / 1[cm] = -6dB/cm\\) 이지만 감쇠계수이므로 부호를 바꿔줘야 한다.\n\n\n\n초음파가 감쇠될 떄, 일반적으로 감쇠되는 양은 초음파의 주파수에 따라 크게 달라진다. 그러므로 위에서 언급한 것 처럼 초음파진단기 공학에서의 감쇠계수 단위는 \\(dB/(cm \\cdot MHz)\\)이다.\n즉 \\(감쇠량[dB] = 감쇠계수[dB/cm] \\times 진행거리[cm] \\times 주파수[MHz]\\) 이다.\n\n\n\n지금까지의 주파수는 한가지 주파수만을 갖는 음파가 겪는 감쇠에 대한 설명이다\n실제 초음파는 여러 주파수 성분이 한꺼번에 섞여 들어있는 경우가 대부분이며, 그런 음파의 경우 각각의 주파수 성분에 대한 감쇠의 양이 각각 다르므로 감쇠가 된 다음에는 음파의 스펙트럼이 변화하게 된다. 즉, 신호의 대역폭이 넓을수록 스펙트럼의 변화가 커진다.\n\n반사되어 돌아오는 신호를 에코신호로 생각하면 되고 세로축은 각 주파수 성분의 진폭을 나타낸다.\n위의 그림을 통해 확인할 수 있는\n첫 번째 정보는 높은 주파수의 음파가 감쇠가 크므로 스펙트럼의 높은 주파수 성분이 줄어들어 저주파 성분이 비교적 많이 남게 된다는 점이고\n두 번째 정보는 음파의 진행거리가 길어질수록 높은 주파수 성분의 감쇠가 급격하게 커지므로 먼 거리를 투과해 온 음파의 스펙트럼일수록 중심주파수는 낮아지게 된다는 것이다."
  },
  {
    "objectID": "docs/BME/posts/ultrasound1.html#음파란",
    "href": "docs/BME/posts/ultrasound1.html#음파란",
    "title": "초음파 물리",
    "section": "",
    "text": "주파수에 따라 음파는 가청주파수와 초음파로 나뉜다. 가청주파수의 주파수는 20 ~ 20kHz이고 초음파는 20kHz보다 높은 주파수를 의미한다. 초음파 진단기에서는 주로 2 ~ 20MHz를 사용한다.\n음파의 전달 현상: 매질 내의 압력의 변화가 입자의 속도를 변화시키고, 입자의 속도 변화가 매질 내의 새로운 압력 변화를 만들어 내는 과정"
  },
  {
    "objectID": "docs/BME/posts/ultrasound1.html#음향의-물리량",
    "href": "docs/BME/posts/ultrasound1.html#음향의-물리량",
    "title": "초음파 물리",
    "section": "",
    "text": "주파수(frequency, \\(f, Hz\\)): 1초에 몇 회 진동하는지를 나타내는 값\n주기(period, \\(T, s\\)): 한 번 진동하는 동안 걸리는 시간\n\\(f = \\frac{1}{T}[Hz], T = \\frac{1}{f}[s]\\)\n초음파의 전달속도(\\(c, m/s\\)): 초음파가 1초에 몇 m를 퍼져나가는지를 나타내는 값\n초음파의 파장(\\(\\lambda, m\\)): 음파의 인접한 골 사이의 거리를 의미\n\\(\\lambda = c \\cdot T = \\frac{c}{f}[m]\\)\n액체 내에서의 초음파 전달속도는 아래와 같이 근사적으로 나타낼 수 있음\n\\(c = \\sqrt{\\frac{B}{\\rho}}[m/s]\\)\n여기서 \\(B[N/m^2]\\) 는 Bulk Modulus로 매질의 경도(stiffness = incompressibility)를 나타낸다. B의 값이 크면 물질이 딱딱하고 B의 값이 작으면 물질이 물렁하다.\n\\(\\rho [kg/m^3]\\) 는 매질의 밀도를 나타낸다.\n\n\n\n매질에 정현파(sin wave)의 압력을 계속 가해 주면 입자의 속도가 정현파가 된다.\n음향 임피던스(Acoustic Impedence, \\(Z, Rayl = kg/m^2/s\\)): 같은 크기의 정현파 압력을 가해 주어도 매질의 특성에 따라서 입자의 진동의 크기가 달라질 수 있다. 여기서 매질의 특성을 나타내는 것이 음향 임피던스이다.\n\\(Z = \\rho \\cdot c = P/v = \\sqrt{\\rho \\cdot B}\\) \\([kg/m^2/s]\\)\n음압(Sound Pressure): 음파의 진폭으로 압력, 입자의 속도 등으로 표현할 수 있다. 정현파의 경우 첨두치(peak) 압력을 음압으로 본다.\n강도(Intensity): 단위면적을 통과하는 파워로 아래와 같이 나타낼 수 있다.\n\\(I = VP = \\frac{P^2}{Z} = V^2 Z\\)\n\\(V\\)는 입자의 속력, \\(P\\)는 음압, \\(Z\\)는 음향 임피던스를 의미한다.\n단위면적을 통과하는 파워이므로 골고루 퍼져 넓은 면적을 통과한다면 강도는 약해지게 된다.\n\n\n\n전압은 음압, 전류는 음향의 입자 속력, 임피던스는 음향 임피던스, 전력은 음향의 강도에 비유할 수 있다.\n전기회로(교류): \\(V = IZ\\)\n음향: \\(P = UZ\\), \\(P\\)는 음압, \\(U\\)는 매질 입자의 속도, \\(Z\\)는 음향 임피던스를 의미\n전기회로: \\(P\\)(전력, \\(W\\)) = \\(VI\\)\n음향: \\(J\\)(강도, \\(W/m^2\\)) = \\(\\frac{UP}{2}\\)\n\\(\\frac{1}{2}\\) 계수는 음향에서는 P와 U는 첨두치, J는 실효치를 사용했지만 회로에서는 실효치만 사용했기 떄문"
  },
  {
    "objectID": "docs/BME/posts/ultrasound1.html#경계면과-초음파",
    "href": "docs/BME/posts/ultrasound1.html#경계면과-초음파",
    "title": "초음파 물리",
    "section": "",
    "text": "다른 성질을 갖는 매질이 서로 접해 있는 경계면에 초음파가 도달하면 반사, 투과, 굴절 등의 현상이 생김\n\n\n\n위의 사진을 통해 초음파의 반사(reflection)와 투과(transmission)를 확인할 수 있다.\n매질이나 경계면에서 에너지의 손실이 없으면 반사되는 음압\\(P_r\\)은 입사된 음압\\(P_i\\)에 대해 아래와 같은 관계를 갖게 됨\n\\(P_r = R \\cdot P_i\\)\n여기서 R은 반사계수로 아래와 같이 표현할 수 있다\n\\(R = \\frac{P_r}{P_i} = \\frac{Z_2 - Z_1}{Z_2 + Z_1}\\) (만약 R &lt; 0이라면 반사파의 위상이 반전됨)\n에너지 보존법칙에 따라 \\(I_i = I_r + I_t\\) 와 같다.\n\\(1 = R^2 + T^2 \\frac{Z_1}{Z_2}\\) \\(\\to\\) \\(T = \\frac{2Z_2}{Z_1 + Z_2} = 1+R\\)\n투과파의 음압은 \\(P_t = T \\cdot P_i\\) 이다.\n\n\n\n굴절(reflection): 초음파가 경계면에 수직으로 입사하지 않는 경우, 입사각과 투과각이 달라지는 현상\n\n스넬의 법칙: \\(\\frac{\\sin{\\theta_1}}{c_1} = \\frac{\\sin{\\theta_2}}{c_2}\\) (\\(\\theta_1\\)은 입사각(반사각), \\(\\theta_2\\)는 굴절각을 의미)\n입사각과 굴절각의 관계는 각각의 매질의 음속에만 직접적으로 관련이 있으며 밀도, 음향 임피던스는 직접적으로 관련이 있지 않음을 의미\n만약 굴절각(\\(\\theta_2\\))가 90\\(\\degree\\)가 넘게 되면 반사파만이 존재하게 되어 전반사가 일어남을 의미\n\n\n\n산란(scattering): 평면 초음파가 만난 경계면이 파장에 비해 길이가 작은 면을 만나면 반사파는 구면파에 가깝게 되어 넓게 퍼져나가게 되는 현상\n산란체(scatter): 위와 같은 경계면을 만든 작은 매질\n레일리 산란(Rayleigh Scattering): 산란체의 크기가 파장에 비해 상당히 작은 경우에 발생하는 산란\n\n\n\n감쇠(attenuation): 매질의 에너지 손실(음향 에너지 \\(\\to\\) 주로 열 에너지) 및 산란으로 인해 음압이 줄어드는 현상\n초음파 공학에서는 감쇠가 주파수에 비례한다고 근사화하여 감쇠계수(\\(\\alpha, dB/(cm \\cdot MHz)\\)) 를 이와 같이 표기한다.\n일반적인 공학에서는 감쇠계수의 단위는 \\(dB/cm\\)이다\n\n\n감쇠계수에 사용되는 \\(dB\\)라는 단위에 대해서 먼저 알아보면\n\\(dB\\): 두 물리량의 크기의 비를 나타내는 방법 중 하나\n신호 B에 대한 신호 A에 대한 \\(dB\\)는 아래와 같이 표현할 수 있음 (신호 A,B는 전류, 전압, 음압 등을 의미)\n\\(x = 20 \\cdot \\log_{10}(\\frac{A}{B})\\)\n전력 또는 음향의 강도에 대한 \\(dB\\)는 아래와 같이 표현할 수 있음\n\\(y = 10 \\cdot \\log_{10}(\\frac{C}{D}) = 10 \\cdot \\log_{10}(\\frac{A^2}{B^2}) = 2 \\cdot 10 \\cdot \\log_{10}(\\frac{A}{B}) = x\\)\n이와 같이 표현할 수 있는 이유는 신호가 작용하는 곳의 임피던스(음향 임피던스)가 같은 경우, 전력(음향의 강도)의 비는 전압(음압)의 제곱의 비와 같기 때문임(\\(P = V \\cdot I = V^2 / Z\\))\n정리하자면 임피던스가 통일되어 있는 경우 전압(음압)비를 나타낸 \\(dB\\)와 전력(강도)비를 나타낸 \\(dB\\)는 동일함\n\n공학에서 \\(dB\\)를 사용하는 이유는 사용자연계에서 많은 일들이 지수함수적으로 발생하는데 이를 log를 이용해 표현하면 지수함수를 대수함수로 바꾸어 표현할 수 있어 사람이 다루기 쉬워진다 장점이 있기 때문\n\n\n\n\n일반적인 공학에서 사용하는 감쇠계수\\(\\alpha\\)는 다음과 같다.\n\\(\\alpha = 감쇠량[dB]/진행거리[cm]\\)\nif) 특정 주파수를 갖는 초음파가 1cm 진행하였더니 음압이 1/2로 줄었다면 해당 주파수에서의 매질의 감쇠계수 \\(6dB/cm\\)이다.\n계산을 하면 \\(\\alpha = 20\\log_{10}\\frac{1}{2}[dB] / 1[cm] = 20 \\cdot -0.301[dB] / 1[cm] = -6dB/cm\\) 이지만 감쇠계수이므로 부호를 바꿔줘야 한다.\n\n\n\n초음파가 감쇠될 떄, 일반적으로 감쇠되는 양은 초음파의 주파수에 따라 크게 달라진다. 그러므로 위에서 언급한 것 처럼 초음파진단기 공학에서의 감쇠계수 단위는 \\(dB/(cm \\cdot MHz)\\)이다.\n즉 \\(감쇠량[dB] = 감쇠계수[dB/cm] \\times 진행거리[cm] \\times 주파수[MHz]\\) 이다.\n\n\n\n지금까지의 주파수는 한가지 주파수만을 갖는 음파가 겪는 감쇠에 대한 설명이다\n실제 초음파는 여러 주파수 성분이 한꺼번에 섞여 들어있는 경우가 대부분이며, 그런 음파의 경우 각각의 주파수 성분에 대한 감쇠의 양이 각각 다르므로 감쇠가 된 다음에는 음파의 스펙트럼이 변화하게 된다. 즉, 신호의 대역폭이 넓을수록 스펙트럼의 변화가 커진다.\n\n반사되어 돌아오는 신호를 에코신호로 생각하면 되고 세로축은 각 주파수 성분의 진폭을 나타낸다.\n위의 그림을 통해 확인할 수 있는\n첫 번째 정보는 높은 주파수의 음파가 감쇠가 크므로 스펙트럼의 높은 주파수 성분이 줄어들어 저주파 성분이 비교적 많이 남게 된다는 점이고\n두 번째 정보는 음파의 진행거리가 길어질수록 높은 주파수 성분의 감쇠가 급격하게 커지므로 먼 거리를 투과해 온 음파의 스펙트럼일수록 중심주파수는 낮아지게 된다는 것이다."
  }
]