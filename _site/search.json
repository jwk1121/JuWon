[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JuWon Kwon",
    "section": "",
    "text": "바이오메디컬공학을 전공하고 있는 학부생 입니다.\n초음파와 딥러닝을 공부하고 있습니다."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "JuWon Kwon",
    "section": "",
    "text": "바이오메디컬공학을 전공하고 있는 학부생 입니다.\n초음파와 딥러닝을 공부하고 있습니다."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "rstat101.html",
    "href": "rstat101.html",
    "title": "Tutorial",
    "section": "",
    "text": "test12414입니\n123123다"
  },
  {
    "objectID": "tutorial_1.html",
    "href": "tutorial_1.html",
    "title": "test123",
    "section": "",
    "text": "안녕하세요 권주원입니다\nqmd test 진행중입니다"
  },
  {
    "objectID": "docs/rstat/rstat101.html",
    "href": "docs/rstat/rstat101.html",
    "title": "Tutorial",
    "section": "",
    "text": "test12414입니\n123123다"
  },
  {
    "objectID": "docs/rstat/rstat_lecture.html",
    "href": "docs/rstat/rstat_lecture.html",
    "title": "rstat lecture",
    "section": "",
    "text": "1강\n2강\n3강"
  },
  {
    "objectID": "docs/rstat/index.html",
    "href": "docs/rstat/index.html",
    "title": "Quarto blog",
    "section": "",
    "text": "CNN\n\n\nCNN part1 test\n\n\n\n\nDL\n\n\nCNN\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nTutorial\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/rstat/posts/rstat101.html",
    "href": "docs/rstat/posts/rstat101.html",
    "title": "Tutorial",
    "section": "",
    "text": "test12414입니\n123123다"
  },
  {
    "objectID": "docs/rstat/posts/rstat_lecture.html",
    "href": "docs/rstat/posts/rstat_lecture.html",
    "title": "rstat lecture",
    "section": "",
    "text": "1강\n2강\n3강"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html",
    "href": "docs/rstat/posts/DL_CNN.html",
    "title": "CNN",
    "section": "",
    "text": "toc:true\n\n\nimport torch \nimport torchvision\nfrom fastai.vision.all import * \nimport time\n\n\n\n간략하게 CNN의 구조를 설명하자면 - conv layer: 커널층(선형변환처럼 feature를 늘려주는 역할) - ReLU layer: 비선형을 추가해 표현력을 늘려주는 역할 - pooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n# (굳이 (2,2) 이런식으로 안해도 됨 어차피 윈도우 사이즈는 정사각행렬이므로 상수하나만 입력해도 됨됨)\n\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv.weight.data = torch.tensor([[[[0.25, 0.25],[0.25,0.25]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]), tensor([0.]))\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임) - 해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음 - 윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함 - 첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용 - 두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n\n\n\n\n사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.\n\\(ReLU(x) = \\max(0,x)\\)\n\n\n\n\n\npooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\nmaxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_maxpooling(_X) \n\ntensor([[[ 6.,  8.],\n         [16., 18.]]])\n\n\n\nmaxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\npooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n\n이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다\n\n\n\n\n\n\n\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02&lt;00:00]\n    \n    \n\n\n\n\n\n(path/'원하는 경로').ls()\n\n\n\ntorchvision.io.read_image()\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/1').ls()])\nx_tr = torch.concat([x0, x1])/255\n\ny_tr = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/1').ls()])\nx_test = torch.concat([x0, x1])/255\n\ny_test = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\nx_tr.shape, y_tr.shape, x_test.shape, y_test.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([2115, 1]))\n\n\n\n\n\n# conv\nc1 = torch.nn.Conv2d(1, 16 , 5) # 만약 color image였다면 입력 채널의 수를 3으로 지정해야 함\nx_tr.shape ,c1(x_tr).shape\n\n(torch.Size([12665, 1, 28, 28]), torch.Size([12665, 16, 24, 24]))\n\n\n\nsize계산 공식: 윈도우(커널)사이즈가 n이면 \\(size = height(width) - ( n - 1)\\)\n\n\n# ReLU\na1 = torch.nn.ReLU()\n\n\n# maxpooling\nm1 = torch.nn.MaxPool2d(2)\nprint(m1(a1(c1(x_tr))).shape)\n\ntorch.Size([12665, 16, 12, 12])\n\n\n\n행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n\n# flatten(이미지를 한줄로 펼치는 것)\nf1 = torch.nn.Flatten()\nprint(f1(a1(m1(c1(x_tr)))).shape) #16 * 12 * 12 = 2304\n\ntorch.Size([12665, 2304])\n\n\n\n# sigmoid에 올리기 위해서는 2304 디멘젼을 1로 만들어야함\nl1=torch.nn.Linear(in_features=2304,out_features=1) \n\n\n# sigmoid (값이 0에서 1사이의 값, 즉 확률로 출력되도록)\na2 = torch.nn.Sigmoid()\nprint(a2(f1(a1(m1(c1(x_tr))))).shape)\n\ntorch.Size([12665, 2304])\n\n\n\nprint('이미지 사이즈:                     ', x_tr.shape)\nprint('conv:                              ',c1(x_tr).shape)\nprint('(ReLU) -&gt; maxpooling:              ',m1(a1(c1(x_tr))).shape)\nprint('이미지 펼치기:                     ',f1(a1(m1(c1(x_tr)))).shape)\nprint('이미지를 하나의 스칼라로 선형변환: ',l1(f1(a1(m1(c1(x_tr))))).shape)\nprint('시그모이드:                        ',a2(l1(f1(a1(m1(c1(x_tr)))))).shape)\n\n이미지 사이즈:                      torch.Size([12665, 1, 28, 28])\nconv:                               torch.Size([12665, 16, 24, 24])\n(ReLU) -&gt; maxpooling:               torch.Size([12665, 16, 12, 12])\n이미지 펼치기:                      torch.Size([12665, 2304])\n이미지를 하나의 스칼라로 선형변환:  torch.Size([12665, 1])\n시그모이드:                         torch.Size([12665, 1])\n\n\n\n\n\n\n원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\\(\\to\\) overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n# 데이터 로더에 들어갈 데이터세트 준비\nds_tr = torch.utils.data.TensorDataset(x_tr, y_tr)\nds_test = torch.utils.data.TensorDataset(x_test, y_test)\n\n\n데이터 로더는 배치크기를 지정해줘야 함\n\n\nlen(x_tr), len(x_test)\n\n(12665, 2115)\n\n\n\n데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n\nstochastic gradient descent(구용어: mini-batch gradient descent)\n\n\n\n# 데이터 로더\ndl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=1266) \ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=2115) \n\n\ndls = DataLoaders(dl_tr,dl_test)\n\n\nnet = torch.nn.Sequential(\n    # conv layer \n    c1, \n    # ReLU\n    a1, \n    # maxpool\n    m1, \n    # flatten\n    f1, \n    # linear transform (n -&gt; 1)\n    l1,\n    # Sigmoid\n    a2\n)\n\nloss_fn=torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn) # fastai의 Learner는 오미타이저의 기본값이 adam이므로 따로 지정해주지 않아도 됨\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.979273\n0.639250\n00:05\n\n\n1\n0.703008\n0.402466\n00:00\n\n\n2\n0.547401\n0.256881\n00:00\n\n\n3\n0.434025\n0.142217\n00:00\n\n\n4\n0.340996\n0.079636\n00:00\n\n\n5\n0.267902\n0.048050\n00:00\n\n\n6\n0.211895\n0.031742\n00:00\n\n\n7\n0.169176\n0.022921\n00:00\n\n\n8\n0.136331\n0.017658\n00:00\n\n\n9\n0.110770\n0.014233\n00:00\n\n\n\n\n\n\nLearner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음 &gt; python    net[0].weight 위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = ’cuda:0’라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다.\n+ tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n\nnet = net.to('cpu')\nplt.plot(net(x_tr).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(x_test).data,'.')\nplt.title(\"Testing Set\",size=15)\n\nText(0.5, 1.0, 'Testing Set')\n\n\n\n\n\n\n\n\n\n\n\n\nBCEWithLogitsLoss = Sigmoid + BCELoss\n손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음\n\n\n\n\n\n\n다중(k개) 클래스를 분류 - LossFunction: CrossEntrophyLoss\n- ActivationFunction: SoftmaxFunction - 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n\n소프트맥스 함수가 계산하는 과정은 아래와 같음 (3개를 분류할 경우) \\(softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}\\)\n\n\n\n\n\nk개의 클래스를 분류하는 모델의 Loss 계산 방법\n\nsftmax(_netout) # -&gt; 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -&gt; 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -&gt; 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n+ 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임\n+ 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n\n\n\ntype 1) int형을 사용하는 방법 (vector)\ntype 2) float형을 사용하는 방법 (one-hot encoded vector)\n만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n\ntype 1)의 경우 &lt;사슴: 0, 강아지: 1, 고양이: 2&gt; (단, 데이터 형태는는 int(정수))\ntype 2)의 경우 &lt;사슴: [1, 0, 0], 강아지: [0, 1, 0], 고양이: [0, 0, 1] &gt; (단, 데이터의 형태는 float(실수))\n\n\n\n\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n\n\ntorch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n\n\n\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\ntorch.nn.functional.one_hot(yy).float()\n\ntensor([[1., 0.],\n        [1., 0.],\n        [1., 0.],\n        ...,\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])\n\n\n\n\n\n\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\nlen(X) # 18623\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번= 1862 꽉 채워서 10번하고 3개정도 남은 걸로 한 번\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # test는 전부다 넣어서\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\n\nloss_fn = torch.nn.CrossEntropyLoss() # 여기에는 softmax함수가 내장되어 있음 \n#-&gt; net마지막에 softmax를 넣으면 안됨\n# BCEWithLogitsLoss 느낌(시그모이드 + BCE)\n\n\nlrnr = Learner(dls,net,loss_fn) # 옵티마이저는 아답이 디폴트 값이어서 굳이 안넣어도 됨\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.797101\n1.103916\n00:00\n\n\n1\n1.267011\n0.823797\n00:00\n\n\n2\n1.055513\n0.667210\n00:00\n\n\n3\n0.910746\n0.435414\n00:00\n\n\n4\n0.762887\n0.284001\n00:00\n\n\n5\n0.625515\n0.199502\n00:00\n\n\n6\n0.515352\n0.152906\n00:00\n\n\n7\n0.429145\n0.123678\n00:00\n\n\n8\n0.359694\n0.105466\n00:00\n\n\n9\n0.303888\n0.092883\n00:00\n\n\n\n\n\n\nlrnr.model.to(\"cpu\")\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n3142\n-1.138366\n-5.435792\n0.370670\n2\n\n\n3143\n-4.458741\n-4.281343\n2.052410\n2\n\n\n3144\n-2.836508\n-3.204013\n0.012610\n2\n\n\n3145\n-1.704158\n-10.621873\n2.024313\n2\n\n\n3146\n-2.467648\n-4.612999\n0.656284\n2\n\n\n\n\n\n3147 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n975\n0.432969\n-5.653580\n-1.944451\n0\n\n\n976\n2.685695\n-10.254354\n-2.466680\n0\n\n\n977\n2.474842\n-9.650204\n-2.452746\n0\n\n\n978\n1.268743\n-6.928779\n-1.419695\n0\n\n\n979\n4.371464\n-8.959077\n-4.056257\n0\n\n\n\n\n\n980 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼\n\n\n\n\n\n이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n\n\n\n\nfastai에서 지원\nfastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\ny의 형태를 주의해서 사용해야 함\n\n앞서 말한 것처럼 두 가지 타입이 있음\n\nvector + int\none-hot encoded vector + float\n\n\n\ntype 1) y의 형태가 vector + int일 때 - metrics = accuracy를 사용해야 함\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\ny.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\n\n\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nyy.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n1.128365\n0.601474\n0.463357\n0.536643\n00:00\n\n\n1\n0.684630\n0.304262\n0.975414\n0.024586\n00:00\n\n\n2\n0.503124\n0.144147\n0.989598\n0.010402\n00:00\n\n\n3\n0.373899\n0.068306\n0.996217\n0.003783\n00:00\n\n\n4\n0.281332\n0.040790\n0.996217\n0.003783\n00:00\n\n\n5\n0.215743\n0.026980\n0.996690\n0.003310\n00:00\n\n\n6\n0.168349\n0.019467\n0.996690\n0.003310\n00:00\n\n\n7\n0.133313\n0.014856\n0.998109\n0.001891\n00:00\n\n\n8\n0.106776\n0.011745\n0.998109\n0.001891\n00:00\n\n\n9\n0.086275\n0.009553\n0.999054\n0.000946\n00:00\n\n\n\n\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때 - metrics = accuracy_multi를 사용해야 함 - error_rate는 사용못함\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n\n\ny_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\nyy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\ntorch.nn.functional.one_hot() 함수 조건\n\n기본적으로 크기가 n인 벡터가 들어오길 기대\n정수가 들어오는 것을 기대\n\n하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n1.213650\n0.647844\n0.463357\n00:00\n\n\n1\n0.744869\n0.384000\n0.933570\n00:00\n\n\n2\n0.571879\n0.187119\n0.986525\n00:00\n\n\n3\n0.432096\n0.090841\n0.994563\n00:00\n\n\n4\n0.326718\n0.048868\n0.995508\n00:00\n\n\n5\n0.250235\n0.028477\n0.996217\n00:00\n\n\n6\n0.194605\n0.018616\n0.996454\n00:00\n\n\n7\n0.153338\n0.013278\n0.996927\n00:00\n\n\n8\n0.122056\n0.010130\n0.997872\n00:00\n\n\n9\n0.097957\n0.008112\n0.998109\n00:00"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#cnn-구조",
    "href": "docs/rstat/posts/DL_CNN.html#cnn-구조",
    "title": "CNN",
    "section": "",
    "text": "간략하게 CNN의 구조를 설명하자면 - conv layer: 커널층(선형변환처럼 feature를 늘려주는 역할) - ReLU layer: 비선형을 추가해 표현력을 늘려주는 역할 - pooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n# (굳이 (2,2) 이런식으로 안해도 됨 어차피 윈도우 사이즈는 정사각행렬이므로 상수하나만 입력해도 됨됨)\n\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv.weight.data = torch.tensor([[[[0.25, 0.25],[0.25,0.25]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]), tensor([0.]))\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임) - 해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음 - 윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함 - 첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용 - 두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n\n\n\n\n사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.\n\\(ReLU(x) = \\max(0,x)\\)\n\n\n\n\n\npooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\nmaxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_maxpooling(_X) \n\ntensor([[[ 6.,  8.],\n         [16., 18.]]])\n\n\n\nmaxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\npooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n\n이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#cnn-구현",
    "href": "docs/rstat/posts/DL_CNN.html#cnn-구현",
    "title": "CNN",
    "section": "",
    "text": "path = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02&lt;00:00]"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#path-사용법-데이터-준비",
    "href": "docs/rstat/posts/DL_CNN.html#path-사용법-데이터-준비",
    "title": "CNN",
    "section": "",
    "text": "(path/'원하는 경로').ls()"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#위-코드를-사용하면-폴더-안에-있는-데이터들의-이름을-출력",
    "href": "docs/rstat/posts/DL_CNN.html#위-코드를-사용하면-폴더-안에-있는-데이터들의-이름을-출력",
    "title": "CNN",
    "section": "",
    "text": "torchvision.io.read_image()\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/1').ls()])\nx_tr = torch.concat([x0, x1])/255\n\ny_tr = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/1').ls()])\nx_test = torch.concat([x0, x1])/255\n\ny_test = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\nx_tr.shape, y_tr.shape, x_test.shape, y_test.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([2115, 1]))\n\n\n\n\n\n# conv\nc1 = torch.nn.Conv2d(1, 16 , 5) # 만약 color image였다면 입력 채널의 수를 3으로 지정해야 함\nx_tr.shape ,c1(x_tr).shape\n\n(torch.Size([12665, 1, 28, 28]), torch.Size([12665, 16, 24, 24]))\n\n\n\nsize계산 공식: 윈도우(커널)사이즈가 n이면 \\(size = height(width) - ( n - 1)\\)\n\n\n# ReLU\na1 = torch.nn.ReLU()\n\n\n# maxpooling\nm1 = torch.nn.MaxPool2d(2)\nprint(m1(a1(c1(x_tr))).shape)\n\ntorch.Size([12665, 16, 12, 12])\n\n\n\n행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n\n# flatten(이미지를 한줄로 펼치는 것)\nf1 = torch.nn.Flatten()\nprint(f1(a1(m1(c1(x_tr)))).shape) #16 * 12 * 12 = 2304\n\ntorch.Size([12665, 2304])\n\n\n\n# sigmoid에 올리기 위해서는 2304 디멘젼을 1로 만들어야함\nl1=torch.nn.Linear(in_features=2304,out_features=1) \n\n\n# sigmoid (값이 0에서 1사이의 값, 즉 확률로 출력되도록)\na2 = torch.nn.Sigmoid()\nprint(a2(f1(a1(m1(c1(x_tr))))).shape)\n\ntorch.Size([12665, 2304])\n\n\n\nprint('이미지 사이즈:                     ', x_tr.shape)\nprint('conv:                              ',c1(x_tr).shape)\nprint('(ReLU) -&gt; maxpooling:              ',m1(a1(c1(x_tr))).shape)\nprint('이미지 펼치기:                     ',f1(a1(m1(c1(x_tr)))).shape)\nprint('이미지를 하나의 스칼라로 선형변환: ',l1(f1(a1(m1(c1(x_tr))))).shape)\nprint('시그모이드:                        ',a2(l1(f1(a1(m1(c1(x_tr)))))).shape)\n\n이미지 사이즈:                      torch.Size([12665, 1, 28, 28])\nconv:                               torch.Size([12665, 16, 24, 24])\n(ReLU) -&gt; maxpooling:               torch.Size([12665, 16, 12, 12])\n이미지 펼치기:                      torch.Size([12665, 2304])\n이미지를 하나의 스칼라로 선형변환:  torch.Size([12665, 1])\n시그모이드:                         torch.Size([12665, 1])\n\n\n\n\n\n\n원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\\(\\to\\) overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n# 데이터 로더에 들어갈 데이터세트 준비\nds_tr = torch.utils.data.TensorDataset(x_tr, y_tr)\nds_test = torch.utils.data.TensorDataset(x_test, y_test)\n\n\n데이터 로더는 배치크기를 지정해줘야 함\n\n\nlen(x_tr), len(x_test)\n\n(12665, 2115)\n\n\n\n데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n\nstochastic gradient descent(구용어: mini-batch gradient descent)\n\n\n\n# 데이터 로더\ndl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=1266) \ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=2115) \n\n\ndls = DataLoaders(dl_tr,dl_test)\n\n\nnet = torch.nn.Sequential(\n    # conv layer \n    c1, \n    # ReLU\n    a1, \n    # maxpool\n    m1, \n    # flatten\n    f1, \n    # linear transform (n -&gt; 1)\n    l1,\n    # Sigmoid\n    a2\n)\n\nloss_fn=torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn) # fastai의 Learner는 오미타이저의 기본값이 adam이므로 따로 지정해주지 않아도 됨\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.979273\n0.639250\n00:05\n\n\n1\n0.703008\n0.402466\n00:00\n\n\n2\n0.547401\n0.256881\n00:00\n\n\n3\n0.434025\n0.142217\n00:00\n\n\n4\n0.340996\n0.079636\n00:00\n\n\n5\n0.267902\n0.048050\n00:00\n\n\n6\n0.211895\n0.031742\n00:00\n\n\n7\n0.169176\n0.022921\n00:00\n\n\n8\n0.136331\n0.017658\n00:00\n\n\n9\n0.110770\n0.014233\n00:00\n\n\n\n\n\n\nLearner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음 &gt; python    net[0].weight 위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = ’cuda:0’라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다.\n+ tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n\nnet = net.to('cpu')\nplt.plot(net(x_tr).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(x_test).data,'.')\nplt.title(\"Testing Set\",size=15)\n\nText(0.5, 1.0, 'Testing Set')"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#loss-function",
    "href": "docs/rstat/posts/DL_CNN.html#loss-function",
    "title": "CNN",
    "section": "",
    "text": "BCEWithLogitsLoss = Sigmoid + BCELoss\n손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음"
  },
  {
    "objectID": "docs/rstat/posts/DL_CNN.html#k개의-클래스를-분류하는-모델",
    "href": "docs/rstat/posts/DL_CNN.html#k개의-클래스를-분류하는-모델",
    "title": "CNN",
    "section": "",
    "text": "다중(k개) 클래스를 분류 - LossFunction: CrossEntrophyLoss\n- ActivationFunction: SoftmaxFunction - 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n\n소프트맥스 함수가 계산하는 과정은 아래와 같음 (3개를 분류할 경우) \\(softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}\\)\n\n\n\n\n\nk개의 클래스를 분류하는 모델의 Loss 계산 방법\n\nsftmax(_netout) # -&gt; 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -&gt; 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -&gt; 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n+ 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임\n+ 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n\n\n\ntype 1) int형을 사용하는 방법 (vector)\ntype 2) float형을 사용하는 방법 (one-hot encoded vector)\n만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n\ntype 1)의 경우 &lt;사슴: 0, 강아지: 1, 고양이: 2&gt; (단, 데이터 형태는는 int(정수))\ntype 2)의 경우 &lt;사슴: [1, 0, 0], 강아지: [0, 1, 0], 고양이: [0, 0, 1] &gt; (단, 데이터의 형태는 float(실수))\n\n\n\n\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n\n\ntorch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n\n\n\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\ntorch.nn.functional.one_hot(yy).float()\n\ntensor([[1., 0.],\n        [1., 0.],\n        [1., 0.],\n        ...,\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])\n\n\n\n\n\n\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\nlen(X) # 18623\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번= 1862 꽉 채워서 10번하고 3개정도 남은 걸로 한 번\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # test는 전부다 넣어서\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\n\nloss_fn = torch.nn.CrossEntropyLoss() # 여기에는 softmax함수가 내장되어 있음 \n#-&gt; net마지막에 softmax를 넣으면 안됨\n# BCEWithLogitsLoss 느낌(시그모이드 + BCE)\n\n\nlrnr = Learner(dls,net,loss_fn) # 옵티마이저는 아답이 디폴트 값이어서 굳이 안넣어도 됨\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.797101\n1.103916\n00:00\n\n\n1\n1.267011\n0.823797\n00:00\n\n\n2\n1.055513\n0.667210\n00:00\n\n\n3\n0.910746\n0.435414\n00:00\n\n\n4\n0.762887\n0.284001\n00:00\n\n\n5\n0.625515\n0.199502\n00:00\n\n\n6\n0.515352\n0.152906\n00:00\n\n\n7\n0.429145\n0.123678\n00:00\n\n\n8\n0.359694\n0.105466\n00:00\n\n\n9\n0.303888\n0.092883\n00:00\n\n\n\n\n\n\nlrnr.model.to(\"cpu\")\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n3142\n-1.138366\n-5.435792\n0.370670\n2\n\n\n3143\n-4.458741\n-4.281343\n2.052410\n2\n\n\n3144\n-2.836508\n-3.204013\n0.012610\n2\n\n\n3145\n-1.704158\n-10.621873\n2.024313\n2\n\n\n3146\n-2.467648\n-4.612999\n0.656284\n2\n\n\n\n\n\n3147 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n975\n0.432969\n-5.653580\n-1.944451\n0\n\n\n976\n2.685695\n-10.254354\n-2.466680\n0\n\n\n977\n2.474842\n-9.650204\n-2.452746\n0\n\n\n978\n1.268743\n-6.928779\n-1.419695\n0\n\n\n979\n4.371464\n-8.959077\n-4.056257\n0\n\n\n\n\n\n980 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼\n\n\n\n\n\n이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n\n\n\n\nfastai에서 지원\nfastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\ny의 형태를 주의해서 사용해야 함\n\n앞서 말한 것처럼 두 가지 타입이 있음\n\nvector + int\none-hot encoded vector + float\n\n\n\ntype 1) y의 형태가 vector + int일 때 - metrics = accuracy를 사용해야 함\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\ny.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\n\n\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nyy.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n1.128365\n0.601474\n0.463357\n0.536643\n00:00\n\n\n1\n0.684630\n0.304262\n0.975414\n0.024586\n00:00\n\n\n2\n0.503124\n0.144147\n0.989598\n0.010402\n00:00\n\n\n3\n0.373899\n0.068306\n0.996217\n0.003783\n00:00\n\n\n4\n0.281332\n0.040790\n0.996217\n0.003783\n00:00\n\n\n5\n0.215743\n0.026980\n0.996690\n0.003310\n00:00\n\n\n6\n0.168349\n0.019467\n0.996690\n0.003310\n00:00\n\n\n7\n0.133313\n0.014856\n0.998109\n0.001891\n00:00\n\n\n8\n0.106776\n0.011745\n0.998109\n0.001891\n00:00\n\n\n9\n0.086275\n0.009553\n0.999054\n0.000946\n00:00\n\n\n\n\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때 - metrics = accuracy_multi를 사용해야 함 - error_rate는 사용못함\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n\n\ny_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\nyy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\ntorch.nn.functional.one_hot() 함수 조건\n\n기본적으로 크기가 n인 벡터가 들어오길 기대\n정수가 들어오는 것을 기대\n\n하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n1.213650\n0.647844\n0.463357\n00:00\n\n\n1\n0.744869\n0.384000\n0.933570\n00:00\n\n\n2\n0.571879\n0.187119\n0.986525\n00:00\n\n\n3\n0.432096\n0.090841\n0.994563\n00:00\n\n\n4\n0.326718\n0.048868\n0.995508\n00:00\n\n\n5\n0.250235\n0.028477\n0.996217\n00:00\n\n\n6\n0.194605\n0.018616\n0.996454\n00:00\n\n\n7\n0.153338\n0.013278\n0.996927\n00:00\n\n\n8\n0.122056\n0.010130\n0.997872\n00:00\n\n\n9\n0.097957\n0.008112\n0.998109\n00:00"
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "JuWon Kwon",
    "section": "",
    "text": "This is the contents of the about page for my blog."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "JuWon Kwon",
    "section": "",
    "text": "바이오메디컬공학을 전공하고 있는 학부생 입니다.\n초음파와 딥러닝을 공부하고 있습니다."
  },
  {
    "objectID": "docs/DL/index.html",
    "href": "docs/DL/index.html",
    "title": "Quarto blog",
    "section": "",
    "text": "CNN\n\n\nCNN part1\n\n\n\n\nDL\n\n\nCNN\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nCNN\n\n\nCNN part2\n\n\n\n\nDL\n\n\nCNN\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2022\n\n\nJuWon\n\n\n\n\n\n\n  \n\n\n\n\nTutorial\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html",
    "href": "docs/DL/posts/DL_CNN.html",
    "title": "CNN",
    "section": "",
    "text": "toc:true\n\n\nimport torch \nimport torchvision\nfrom fastai.vision.all import * \nimport time\n\n\n\n간략하게 CNN의 구조를 설명하자면 - conv layer: 커널층(선형변환처럼 feature를 늘려주는 역할) - ReLU layer: 비선형을 추가해 표현력을 늘려주는 역할 - pooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n# (굳이 (2,2) 이런식으로 안해도 됨 어차피 윈도우 사이즈는 정사각행렬이므로 상수하나만 입력해도 됨됨)\n\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv.weight.data = torch.tensor([[[[0.25, 0.25],[0.25,0.25]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]), tensor([0.]))\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임) - 해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음 - 윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함 - 첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용 - 두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n\n\n\n\n사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.\n\\(ReLU(x) = \\max(0,x)\\)\n\n\n\n\n\npooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\nmaxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_maxpooling(_X) \n\ntensor([[[ 6.,  8.],\n         [16., 18.]]])\n\n\n\nmaxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\npooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n\n이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다\n\n\n\n\n\n\n\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02&lt;00:00]\n    \n    \n\n\n\n\n\n(path/'원하는 경로').ls()\n\n\n\ntorchvision.io.read_image()\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/1').ls()])\nx_tr = torch.concat([x0, x1])/255\n\ny_tr = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/1').ls()])\nx_test = torch.concat([x0, x1])/255\n\ny_test = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\nx_tr.shape, y_tr.shape, x_test.shape, y_test.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([2115, 1]))\n\n\n\n\n\n# conv\nc1 = torch.nn.Conv2d(1, 16 , 5) # 만약 color image였다면 입력 채널의 수를 3으로 지정해야 함\nx_tr.shape ,c1(x_tr).shape\n\n(torch.Size([12665, 1, 28, 28]), torch.Size([12665, 16, 24, 24]))\n\n\n\nsize계산 공식: 윈도우(커널)사이즈가 n이면 \\(size = height(width) - ( n - 1)\\)\n\n\n# ReLU\na1 = torch.nn.ReLU()\n\n\n# maxpooling\nm1 = torch.nn.MaxPool2d(2)\nprint(m1(a1(c1(x_tr))).shape)\n\ntorch.Size([12665, 16, 12, 12])\n\n\n\n행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n\n# flatten(이미지를 한줄로 펼치는 것)\nf1 = torch.nn.Flatten()\nprint(f1(a1(m1(c1(x_tr)))).shape) #16 * 12 * 12 = 2304\n\ntorch.Size([12665, 2304])\n\n\n\n# sigmoid에 올리기 위해서는 2304 디멘젼을 1로 만들어야함\nl1=torch.nn.Linear(in_features=2304,out_features=1) \n\n\n# sigmoid (값이 0에서 1사이의 값, 즉 확률로 출력되도록)\na2 = torch.nn.Sigmoid()\nprint(a2(f1(a1(m1(c1(x_tr))))).shape)\n\ntorch.Size([12665, 2304])\n\n\n\nprint('이미지 사이즈:                     ', x_tr.shape)\nprint('conv:                              ',c1(x_tr).shape)\nprint('(ReLU) -&gt; maxpooling:              ',m1(a1(c1(x_tr))).shape)\nprint('이미지 펼치기:                     ',f1(a1(m1(c1(x_tr)))).shape)\nprint('이미지를 하나의 스칼라로 선형변환: ',l1(f1(a1(m1(c1(x_tr))))).shape)\nprint('시그모이드:                        ',a2(l1(f1(a1(m1(c1(x_tr)))))).shape)\n\n이미지 사이즈:                      torch.Size([12665, 1, 28, 28])\nconv:                               torch.Size([12665, 16, 24, 24])\n(ReLU) -&gt; maxpooling:               torch.Size([12665, 16, 12, 12])\n이미지 펼치기:                      torch.Size([12665, 2304])\n이미지를 하나의 스칼라로 선형변환:  torch.Size([12665, 1])\n시그모이드:                         torch.Size([12665, 1])\n\n\n\n\n\n\n원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\\(\\to\\) overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n# 데이터 로더에 들어갈 데이터세트 준비\nds_tr = torch.utils.data.TensorDataset(x_tr, y_tr)\nds_test = torch.utils.data.TensorDataset(x_test, y_test)\n\n\n데이터 로더는 배치크기를 지정해줘야 함\n\n\nlen(x_tr), len(x_test)\n\n(12665, 2115)\n\n\n\n데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n\nstochastic gradient descent(구용어: mini-batch gradient descent)\n\n\n\n# 데이터 로더\ndl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=1266) \ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=2115) \n\n\ndls = DataLoaders(dl_tr,dl_test)\n\n\nnet = torch.nn.Sequential(\n    # conv layer \n    c1, \n    # ReLU\n    a1, \n    # maxpool\n    m1, \n    # flatten\n    f1, \n    # linear transform (n -&gt; 1)\n    l1,\n    # Sigmoid\n    a2\n)\n\nloss_fn=torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn) # fastai의 Learner는 오미타이저의 기본값이 adam이므로 따로 지정해주지 않아도 됨\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.979273\n0.639250\n00:05\n\n\n1\n0.703008\n0.402466\n00:00\n\n\n2\n0.547401\n0.256881\n00:00\n\n\n3\n0.434025\n0.142217\n00:00\n\n\n4\n0.340996\n0.079636\n00:00\n\n\n5\n0.267902\n0.048050\n00:00\n\n\n6\n0.211895\n0.031742\n00:00\n\n\n7\n0.169176\n0.022921\n00:00\n\n\n8\n0.136331\n0.017658\n00:00\n\n\n9\n0.110770\n0.014233\n00:00\n\n\n\n\n\n\nLearner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음 &gt; python    net[0].weight 위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = ’cuda:0’라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다.\n+ tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n\nnet = net.to('cpu')\nplt.plot(net(x_tr).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(x_test).data,'.')\nplt.title(\"Testing Set\",size=15)\n\nText(0.5, 1.0, 'Testing Set')\n\n\n\n\n\n\n\n\n\n\n\n\nBCEWithLogitsLoss = Sigmoid + BCELoss\n손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음\n\n\n\n\n\n\n다중(k개) 클래스를 분류 - LossFunction: CrossEntrophyLoss\n- ActivationFunction: SoftmaxFunction - 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n\n소프트맥스 함수가 계산하는 과정은 아래와 같음 (3개를 분류할 경우) \\(softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}\\)\n\n\n\n\n\nk개의 클래스를 분류하는 모델의 Loss 계산 방법\n\nsftmax(_netout) # -&gt; 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -&gt; 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -&gt; 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n+ 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임\n+ 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n\n\n\ntype 1) int형을 사용하는 방법 (vector)\ntype 2) float형을 사용하는 방법 (one-hot encoded vector)\n만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n\ntype 1)의 경우 &lt;사슴: 0, 강아지: 1, 고양이: 2&gt; (단, 데이터 형태는는 int(정수))\ntype 2)의 경우 &lt;사슴: [1, 0, 0], 강아지: [0, 1, 0], 고양이: [0, 0, 1] &gt; (단, 데이터의 형태는 float(실수))\n\n\n\n\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n\n\ntorch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n\n\n\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\ntorch.nn.functional.one_hot(yy).float()\n\ntensor([[1., 0.],\n        [1., 0.],\n        [1., 0.],\n        ...,\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])\n\n\n\n\n\n\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\nlen(X) # 18623\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번= 1862 꽉 채워서 10번하고 3개정도 남은 걸로 한 번\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # test는 전부다 넣어서\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\n\nloss_fn = torch.nn.CrossEntropyLoss() # 여기에는 softmax함수가 내장되어 있음 \n#-&gt; net마지막에 softmax를 넣으면 안됨\n# BCEWithLogitsLoss 느낌(시그모이드 + BCE)\n\n\nlrnr = Learner(dls,net,loss_fn) # 옵티마이저는 아답이 디폴트 값이어서 굳이 안넣어도 됨\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.797101\n1.103916\n00:00\n\n\n1\n1.267011\n0.823797\n00:00\n\n\n2\n1.055513\n0.667210\n00:00\n\n\n3\n0.910746\n0.435414\n00:00\n\n\n4\n0.762887\n0.284001\n00:00\n\n\n5\n0.625515\n0.199502\n00:00\n\n\n6\n0.515352\n0.152906\n00:00\n\n\n7\n0.429145\n0.123678\n00:00\n\n\n8\n0.359694\n0.105466\n00:00\n\n\n9\n0.303888\n0.092883\n00:00\n\n\n\n\n\n\nlrnr.model.to(\"cpu\")\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n3142\n-1.138366\n-5.435792\n0.370670\n2\n\n\n3143\n-4.458741\n-4.281343\n2.052410\n2\n\n\n3144\n-2.836508\n-3.204013\n0.012610\n2\n\n\n3145\n-1.704158\n-10.621873\n2.024313\n2\n\n\n3146\n-2.467648\n-4.612999\n0.656284\n2\n\n\n\n\n\n3147 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n975\n0.432969\n-5.653580\n-1.944451\n0\n\n\n976\n2.685695\n-10.254354\n-2.466680\n0\n\n\n977\n2.474842\n-9.650204\n-2.452746\n0\n\n\n978\n1.268743\n-6.928779\n-1.419695\n0\n\n\n979\n4.371464\n-8.959077\n-4.056257\n0\n\n\n\n\n\n980 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼\n\n\n\n\n\n이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n\n\n\n\nfastai에서 지원\nfastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\ny의 형태를 주의해서 사용해야 함\n\n앞서 말한 것처럼 두 가지 타입이 있음\n\nvector + int\none-hot encoded vector + float\n\n\n\ntype 1) y의 형태가 vector + int일 때 - metrics = accuracy를 사용해야 함\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\ny.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\n\n\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nyy.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n1.128365\n0.601474\n0.463357\n0.536643\n00:00\n\n\n1\n0.684630\n0.304262\n0.975414\n0.024586\n00:00\n\n\n2\n0.503124\n0.144147\n0.989598\n0.010402\n00:00\n\n\n3\n0.373899\n0.068306\n0.996217\n0.003783\n00:00\n\n\n4\n0.281332\n0.040790\n0.996217\n0.003783\n00:00\n\n\n5\n0.215743\n0.026980\n0.996690\n0.003310\n00:00\n\n\n6\n0.168349\n0.019467\n0.996690\n0.003310\n00:00\n\n\n7\n0.133313\n0.014856\n0.998109\n0.001891\n00:00\n\n\n8\n0.106776\n0.011745\n0.998109\n0.001891\n00:00\n\n\n9\n0.086275\n0.009553\n0.999054\n0.000946\n00:00\n\n\n\n\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때 - metrics = accuracy_multi를 사용해야 함 - error_rate는 사용못함\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n\n\ny_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\nyy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\ntorch.nn.functional.one_hot() 함수 조건\n\n기본적으로 크기가 n인 벡터가 들어오길 기대\n정수가 들어오는 것을 기대\n\n하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n1.213650\n0.647844\n0.463357\n00:00\n\n\n1\n0.744869\n0.384000\n0.933570\n00:00\n\n\n2\n0.571879\n0.187119\n0.986525\n00:00\n\n\n3\n0.432096\n0.090841\n0.994563\n00:00\n\n\n4\n0.326718\n0.048868\n0.995508\n00:00\n\n\n5\n0.250235\n0.028477\n0.996217\n00:00\n\n\n6\n0.194605\n0.018616\n0.996454\n00:00\n\n\n7\n0.153338\n0.013278\n0.996927\n00:00\n\n\n8\n0.122056\n0.010130\n0.997872\n00:00\n\n\n9\n0.097957\n0.008112\n0.998109\n00:00"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#cnn-구조",
    "href": "docs/DL/posts/DL_CNN.html#cnn-구조",
    "title": "CNN",
    "section": "",
    "text": "간략하게 CNN의 구조를 설명하자면 - conv layer: 커널층(선형변환처럼 feature를 늘려주는 역할) - ReLU layer: 비선형을 추가해 표현력을 늘려주는 역할 - pooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n# (굳이 (2,2) 이런식으로 안해도 됨 어차피 윈도우 사이즈는 정사각행렬이므로 상수하나만 입력해도 됨됨)\n\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv.weight.data = torch.tensor([[[[0.25, 0.25],[0.25,0.25]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]), tensor([0.]))\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임) - 해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음 - 윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함 - 첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용 - 두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n\n\n\n\n사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.\n\\(ReLU(x) = \\max(0,x)\\)\n\n\n\n\n\npooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\nmaxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_maxpooling(_X) \n\ntensor([[[ 6.,  8.],\n         [16., 18.]]])\n\n\n\nmaxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\npooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n\n이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#cnn-구현",
    "href": "docs/DL/posts/DL_CNN.html#cnn-구현",
    "title": "CNN",
    "section": "",
    "text": "path = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02&lt;00:00]"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#path-사용법-데이터-준비",
    "href": "docs/DL/posts/DL_CNN.html#path-사용법-데이터-준비",
    "title": "CNN",
    "section": "",
    "text": "(path/'원하는 경로').ls()"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#위-코드를-사용하면-폴더-안에-있는-데이터들의-이름을-출력",
    "href": "docs/DL/posts/DL_CNN.html#위-코드를-사용하면-폴더-안에-있는-데이터들의-이름을-출력",
    "title": "CNN",
    "section": "",
    "text": "torchvision.io.read_image()\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'training/1').ls()])\nx_tr = torch.concat([x0, x1])/255\n\ny_tr = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\n# train data\nx0 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/0').ls()])\nx1 = torch.stack([torchvision.io.read_image(str(fnames)) for fnames in (path/'testing/1').ls()])\nx_test = torch.concat([x0, x1])/255\n\ny_test = torch.tensor([0.0]*len(x0) + [1.0]*len(x1)).reshape(-1,1)\n\n\nx_tr.shape, y_tr.shape, x_test.shape, y_test.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([2115, 1]))\n\n\n\n\n\n# conv\nc1 = torch.nn.Conv2d(1, 16 , 5) # 만약 color image였다면 입력 채널의 수를 3으로 지정해야 함\nx_tr.shape ,c1(x_tr).shape\n\n(torch.Size([12665, 1, 28, 28]), torch.Size([12665, 16, 24, 24]))\n\n\n\nsize계산 공식: 윈도우(커널)사이즈가 n이면 \\(size = height(width) - ( n - 1)\\)\n\n\n# ReLU\na1 = torch.nn.ReLU()\n\n\n# maxpooling\nm1 = torch.nn.MaxPool2d(2)\nprint(m1(a1(c1(x_tr))).shape)\n\ntorch.Size([12665, 16, 12, 12])\n\n\n\n행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n\n# flatten(이미지를 한줄로 펼치는 것)\nf1 = torch.nn.Flatten()\nprint(f1(a1(m1(c1(x_tr)))).shape) #16 * 12 * 12 = 2304\n\ntorch.Size([12665, 2304])\n\n\n\n# sigmoid에 올리기 위해서는 2304 디멘젼을 1로 만들어야함\nl1=torch.nn.Linear(in_features=2304,out_features=1) \n\n\n# sigmoid (값이 0에서 1사이의 값, 즉 확률로 출력되도록)\na2 = torch.nn.Sigmoid()\nprint(a2(f1(a1(m1(c1(x_tr))))).shape)\n\ntorch.Size([12665, 2304])\n\n\n\nprint('이미지 사이즈:                     ', x_tr.shape)\nprint('conv:                              ',c1(x_tr).shape)\nprint('(ReLU) -&gt; maxpooling:              ',m1(a1(c1(x_tr))).shape)\nprint('이미지 펼치기:                     ',f1(a1(m1(c1(x_tr)))).shape)\nprint('이미지를 하나의 스칼라로 선형변환: ',l1(f1(a1(m1(c1(x_tr))))).shape)\nprint('시그모이드:                        ',a2(l1(f1(a1(m1(c1(x_tr)))))).shape)\n\n이미지 사이즈:                      torch.Size([12665, 1, 28, 28])\nconv:                               torch.Size([12665, 16, 24, 24])\n(ReLU) -&gt; maxpooling:               torch.Size([12665, 16, 12, 12])\n이미지 펼치기:                      torch.Size([12665, 2304])\n이미지를 하나의 스칼라로 선형변환:  torch.Size([12665, 1])\n시그모이드:                         torch.Size([12665, 1])\n\n\n\n\n\n\n원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\\(\\to\\) overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n# 데이터 로더에 들어갈 데이터세트 준비\nds_tr = torch.utils.data.TensorDataset(x_tr, y_tr)\nds_test = torch.utils.data.TensorDataset(x_test, y_test)\n\n\n데이터 로더는 배치크기를 지정해줘야 함\n\n\nlen(x_tr), len(x_test)\n\n(12665, 2115)\n\n\n\n데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n\nstochastic gradient descent(구용어: mini-batch gradient descent)\n\n\n\n# 데이터 로더\ndl_tr = torch.utils.data.DataLoader(ds_tr,batch_size=1266) \ndl_test = torch.utils.data.DataLoader(ds_test,batch_size=2115) \n\n\ndls = DataLoaders(dl_tr,dl_test)\n\n\nnet = torch.nn.Sequential(\n    # conv layer \n    c1, \n    # ReLU\n    a1, \n    # maxpool\n    m1, \n    # flatten\n    f1, \n    # linear transform (n -&gt; 1)\n    l1,\n    # Sigmoid\n    a2\n)\n\nloss_fn=torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn) # fastai의 Learner는 오미타이저의 기본값이 adam이므로 따로 지정해주지 않아도 됨\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.979273\n0.639250\n00:05\n\n\n1\n0.703008\n0.402466\n00:00\n\n\n2\n0.547401\n0.256881\n00:00\n\n\n3\n0.434025\n0.142217\n00:00\n\n\n4\n0.340996\n0.079636\n00:00\n\n\n5\n0.267902\n0.048050\n00:00\n\n\n6\n0.211895\n0.031742\n00:00\n\n\n7\n0.169176\n0.022921\n00:00\n\n\n8\n0.136331\n0.017658\n00:00\n\n\n9\n0.110770\n0.014233\n00:00\n\n\n\n\n\n\nLearner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음 &gt; python    net[0].weight 위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = ’cuda:0’라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다.\n+ tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n\nnet = net.to('cpu')\nplt.plot(net(x_tr).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(x_test).data,'.')\nplt.title(\"Testing Set\",size=15)\n\nText(0.5, 1.0, 'Testing Set')"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#loss-function",
    "href": "docs/DL/posts/DL_CNN.html#loss-function",
    "title": "CNN",
    "section": "",
    "text": "BCEWithLogitsLoss = Sigmoid + BCELoss\n손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN.html#k개의-클래스를-분류하는-모델",
    "href": "docs/DL/posts/DL_CNN.html#k개의-클래스를-분류하는-모델",
    "title": "CNN",
    "section": "",
    "text": "다중(k개) 클래스를 분류 - LossFunction: CrossEntrophyLoss\n- ActivationFunction: SoftmaxFunction - 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n\n소프트맥스 함수가 계산하는 과정은 아래와 같음 (3개를 분류할 경우) \\(softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}\\)\n\n\n\n\n\nk개의 클래스를 분류하는 모델의 Loss 계산 방법\n\nsftmax(_netout) # -&gt; 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -&gt; 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -&gt; 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n+ 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임\n+ 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n\n\n\ntype 1) int형을 사용하는 방법 (vector)\ntype 2) float형을 사용하는 방법 (one-hot encoded vector)\n만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n\ntype 1)의 경우 &lt;사슴: 0, 강아지: 1, 고양이: 2&gt; (단, 데이터 형태는는 int(정수))\ntype 2)의 경우 &lt;사슴: [1, 0, 0], 강아지: [0, 1, 0], 고양이: [0, 0, 1] &gt; (단, 데이터의 형태는 float(실수))\n\n\n\n\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n\n\ntorch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n\n\n\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\ntorch.nn.functional.one_hot(yy).float()\n\ntensor([[1., 0.],\n        [1., 0.],\n        [1., 0.],\n        ...,\n        [0., 1.],\n        [0., 1.],\n        [0., 1.]])\n\n\n\n\n\n\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\nlen(X) # 18623\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번= 1862 꽉 채워서 10번하고 3개정도 남은 걸로 한 번\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # test는 전부다 넣어서\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\n\nloss_fn = torch.nn.CrossEntropyLoss() # 여기에는 softmax함수가 내장되어 있음 \n#-&gt; net마지막에 softmax를 넣으면 안됨\n# BCEWithLogitsLoss 느낌(시그모이드 + BCE)\n\n\nlrnr = Learner(dls,net,loss_fn) # 옵티마이저는 아답이 디폴트 값이어서 굳이 안넣어도 됨\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.797101\n1.103916\n00:00\n\n\n1\n1.267011\n0.823797\n00:00\n\n\n2\n1.055513\n0.667210\n00:00\n\n\n3\n0.910746\n0.435414\n00:00\n\n\n4\n0.762887\n0.284001\n00:00\n\n\n5\n0.625515\n0.199502\n00:00\n\n\n6\n0.515352\n0.152906\n00:00\n\n\n7\n0.429145\n0.123678\n00:00\n\n\n8\n0.359694\n0.105466\n00:00\n\n\n9\n0.303888\n0.092883\n00:00\n\n\n\n\n\n\nlrnr.model.to(\"cpu\")\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n3142\n-1.138366\n-5.435792\n0.370670\n2\n\n\n3143\n-4.458741\n-4.281343\n2.052410\n2\n\n\n3144\n-2.836508\n-3.204013\n0.012610\n2\n\n\n3145\n-1.704158\n-10.621873\n2.024313\n2\n\n\n3146\n-2.467648\n-4.612999\n0.656284\n2\n\n\n\n\n\n3147 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.981043\n-9.135092\n-1.149270\n0\n\n\n1\n-0.292905\n-4.281692\n-0.924575\n0\n\n\n2\n4.085316\n-9.199694\n-3.482234\n0\n\n\n3\n2.484926\n-9.336347\n-3.127304\n0\n\n\n4\n3.310040\n-12.257785\n-2.177761\n0\n\n\n...\n...\n...\n...\n...\n\n\n975\n0.432969\n-5.653580\n-1.944451\n0\n\n\n976\n2.685695\n-10.254354\n-2.466680\n0\n\n\n977\n2.474842\n-9.650204\n-2.452746\n0\n\n\n978\n1.268743\n-6.928779\n-1.419695\n0\n\n\n979\n4.371464\n-8.959077\n-4.056257\n0\n\n\n\n\n\n980 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼\n\n\n\n\n\n이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n\n\n\n\nfastai에서 지원\nfastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\ny의 형태를 주의해서 사용해야 함\n\n앞서 말한 것처럼 두 가지 타입이 있음\n\nvector + int\none-hot encoded vector + float\n\n\n\ntype 1) y의 형태가 vector + int일 때 - metrics = accuracy를 사용해야 함\n\n# train\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\ny.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\n# test\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\n\n\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\nyy.to(torch.int64).reshape(-1)\n\ntensor([0, 0, 0,  ..., 1, 1, 1])\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n1.128365\n0.601474\n0.463357\n0.536643\n00:00\n\n\n1\n0.684630\n0.304262\n0.975414\n0.024586\n00:00\n\n\n2\n0.503124\n0.144147\n0.989598\n0.010402\n00:00\n\n\n3\n0.373899\n0.068306\n0.996217\n0.003783\n00:00\n\n\n4\n0.281332\n0.040790\n0.996217\n0.003783\n00:00\n\n\n5\n0.215743\n0.026980\n0.996690\n0.003310\n00:00\n\n\n6\n0.168349\n0.019467\n0.996690\n0.003310\n00:00\n\n\n7\n0.133313\n0.014856\n0.998109\n0.001891\n00:00\n\n\n8\n0.106776\n0.011745\n0.998109\n0.001891\n00:00\n\n\n9\n0.086275\n0.009553\n0.999054\n0.000946\n00:00\n\n\n\n\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때 - metrics = accuracy_multi를 사용해야 함 - error_rate는 사용못함\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n\n\ny_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\nyy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\ntorch.nn.functional.one_hot() 함수 조건\n\n기본적으로 크기가 n인 벡터가 들어오길 기대\n정수가 들어오는 것을 기대\n\n하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n1.213650\n0.647844\n0.463357\n00:00\n\n\n1\n0.744869\n0.384000\n0.933570\n00:00\n\n\n2\n0.571879\n0.187119\n0.986525\n00:00\n\n\n3\n0.432096\n0.090841\n0.994563\n00:00\n\n\n4\n0.326718\n0.048868\n0.995508\n00:00\n\n\n5\n0.250235\n0.028477\n0.996217\n00:00\n\n\n6\n0.194605\n0.018616\n0.996454\n00:00\n\n\n7\n0.153338\n0.013278\n0.996927\n00:00\n\n\n8\n0.122056\n0.010130\n0.997872\n00:00\n\n\n9\n0.097957\n0.008112\n0.998109\n00:00"
  },
  {
    "objectID": "docs/DL/posts/rstat101.html",
    "href": "docs/DL/posts/rstat101.html",
    "title": "Tutorial",
    "section": "",
    "text": "test12414입니\n123123다"
  },
  {
    "objectID": "docs/DL/rstat_lecture.html",
    "href": "docs/DL/rstat_lecture.html",
    "title": "rstat lecture",
    "section": "",
    "text": "1강\n2강\n3강"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "JuWon Kwon",
    "section": "",
    "text": "바이오메디컬공학을 전공하고 있는 학부생 입니다.\n초음파와 딥러닝을 공부하고 있습니다."
  },
  {
    "objectID": "docs/DL/posts/DL_CNN_2(XAI_정리).html",
    "href": "docs/DL/posts/DL_CNN_2(XAI_정리).html",
    "title": "CNN",
    "section": "",
    "text": "toc:true\n\n\nimport torch \nimport torchvision\nfrom fastai.vision.all import * \n\n\n\n\n누군가가 만들어 놓은 모델의 구조를 사용하는 것\nfine_tune을 사용하면 전이학습이 되어 학습시간이 줄어들음\nlrnr.fit을 사용하면 그냥 학습을 시키는 것\n\n\n\n\npath = untar_data(URLs.CIFAR)\n\n\npath.ls() # path에는 labels라는 텍스트 파일과 test,train이라는 폴더가 있음\n\n(#3) [Path('/root/.fastai/data/cifar10/labels.txt'),Path('/root/.fastai/data/cifar10/test'),Path('/root/.fastai/data/cifar10/train')]\n\n\n\n!ls /root/.fastai/data/cifar10/train\n\nairplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n\n\n\ntrain 폴더 안에는 위위와 같은 폴더들이 있음\n각 폴더에는 이름과 맞는 이미지 파일이 있음\n10개의 클래스\n앞 장에서 만든 CNN Architecture를 사용하면 정확도가 매우 떨어짐\n\n학습과정은 패스했음\n\n더 복잡하고 정교한 모델을 만들어야 함 \\(\\to\\) transfer learning 사용\n\n\n\n\n\nResNet의 weights를 그대로 가져와 학습에 사용\n\n\n\n\ndls = ImageDataLoaders.from_folder(path,train='train',valid='test')\n\n\n_x, _y = dls.one_batch()\n_x.shape, _y.shape\n\n(torch.Size([64, 3, 32, 32]), torch.Size([64]))\n\n\n\nx의 배치크기는 64(= 하나의 배치에 64개의 데이터(이미지)가 있음을 의미), 채널은 3(컬러 이미지), size는 32 x 32이다.\ny는 x의 레이블로 64개의 이미지가 있으므로 64개의 값이 y에 저장됨\n\n\n_y\n\nTensorCategory([5, 2, 8, 6, 8, 5, 1, 2, 1, 0, 4, 8, 7, 7, 0, 4, 2, 6, 8, 6, 6,\n                2, 0, 7, 1, 9, 5, 4, 3, 6, 4, 9, 6, 4, 3, 7, 3, 9, 3, 6, 5, 3,\n                6, 6, 6, 8, 6, 3, 5, 3, 5, 9, 5, 3, 9, 0, 7, 3, 6, 6, 7, 7, 8,\n                9], device='cuda:0')\n\n\n\n#collapse_output\nnet = torchvision.models.resnet18(weights=torchvision.models.resnet.ResNet18_Weights.IMAGENET1K_V1)\nnet\n\n\n마지막 출력층을 확인해보면 1000개를 출력함\n하지만 우리는 10개의 클래스를 구분하는 모델을 만들 것이므로 1000을 10으로 바꿔야 함\n\n\nnet.fc = torch.nn.Linear(in_features=512, out_features=10) \n\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\nlrnr = Learner(dls, net, loss_fn, metrics = accuracy)\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.804860\n0.821773\n0.724900\n01:15\n\n\n1\n0.640089\n0.679617\n0.773700\n01:09\n\n\n2\n0.519974\n0.647182\n0.783100\n01:05\n\n\n3\n0.405207\n0.564490\n0.811400\n01:14\n\n\n4\n0.344672\n0.683868\n0.783300\n01:05\n\n\n5\n0.269288\n0.737170\n0.785200\n01:12\n\n\n6\n0.272949\n0.788109\n0.769800\n01:12\n\n\n7\n0.188042\n0.690548\n0.808600\n01:10\n\n\n8\n0.175680\n0.736700\n0.800500\n01:09\n\n\n9\n0.151409\n0.821169\n0.795500\n01:06\n\n\n\n\n\n\n\n\n\noverview에서 fastai를 사용한 방법 (fastai로만 모델을 구현)\n\n\npath = untar_data(URLs.PETS)/'images'\n\nfiles= get_image_files(path)\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:08&lt;00:00]\n    \n    \n\n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.175012\n0.026964\n0.989851\n01:54\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.047284\n0.022501\n0.994587\n02:29\n\n\n\n\n\n\n아래의 코드를 사용하면 network의 구조를 볼 수 있음\n\nlrnr.model()\n\n\n\n\n\n\n# 위의 코드(fastai로 transfer learning) 똑같음\npath = untar_data(URLs.PETS)/'images'\n\nfiles= get_image_files(path)\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:04&lt;00:00]\n    \n    \n\n\n\npath.ls()\n\n(#7393) [Path('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_7.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_97.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/chihuahua_135.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/keeshond_110.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_54.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/wheaten_terrier_11.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/British_Shorthair_62.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/german_shorthaired_171.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/havanese_84.jpg')...]\n\n\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg')\nximg\n\n\n\n\n\nx= first(dls.test_dl([ximg]))[0]\nx.shape # 위 이미지의 shape\n\ntorch.Size([1, 3, 512, 512])\n\n\n\ntorch.nn.AdaptiveAvgPool2d()\n\npooling을 평균으로 하는 것으로 output_size = 1이라면 채널당 하나의 값을 출력해줌\n위의 이미지는 채널이 3개이므로 이미지 하나당 3개의 값을 출력\n\n\n\nap = torch.nn.AdaptiveAvgPool2d(output_size=1) # output size가 1이라는 것은 (1x1)로 출력하겠다는 뜻\n# 만약 2이면 출력이 (2 x 2) 즉 채널당 4개의 값이 나오는 것임\nap(x)\n\nTensorImage([[[[0.6086]],\n\n              [[0.5648]],\n\n              [[0.5058]]]], device='cuda:0')\n\n\n\n\n\n데이터 로더를 사용해 불러온 이미지 데이트의 차원을 확인해보면 [1, 3, 512, 512]이다. 이는 배치사이즈, 채널, 높이, 너비 순으로 되어있는 것임\n이러한 데이터를 높이, 너비, 채널 순으로 바꾸고 싶음 \\(\\to\\) torch.einsum()사용\n\ntorch.einsum('ij,jk-&gt;ik',tensor1, tensor2) # 크기가 i x j인 tensor1과 j x k인 tensor2의 행렬 곱을 해줌\n\ntorch.Size([1, 3, 512, 512]) \\(\\to\\) torch.Size([512, 512, 3]) 형태로 만들고 싶음\n\n→ tensor의 차원을 변경하거나 계산을 할 때에는 torch.einsum()함수를 사용하면 좋음\n\nx_new = torch.einsum('ocij -&gt; ijc',x.to('cpu'))\n\n\nplt는 높이 x 너비 x 채널 순으로 데이터가 입력되길 기대\n만약 x 즉, 채널 x 높이 x 너비 순으로 되어있는 데이터를 입력하면 에러가 발생함\n\n\nplt.imshow(x_new)\n\n&lt;matplotlib.image.AxesImage at 0x7fb7e874cf10&gt;\n\n\n\n\n\n\n\n\n\n\nCAM(Class Activation Mapping): CNN의 판단근거를 시각화하는 기술\n\n\n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.208073\n0.010794\n0.995940\n01:55\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.047890\n0.002702\n0.998647\n02:25\n\n\n\n\n\n\nlrnr.model 함수를 사용하면 resnet34의 구조를 볼 수 있음\n구조를 확인해보면 resnet34는 두 개의 모델이 합쳐져 있음\n첫 번째 모델은 그대로 사용하고 두 번째 모델만 약간 수정하여 사용해보자\n\n\n\n\n\nnet1 = lrnr.model[0]\nnet2 = lrnr.model[1]\n\n\nnet1.to('cpu')\nnet2.to('cpu')\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): fastai.layers.Flatten(full=False)\n  (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=1024, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\nBatchNorm1d과 Dropout, ReLU 층은 파라미터 개수에 변화를 주지 않음\npooling층과 Linear층을 주목해야 함\n\n\n_X, _y = dls.one_batch() \n\n\n_X = _X.to('cpu')\n# net1(_X).shape #torch.Size([64, 512, 16, 16]) 이러한 형태로 출력됨\n\n\nnet2= torch.nn.Sequential(\n    torch.nn.AdaptiveAvgPool2d(output_size=1), # (64,512,16,16) -&gt; (64,512,1,1) \n    torch.nn.Flatten(), # (64,512,1,1) -&gt; (64,512) \n    torch.nn.Linear(512,2,bias=False) # (64,512) -&gt; (64,2) \n)\n\n\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\n\nlrnr2= Learner(dls,net,metrics=accuracy)\n\nlrnr2.fine_tune(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.232032\n1.522452\n0.836265\n02:25\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.134071\n0.816453\n0.830176\n02:24\n\n\n1\n0.138622\n0.150047\n0.936401\n02:25\n\n\n2\n0.095163\n0.100338\n0.960081\n02:24\n\n\n3\n0.051146\n0.039767\n0.984438\n02:28\n\n\n4\n0.024118\n0.044908\n0.982409\n02:30\n\n\n\n\n\n\n\n\n- net2의 순서 바꾸기 전 전체 네트워크:\n\\[\\underset{(1,3,512,512)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{ap}{\\to} \\underset{(1,512,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,512)}{{\\boldsymbol \\sharp}}\\overset{linear}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [-9.0358,  9.0926]\\]\n\n위의 수식을 아래의 수식으로 바꾸고 싶음\n\n순서만 바꾸는 것임 -&gt; 결과는 똑같음\n\n\n\\[\\underset{(1,3,224,224)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{linear}{\\to} \\underset{(1,2,16,16)}{{\\bf why}}\\overset{ap}{\\to} \\underset{(1,2,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [−9.0358,9.0926]\\]\n\n여기서 주목해야 하는 것은 \\(why\\)\n\\(why\\)의 값들을 평균을 내고 이 값을 선형변환하여 하나의 값을 만듦 [−9.0358 or 9.0926]\n\\(why\\)의 tensor를 확인해보자!\n바꾸기 전 net2의 구조 중 일부분 (배치를 넣는 것이 아니라 하나의 이미지를 net에 넣을때임)\n\nX = net1(x)\n\nX.shape = (1, 512, 16, 16)\n\nXX = torch.nn.AdaptiveAvgPool2d(X)\n\nXX.shape =(1, 512, 1, 1)\n\nXXX= torch.nn.Flatten(XX)\n\nXXX.shape -&gt; (1,512)\n\nXXX @ net2[2].weight \\(\\to\\) shape(1, 2)\n\nnet2[2].weight.shape -&gt; (512, 2)\n\n\n\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg')\nx = first(dls.test_dl([ximg]))[0]\n\n\n# 위의 정리된 것들을 보면\n# net2[2].weight의 크기는 (512, 2)\n# net1(x)의 크기는 (1, 512, 16, 16)\n# why의 크기를 (1, 2, 16, 16)으로 만들고 싶음\nwhy = torch.einsum('cb,abij-&gt;acij',\n                   net2[2].weight,#(512, 2)\n                   net1(x) # (1, 512, 16, 16)\n                   )\nwhy.shape\n\ntorch.Size([1, 2, 16, 16])\n\n\n\n# net2[0]은 avg pooling layer -&gt; 높이와 폭을 0으로 만듦\n# (1, 2, 16, 16) -&gt; (1, 2, 1, 1)\nnet2[0](why)\n\nTensorImage([[[[-7.2395]],\n\n              [[ 7.5647]]]], device='cuda:0', grad_fn=&lt;AliasBackward0&gt;)\n\n\n\n\n\n\n\ntensor의 평균 -9.0358\n해당 tensor의 평균이 크다면 고양이라고 판단을 함\n\n하지만 이번 예시에서는 평균이 음수가 나옴 -&gt; 해당 값이 작으면 작을수록 고양이가 아니라고 생각하는 근거가 됨\n\n그러므로 해당 tensor에서 엄청 작은 값들은 고양이가 아니라고 판단하는 근거가 되는 점들임\n\n\nwhy[0,0,:,:].to(torch.int64)\n\nTensorImage([[  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    1,    1,    0,    1,    1,    1,    1,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    1,    1,    0,    0,   -1,   -2,   -2,\n                -1,    0,    2,    2,    1],\n             [   0,    0,    0,    0,    0,    0,   -2,  -10,  -17,  -18,  -17,\n               -14,   -7,    2,    5,    1],\n             [  -1,    0,    0,    0,    0,    0,   -7,  -26,  -45,  -43,  -38,\n               -34,  -16,    0,    3,    0],\n             [  -1,    0,    0,   -1,   -2,   -3,  -10,  -32,  -56,  -58,  -50,\n               -43,  -22,   -2,    1,    1],\n             [   1,    2,    0,   -6,  -14,  -12,  -11,  -25,  -45,  -66,  -71,\n               -49,  -20,    0,    2,    2],\n             [   8,   10,    2,   -9,  -19,  -18,  -10,  -13,  -34,  -74, -100,\n               -65,  -21,    1,    1,    0],\n             [  11,   12,    2,   -6,  -13,  -13,   -7,   -5,  -23,  -60,  -82,\n               -63,  -21,    0,    0,    0],\n             [   7,    6,    1,    0,   -2,   -4,   -3,   -1,  -12,  -32,  -45,\n               -36,  -14,   -1,    0,   -2],\n             [   0,    1,    0,    1,    2,    0,    0,    0,   -1,   -7,  -12,\n               -11,   -5,   -1,   -1,   -2],\n             [  -2,   -1,   -1,   -1,   -1,   -1,    0,    1,    4,    1,   -1,\n                -1,   -2,   -2,   -2,   -3],\n             [  -4,   -4,   -4,   -5,   -4,   -4,   -4,   -2,   -2,   -3,   -4,\n                -5,   -5,   -5,   -5,   -4],\n             [  -5,   -6,   -6,   -6,   -6,   -6,   -5,   -5,   -5,   -5,   -6,\n                -6,   -6,   -6,   -5,   -5]], device='cuda:0')\n\n\n\n\n\n\ntensor의 평균 9.0926\n해당 tensor는 강아지라고 판단하는 tensor라고 생각하면 됨\n그러므로 해당 tensor에서 엄청 작은 값들은 고양이가 아니라고 판단하는 근거가 되는 점들임\n\n\nwhy[0,1,:,:].to(torch.int64)\n\nTensorImage([[ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 2,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  1,  2,  2,  2,  0, -1, -1,  0],\n             [ 0,  0,  0,  0,  0,  0,  2, 10, 16, 18, 17, 14,  7, -1, -3, -1],\n             [ 1,  1,  0,  0,  0,  0,  6, 25, 43, 42, 37, 32, 16,  1, -2,  0],\n             [ 1,  0,  0,  1,  2,  3, 10, 32, 55, 58, 50, 42, 21,  3,  0,  0],\n             [ 0, -1,  0,  6, 13, 12, 11, 24, 44, 65, 69, 49, 20,  1, -1, -1],\n             [-7, -8, -1,  9, 18, 17, 10, 13, 33, 72, 97, 65, 22,  0,  0,  0],\n             [-9, -9, -2,  5, 12, 13,  7,  5, 22, 58, 81, 62, 21,  1,  0,  1],\n             [-6, -6, -1,  1,  2,  4,  3,  1, 11, 31, 43, 34, 14,  1,  1,  2],\n             [ 0,  0,  0,  0,  0,  0,  1,  0,  1,  7, 12, 10,  5,  1,  1,  3],\n             [ 3,  2,  2,  2,  2,  1,  1,  0, -2,  0,  1,  2,  2,  3,  3,  4],\n             [ 5,  6,  6,  6,  6,  6,  5,  3,  4,  5,  5,  6,  6,  6,  6,  6],\n             [ 5,  6,  7,  7,  7,  7,  6,  6,  6,  6,  6,  7,  7,  7,  6,  6]],\n            device='cuda:0')\n\n\n\n\n\n\n\n\nwhy의 값과 이미지를 동시에 출력한다면 그림의 어떠한 부분을 보고 강아지라고 판단을 한 것인지 알 수 있음\n\n\nwhy_cat = why[0,0,:,:] # 해당 tensor의 값이 작으면 작을 수록 고양이가 아니라고 판단하는 근거가 됨\nwhy_dog = why[0,1,:,:] # 해당 tensor의 값이 크면 클수록 강아지라고 판단하는 근거가 됨\n\n\n밑에서 cmap을 magma로 지정하였는 이는 값이 작은 부분은 검정색 큰 부분은 노란색으로 표시함\n\n검정 -&gt; 보라 -&gt; 빨강 -&gt; 노랑\n\n이를 해석하면 why_cat에서 값이 가장 작은 부분, 즉 검정색이 고양이가 아니라고 판단함\nwhy_dog에서는 값이 가장 큰 부분, 즉 노란색이 강아지라고 판단하는 영역을 표시함\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma')\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc99c0810&gt;\n\n\n\n\n\n\nwhy의 크기 조절\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc90e9410&gt;\n\n\n\n\n\n\n크기조절 후 겹쳐그리기\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc8df78d0&gt;\n\n\n\n\n\n\n\n\n# why값을 확률로 출력하기\nsftmax=torch.nn.Softmax(dim=1)\nsftmax(net(x))\ncatprob, dogprob = sftmax(net(x))[0,0].item(), sftmax(net(x))[0,1].item()\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[0].set_title('catprob= %f' % catprob) \nax[1].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].set_title('dogprob=%f' % dogprob)\n\nText(0.5, 1.0, 'dogprob=1.000000')\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=0 \nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -&gt; acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob&gt;dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN_2(XAI_정리).html#transfer-learning",
    "href": "docs/DL/posts/DL_CNN_2(XAI_정리).html#transfer-learning",
    "title": "CNN",
    "section": "",
    "text": "누군가가 만들어 놓은 모델의 구조를 사용하는 것\nfine_tune을 사용하면 전이학습이 되어 학습시간이 줄어들음\nlrnr.fit을 사용하면 그냥 학습을 시키는 것\n\n\n\n\npath = untar_data(URLs.CIFAR)\n\n\npath.ls() # path에는 labels라는 텍스트 파일과 test,train이라는 폴더가 있음\n\n(#3) [Path('/root/.fastai/data/cifar10/labels.txt'),Path('/root/.fastai/data/cifar10/test'),Path('/root/.fastai/data/cifar10/train')]\n\n\n\n!ls /root/.fastai/data/cifar10/train\n\nairplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n\n\n\ntrain 폴더 안에는 위위와 같은 폴더들이 있음\n각 폴더에는 이름과 맞는 이미지 파일이 있음\n10개의 클래스\n앞 장에서 만든 CNN Architecture를 사용하면 정확도가 매우 떨어짐\n\n학습과정은 패스했음\n\n더 복잡하고 정교한 모델을 만들어야 함 \\(\\to\\) transfer learning 사용\n\n\n\n\n\nResNet의 weights를 그대로 가져와 학습에 사용\n\n\n\n\ndls = ImageDataLoaders.from_folder(path,train='train',valid='test')\n\n\n_x, _y = dls.one_batch()\n_x.shape, _y.shape\n\n(torch.Size([64, 3, 32, 32]), torch.Size([64]))\n\n\n\nx의 배치크기는 64(= 하나의 배치에 64개의 데이터(이미지)가 있음을 의미), 채널은 3(컬러 이미지), size는 32 x 32이다.\ny는 x의 레이블로 64개의 이미지가 있으므로 64개의 값이 y에 저장됨\n\n\n_y\n\nTensorCategory([5, 2, 8, 6, 8, 5, 1, 2, 1, 0, 4, 8, 7, 7, 0, 4, 2, 6, 8, 6, 6,\n                2, 0, 7, 1, 9, 5, 4, 3, 6, 4, 9, 6, 4, 3, 7, 3, 9, 3, 6, 5, 3,\n                6, 6, 6, 8, 6, 3, 5, 3, 5, 9, 5, 3, 9, 0, 7, 3, 6, 6, 7, 7, 8,\n                9], device='cuda:0')\n\n\n\n#collapse_output\nnet = torchvision.models.resnet18(weights=torchvision.models.resnet.ResNet18_Weights.IMAGENET1K_V1)\nnet\n\n\n마지막 출력층을 확인해보면 1000개를 출력함\n하지만 우리는 10개의 클래스를 구분하는 모델을 만들 것이므로 1000을 10으로 바꿔야 함\n\n\nnet.fc = torch.nn.Linear(in_features=512, out_features=10) \n\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\nlrnr = Learner(dls, net, loss_fn, metrics = accuracy)\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.804860\n0.821773\n0.724900\n01:15\n\n\n1\n0.640089\n0.679617\n0.773700\n01:09\n\n\n2\n0.519974\n0.647182\n0.783100\n01:05\n\n\n3\n0.405207\n0.564490\n0.811400\n01:14\n\n\n4\n0.344672\n0.683868\n0.783300\n01:05\n\n\n5\n0.269288\n0.737170\n0.785200\n01:12\n\n\n6\n0.272949\n0.788109\n0.769800\n01:12\n\n\n7\n0.188042\n0.690548\n0.808600\n01:10\n\n\n8\n0.175680\n0.736700\n0.800500\n01:09\n\n\n9\n0.151409\n0.821169\n0.795500\n01:06\n\n\n\n\n\n\n\n\n\noverview에서 fastai를 사용한 방법 (fastai로만 모델을 구현)\n\n\npath = untar_data(URLs.PETS)/'images'\n\nfiles= get_image_files(path)\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:08&lt;00:00]\n    \n    \n\n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.175012\n0.026964\n0.989851\n01:54\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.047284\n0.022501\n0.994587\n02:29\n\n\n\n\n\n\n아래의 코드를 사용하면 network의 구조를 볼 수 있음\n\nlrnr.model()"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN_2(XAI_정리).html#데이터-확인",
    "href": "docs/DL/posts/DL_CNN_2(XAI_정리).html#데이터-확인",
    "title": "CNN",
    "section": "",
    "text": "# 위의 코드(fastai로 transfer learning) 똑같음\npath = untar_data(URLs.PETS)/'images'\n\nfiles= get_image_files(path)\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:04&lt;00:00]\n    \n    \n\n\n\npath.ls()\n\n(#7393) [Path('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_7.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_97.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/chihuahua_135.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/keeshond_110.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_54.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/wheaten_terrier_11.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/British_Shorthair_62.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/german_shorthaired_171.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/havanese_84.jpg')...]\n\n\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg')\nximg\n\n\n\n\n\nx= first(dls.test_dl([ximg]))[0]\nx.shape # 위 이미지의 shape\n\ntorch.Size([1, 3, 512, 512])\n\n\n\ntorch.nn.AdaptiveAvgPool2d()\n\npooling을 평균으로 하는 것으로 output_size = 1이라면 채널당 하나의 값을 출력해줌\n위의 이미지는 채널이 3개이므로 이미지 하나당 3개의 값을 출력\n\n\n\nap = torch.nn.AdaptiveAvgPool2d(output_size=1) # output size가 1이라는 것은 (1x1)로 출력하겠다는 뜻\n# 만약 2이면 출력이 (2 x 2) 즉 채널당 4개의 값이 나오는 것임\nap(x)\n\nTensorImage([[[[0.6086]],\n\n              [[0.5648]],\n\n              [[0.5058]]]], device='cuda:0')\n\n\n\n\n\n데이터 로더를 사용해 불러온 이미지 데이트의 차원을 확인해보면 [1, 3, 512, 512]이다. 이는 배치사이즈, 채널, 높이, 너비 순으로 되어있는 것임\n이러한 데이터를 높이, 너비, 채널 순으로 바꾸고 싶음 \\(\\to\\) torch.einsum()사용\n\ntorch.einsum('ij,jk-&gt;ik',tensor1, tensor2) # 크기가 i x j인 tensor1과 j x k인 tensor2의 행렬 곱을 해줌\n\ntorch.Size([1, 3, 512, 512]) \\(\\to\\) torch.Size([512, 512, 3]) 형태로 만들고 싶음\n\n→ tensor의 차원을 변경하거나 계산을 할 때에는 torch.einsum()함수를 사용하면 좋음\n\nx_new = torch.einsum('ocij -&gt; ijc',x.to('cpu'))\n\n\nplt는 높이 x 너비 x 채널 순으로 데이터가 입력되길 기대\n만약 x 즉, 채널 x 높이 x 너비 순으로 되어있는 데이터를 입력하면 에러가 발생함\n\n\nplt.imshow(x_new)\n\n&lt;matplotlib.image.AxesImage at 0x7fb7e874cf10&gt;"
  },
  {
    "objectID": "docs/DL/posts/DL_CNN_2(XAI_정리).html#cam-구현",
    "href": "docs/DL/posts/DL_CNN_2(XAI_정리).html#cam-구현",
    "title": "CNN",
    "section": "",
    "text": "CAM(Class Activation Mapping): CNN의 판단근거를 시각화하는 기술\n\n\n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.208073\n0.010794\n0.995940\n01:55\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.047890\n0.002702\n0.998647\n02:25\n\n\n\n\n\n\nlrnr.model 함수를 사용하면 resnet34의 구조를 볼 수 있음\n구조를 확인해보면 resnet34는 두 개의 모델이 합쳐져 있음\n첫 번째 모델은 그대로 사용하고 두 번째 모델만 약간 수정하여 사용해보자\n\n\n\n\n\nnet1 = lrnr.model[0]\nnet2 = lrnr.model[1]\n\n\nnet1.to('cpu')\nnet2.to('cpu')\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): fastai.layers.Flatten(full=False)\n  (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=1024, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\nBatchNorm1d과 Dropout, ReLU 층은 파라미터 개수에 변화를 주지 않음\npooling층과 Linear층을 주목해야 함\n\n\n_X, _y = dls.one_batch() \n\n\n_X = _X.to('cpu')\n# net1(_X).shape #torch.Size([64, 512, 16, 16]) 이러한 형태로 출력됨\n\n\nnet2= torch.nn.Sequential(\n    torch.nn.AdaptiveAvgPool2d(output_size=1), # (64,512,16,16) -&gt; (64,512,1,1) \n    torch.nn.Flatten(), # (64,512,1,1) -&gt; (64,512) \n    torch.nn.Linear(512,2,bias=False) # (64,512) -&gt; (64,2) \n)\n\n\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\n\nlrnr2= Learner(dls,net,metrics=accuracy)\n\nlrnr2.fine_tune(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.232032\n1.522452\n0.836265\n02:25\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.134071\n0.816453\n0.830176\n02:24\n\n\n1\n0.138622\n0.150047\n0.936401\n02:25\n\n\n2\n0.095163\n0.100338\n0.960081\n02:24\n\n\n3\n0.051146\n0.039767\n0.984438\n02:28\n\n\n4\n0.024118\n0.044908\n0.982409\n02:30\n\n\n\n\n\n\n\n\n- net2의 순서 바꾸기 전 전체 네트워크:\n\\[\\underset{(1,3,512,512)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{ap}{\\to} \\underset{(1,512,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,512)}{{\\boldsymbol \\sharp}}\\overset{linear}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [-9.0358,  9.0926]\\]\n\n위의 수식을 아래의 수식으로 바꾸고 싶음\n\n순서만 바꾸는 것임 -&gt; 결과는 똑같음\n\n\n\\[\\underset{(1,3,224,224)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{linear}{\\to} \\underset{(1,2,16,16)}{{\\bf why}}\\overset{ap}{\\to} \\underset{(1,2,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [−9.0358,9.0926]\\]\n\n여기서 주목해야 하는 것은 \\(why\\)\n\\(why\\)의 값들을 평균을 내고 이 값을 선형변환하여 하나의 값을 만듦 [−9.0358 or 9.0926]\n\\(why\\)의 tensor를 확인해보자!\n바꾸기 전 net2의 구조 중 일부분 (배치를 넣는 것이 아니라 하나의 이미지를 net에 넣을때임)\n\nX = net1(x)\n\nX.shape = (1, 512, 16, 16)\n\nXX = torch.nn.AdaptiveAvgPool2d(X)\n\nXX.shape =(1, 512, 1, 1)\n\nXXX= torch.nn.Flatten(XX)\n\nXXX.shape -&gt; (1,512)\n\nXXX @ net2[2].weight \\(\\to\\) shape(1, 2)\n\nnet2[2].weight.shape -&gt; (512, 2)\n\n\n\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/havanese_36.jpg')\nx = first(dls.test_dl([ximg]))[0]\n\n\n# 위의 정리된 것들을 보면\n# net2[2].weight의 크기는 (512, 2)\n# net1(x)의 크기는 (1, 512, 16, 16)\n# why의 크기를 (1, 2, 16, 16)으로 만들고 싶음\nwhy = torch.einsum('cb,abij-&gt;acij',\n                   net2[2].weight,#(512, 2)\n                   net1(x) # (1, 512, 16, 16)\n                   )\nwhy.shape\n\ntorch.Size([1, 2, 16, 16])\n\n\n\n# net2[0]은 avg pooling layer -&gt; 높이와 폭을 0으로 만듦\n# (1, 2, 16, 16) -&gt; (1, 2, 1, 1)\nnet2[0](why)\n\nTensorImage([[[[-7.2395]],\n\n              [[ 7.5647]]]], device='cuda:0', grad_fn=&lt;AliasBackward0&gt;)\n\n\n\n\n\n\n\ntensor의 평균 -9.0358\n해당 tensor의 평균이 크다면 고양이라고 판단을 함\n\n하지만 이번 예시에서는 평균이 음수가 나옴 -&gt; 해당 값이 작으면 작을수록 고양이가 아니라고 생각하는 근거가 됨\n\n그러므로 해당 tensor에서 엄청 작은 값들은 고양이가 아니라고 판단하는 근거가 되는 점들임\n\n\nwhy[0,0,:,:].to(torch.int64)\n\nTensorImage([[  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    1,    1,    0,    1,    1,    1,    1,    0,    0,\n                 0,    0,    0,    0,    0],\n             [  -1,    0,    0,    0,    1,    1,    0,    0,   -1,   -2,   -2,\n                -1,    0,    2,    2,    1],\n             [   0,    0,    0,    0,    0,    0,   -2,  -10,  -17,  -18,  -17,\n               -14,   -7,    2,    5,    1],\n             [  -1,    0,    0,    0,    0,    0,   -7,  -26,  -45,  -43,  -38,\n               -34,  -16,    0,    3,    0],\n             [  -1,    0,    0,   -1,   -2,   -3,  -10,  -32,  -56,  -58,  -50,\n               -43,  -22,   -2,    1,    1],\n             [   1,    2,    0,   -6,  -14,  -12,  -11,  -25,  -45,  -66,  -71,\n               -49,  -20,    0,    2,    2],\n             [   8,   10,    2,   -9,  -19,  -18,  -10,  -13,  -34,  -74, -100,\n               -65,  -21,    1,    1,    0],\n             [  11,   12,    2,   -6,  -13,  -13,   -7,   -5,  -23,  -60,  -82,\n               -63,  -21,    0,    0,    0],\n             [   7,    6,    1,    0,   -2,   -4,   -3,   -1,  -12,  -32,  -45,\n               -36,  -14,   -1,    0,   -2],\n             [   0,    1,    0,    1,    2,    0,    0,    0,   -1,   -7,  -12,\n               -11,   -5,   -1,   -1,   -2],\n             [  -2,   -1,   -1,   -1,   -1,   -1,    0,    1,    4,    1,   -1,\n                -1,   -2,   -2,   -2,   -3],\n             [  -4,   -4,   -4,   -5,   -4,   -4,   -4,   -2,   -2,   -3,   -4,\n                -5,   -5,   -5,   -5,   -4],\n             [  -5,   -6,   -6,   -6,   -6,   -6,   -5,   -5,   -5,   -5,   -6,\n                -6,   -6,   -6,   -5,   -5]], device='cuda:0')\n\n\n\n\n\n\ntensor의 평균 9.0926\n해당 tensor는 강아지라고 판단하는 tensor라고 생각하면 됨\n그러므로 해당 tensor에서 엄청 작은 값들은 고양이가 아니라고 판단하는 근거가 되는 점들임\n\n\nwhy[0,1,:,:].to(torch.int64)\n\nTensorImage([[ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 2,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 1,  0,  0,  0,  0,  0,  0,  0,  1,  2,  2,  2,  0, -1, -1,  0],\n             [ 0,  0,  0,  0,  0,  0,  2, 10, 16, 18, 17, 14,  7, -1, -3, -1],\n             [ 1,  1,  0,  0,  0,  0,  6, 25, 43, 42, 37, 32, 16,  1, -2,  0],\n             [ 1,  0,  0,  1,  2,  3, 10, 32, 55, 58, 50, 42, 21,  3,  0,  0],\n             [ 0, -1,  0,  6, 13, 12, 11, 24, 44, 65, 69, 49, 20,  1, -1, -1],\n             [-7, -8, -1,  9, 18, 17, 10, 13, 33, 72, 97, 65, 22,  0,  0,  0],\n             [-9, -9, -2,  5, 12, 13,  7,  5, 22, 58, 81, 62, 21,  1,  0,  1],\n             [-6, -6, -1,  1,  2,  4,  3,  1, 11, 31, 43, 34, 14,  1,  1,  2],\n             [ 0,  0,  0,  0,  0,  0,  1,  0,  1,  7, 12, 10,  5,  1,  1,  3],\n             [ 3,  2,  2,  2,  2,  1,  1,  0, -2,  0,  1,  2,  2,  3,  3,  4],\n             [ 5,  6,  6,  6,  6,  6,  5,  3,  4,  5,  5,  6,  6,  6,  6,  6],\n             [ 5,  6,  7,  7,  7,  7,  6,  6,  6,  6,  6,  7,  7,  7,  6,  6]],\n            device='cuda:0')\n\n\n\n\n\n\n\n\nwhy의 값과 이미지를 동시에 출력한다면 그림의 어떠한 부분을 보고 강아지라고 판단을 한 것인지 알 수 있음\n\n\nwhy_cat = why[0,0,:,:] # 해당 tensor의 값이 작으면 작을 수록 고양이가 아니라고 판단하는 근거가 됨\nwhy_dog = why[0,1,:,:] # 해당 tensor의 값이 크면 클수록 강아지라고 판단하는 근거가 됨\n\n\n밑에서 cmap을 magma로 지정하였는 이는 값이 작은 부분은 검정색 큰 부분은 노란색으로 표시함\n\n검정 -&gt; 보라 -&gt; 빨강 -&gt; 노랑\n\n이를 해석하면 why_cat에서 값이 가장 작은 부분, 즉 검정색이 고양이가 아니라고 판단함\nwhy_dog에서는 값이 가장 큰 부분, 즉 노란색이 강아지라고 판단하는 영역을 표시함\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma')\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc99c0810&gt;\n\n\n\n\n\n\nwhy의 크기 조절\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc90e9410&gt;\n\n\n\n\n\n\n크기조절 후 겹쳐그리기\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n\n&lt;matplotlib.image.AxesImage at 0x7f7cc8df78d0&gt;\n\n\n\n\n\n\n\n\n# why값을 확률로 출력하기\nsftmax=torch.nn.Softmax(dim=1)\nsftmax(net(x))\ncatprob, dogprob = sftmax(net(x))[0,0].item(), sftmax(net(x))[0,1].item()\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[0].set_title('catprob= %f' % catprob) \nax[1].imshow(torch.einsum('ocij -&gt; ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].set_title('dogprob=%f' % dogprob)\n\nText(0.5, 1.0, 'dogprob=1.000000')\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=0 \nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -&gt; acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob&gt;dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()"
  }
]