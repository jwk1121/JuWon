{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Feature-Based Knowledge Distillation\n",
    "categories:\n",
    "    - DL\n",
    "    - Knowledge Distillation\n",
    "    - paper review\n",
    "author: JuWon\n",
    "date: 7/18/2023\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-Based Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](RBKD3.png)\n",
    "\n",
    "- Distillation Loss = 모델의 중간 층의 출력인 Attention map을 MSE Loss를 통해 비교한 Loss 2개 + student model의 Cross entropy Loss 1개\n",
    "\n",
    "- Teacher Model(ResNet 18 + SE Block)과 Student Model(CNN with 3 Layers + SE Block)은 사전에 학습을 시킨 후 불러왔음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Distillation을 위한 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction_ratio)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction_ratio, in_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        out = self.pool(x).view(b, c)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = torch.sigmoid(self.fc2(out)).view(b, c, 1, 1)\n",
    "        out = out.expand(b, c, h, w)\n",
    "        return out\n",
    "    \n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride = 1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes,kernel_size=3,padding=1, stride = stride, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,padding=1, stride = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.se = SEBlock(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1: # stride가 1이 아니면 image shape이 변형됨 # stride가 1이면 그냥 패스\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=3, padding=1,stride= stride, bias = False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = F.relu(self.bn1(self.conv1(x)))\n",
    "        output = self.bn2(self.conv2(output))\n",
    "        output = self.se(output) * output\n",
    "        # skip connection\n",
    "        output += self.shortcut(x)\n",
    "        output = F.relu(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks - 1) # stride = 1 -> strides = [1,1] # stride = 2 -> strides = [2,1]\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = F.relu(self.bn1(self.conv1(x))) # 채널수만 바뀜\n",
    "        output = self.layer1(output) #\n",
    "        output = self.layer2(output)\n",
    "        output = self.layer3(output)\n",
    "        output = self.layer4(output)\n",
    "        output = F.avg_pool2d(output, 4) # img shape 1/4\n",
    "        output = output.view(output.size(0), -1) # FC Layer\n",
    "        output = self.linear(output) # img shape = [batch size, num_classes]\n",
    "        return output\n",
    "\n",
    "    def get_features1(self, x):\n",
    "        output = F.relu(self.bn1(self.conv1(x)))\n",
    "        output = self.layer1(output)\n",
    "        output = self.layer2(output)# 16, 16\n",
    "        output = self.layer3(output)# 8, 8\n",
    "        output = torch.sum(output,1)\n",
    "        output = output.view(output.size(0),-1)\n",
    "        return output\n",
    "\n",
    "    def get_features2(self, x):\n",
    "        output = F.relu(self.bn1(self.conv1(x)))\n",
    "        output = self.layer1(output)\n",
    "        output = self.layer2(output) # 16,16\n",
    "        output = self.layer3(output) # 8, 8\n",
    "        output = self.layer4(output) # 4, 4\n",
    "        output = torch.sum(output,1)\n",
    "        output = output.view(output.size(0), -1) # FC Layer\n",
    "        return output\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "class student_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(student_CNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64)\n",
    "            )\n",
    "        self.se1 = SEBlock(64)\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64)\n",
    "            )\n",
    "        self.se2 = SEBlock(64)\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128)\n",
    "            )\n",
    "        self.se3 = SEBlock(128)  \n",
    "\n",
    "        self.linear = nn.Linear(512, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = F.relu(self.layer1(x))\n",
    "        output = self.se1(output) * output # 64,16,16 = 16384\n",
    "        \n",
    "        output = F.relu(self.layer2(output))\n",
    "        output = self.se2(output) * output # 64,8,8 = 4096\n",
    "        \n",
    "        output = F.relu(self.layer3(output))\n",
    "        output = self.se3(output) * output # 128,4,4 = 2048\n",
    "\n",
    "        output = F.avg_pool2d(output, 2)\n",
    "        \n",
    "        output = output.view(output.size(0),-1)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "    def get_features1(self, x):\n",
    "        output = F.relu(self.layer1(x)) # 16,16\n",
    "        output = self.se1(output) * output\n",
    "\n",
    "        output = F.relu(self.layer2(output))\n",
    "        output = self.se2(output) * output\n",
    "\n",
    "        output = torch.sum(output,1)\n",
    "        output = output.view(output.size(0),-1)\n",
    "        return output\n",
    "    \n",
    "    def get_features2(self, x):\n",
    "        output = F.relu(self.layer1(x)) # 16,16\n",
    "        output = self.se1(output)* output\n",
    "\n",
    "        output = F.relu(self.layer2(output)) # 8,8\n",
    "        output = self.se2(output) * output\n",
    "\n",
    "        output = F.relu(self.layer3(output)) # 4,4\n",
    "        output = self.se3(output) * output\n",
    "        \n",
    "        output = torch.sum(output,1)\n",
    "        output = output.view(output.size(0),-1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10('./data', train=True, download=True,transform=transform_train)\n",
    "test_data = datasets.CIFAR10('./data', train=False, download=True,transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size= 128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size= 100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher, Student model 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_model = ResNet18()\n",
    "T_model.load_state_dict(torch.load('teacher_CNN_parm.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_model = student_CNN()\n",
    "S_model.load_state_dict(torch.load('student_CNN_parm.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher, Student model 정확도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model acc: 0.9140\n"
     ]
    }
   ],
   "source": [
    "T_model.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "T_model = T_model.to(device)\n",
    "T_acc = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x,y) in enumerate(test_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        yhat = T_model(x)\n",
    "        y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n",
    "        T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\n",
    "print(\"Teacher model acc: {:.4f}\".format(sum(T_acc)/len(T_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student model acc without distillation: 0.7668\n"
     ]
    }
   ],
   "source": [
    "S_model.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "S_model = S_model.to(device)\n",
    "T_acc = []\n",
    "for batch_idx, (x,y) in enumerate(test_loader):\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    yhat = S_model(x)\n",
    "    y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n",
    "    T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\n",
    "print(\"student model acc without distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Distillation Loss with MSE Loss(attention map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같은 실험을 통해 적절한 loss 가중치를 찾음\n",
    "\n",
    "``` python\n",
    "T_model.eval()\n",
    "S_model1.eval()\n",
    "loss_fn1 = nn.MSELoss()\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x, target) in tqdm(enumerate(train_loader)):\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        target = F.one_hot(target.to(torch.int64), num_classes = 10)\n",
    "        \n",
    "        # attention map 1 loss 계산\n",
    "        teacher_fm1 = T_model.get_features1(x)\n",
    "        student_fm1 = S_model1.get_features1(x)\n",
    "        loss1 = loss_fn1(student_fm1, teacher_fm1)\n",
    "        \n",
    "        # attention map 2 loss 계산\n",
    "        teacher_fm2 = T_model.get_features2(x)\n",
    "        student_fm2 = S_model1.get_features2(x)\n",
    "\n",
    "        loss2 = loss_fn1(student_fm2, teacher_fm2)\n",
    "        \n",
    "        # Cross entropy loss 계산\n",
    "        student_pred = S_model1(x)\n",
    "\n",
    "        loss3 = loss_fn2(student_pred.float(), target.float())\n",
    "\n",
    "        print(loss1,loss2*0.1,loss3*1000)\n",
    "        break\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Distillation_train(T_model, S_model, train_loader, test_loader, epochs, lr = 0.001):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 128\n",
    "\n",
    "    T_model = T_model.to(device)\n",
    "    S_model = S_model.to(device)\n",
    "    optimizr = optim.Adam(S_model.parameters(), lr= lr)\n",
    "    \n",
    "    loss_fn1 = nn.MSELoss()\n",
    "    loss_fn2 = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loss = []\n",
    "    for epch in range(epochs):\n",
    "        epch += 1\n",
    "\n",
    "        t_epoch_start = time.time()\n",
    "        epch_loss = 0.0\n",
    "        val_epch_loss = 0.0\n",
    "\n",
    "        # 모델 학습\n",
    "        T_model.eval()\n",
    "        S_model.train()\n",
    "\n",
    "        for batch_idx, (x, target) in tqdm(enumerate(train_loader)):\n",
    "            x, target = x.to(device), target.to(device)\n",
    "            target = F.one_hot(target.to(torch.int64), num_classes = 10)\n",
    "\n",
    "            # attention map 1 loss 계산\n",
    "            teacher_fm1 = T_model.get_features1(x)\n",
    "            student_fm1 = S_model.get_features1(x)\n",
    "\n",
    "            loss1 = loss_fn1(student_fm1, teacher_fm1)\n",
    "            \n",
    "            # attention map 2 loss 계산\n",
    "            teacher_fm2 = T_model.get_features2(x)\n",
    "            student_fm2 = S_model.get_features2(x)\n",
    "\n",
    "            loss2 = loss_fn1(student_fm2, teacher_fm2)\n",
    "            \n",
    "            # Cross entropy loss 계산\n",
    "            student_pred = S_model(x)\n",
    "\n",
    "            loss3 = loss_fn2(student_pred.float(), target.float())\n",
    "\n",
    "            # loss sum\n",
    "            loss = loss1 + loss2*0.1 + loss3*10000\n",
    "\n",
    "            optimizr.zero_grad()\n",
    "\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizr.step()\n",
    "\n",
    "            epch_loss += loss.item()\n",
    "\n",
    "        # 모델 검증\n",
    "        S_model.eval()\n",
    "        T_acc = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x,y) in enumerate(test_loader):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                yhat = S_model(x)\n",
    "                y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n",
    "                T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\n",
    "\n",
    "        # 모델 결과 확인\n",
    "        t_epoch_finish = time.time()\n",
    "        train_loss.append(epch_loss / batch_size)\n",
    "        if epch%5==0:\n",
    "            clear_output(wait = True)\n",
    "        print('Epoch: {}, Train_Loss: {:4f}, Test acc: {:4f}, Epoch_time: {:.4f}'.format(epch, epch_loss, sum(T_acc)/len(T_acc), t_epoch_finish - t_epoch_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "S_model1 = student_CNN()\n",
    "Distillation_train(T_model, S_model1, train_loader, test_loader, epochs= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obj = S_model1.state_dict(), f= 'student_dist_CNN.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distllation model 불러오기 및 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_model1 = student_CNN()\n",
    "S_model1.load_state_dict(torch.load('student_dist_CNN.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student model acc with distillation: 0.7768\n"
     ]
    }
   ],
   "source": [
    "S_model1.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "S_model1 = S_model1.to(device)\n",
    "T_acc = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x,y) in enumerate(test_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        yhat = S_model1(x)\n",
    "        y_pred = np.argmax(yhat.detach().to('cpu'), axis=1)\n",
    "        T_acc.append((y_pred == y.to('cpu')).sum().item() / len(y))\n",
    "print(\"student model acc with distillation: {:.4f}\".format(sum(T_acc)/len(T_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model|Accuracy|Distillation|\n",
    "|:---:|:---:|:---:|\n",
    "|Teacher|91.40%|x|\n",
    "|Student|76.68%|x|\n",
    "|Student|77.68%|o|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
