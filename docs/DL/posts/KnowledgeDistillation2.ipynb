{
 "cells": [
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Knowledge Distillation paper review part2\n",
    "subtitle: Large Scale Model → Small Scale Model [Distillation Scheme & Teacher–Student Architecture]\n",
    "categories:\n",
    "    - DL\n",
    "    - Knowledge Distillation\n",
    "    - paper review \n",
    "author: JuWon\n",
    "date: 6/28/2023\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation review - Distillation Scheme & Teacher–Student Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Knowledge Distillation에 대해 정리한 survey논문 리뷰\n",
    "- Knowledge Distillation model (KD model)의 목적을 간략히 설명하면 large scale model(teacher model)을 KD model을 통해 small scale model(student model)로 압축(compression)하여 파라미터를 최적화 시키는 것\n",
    "- 이번 페이지에서는 small scale model에게 knowledge를 옮길 때 어떠한 방식으로 student model을 학습시키는 지에 대해 설명할 것"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation Schemes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](DS1.png)\n",
    "\n",
    "`※` 이러한 3가지의 Distillation Scheme은 서로를 보완하기 위해서 합쳐서 사용될 때도 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Distillation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`main idea` 잘 학습된 모델(knowledgeable teacher)가 작은 모델(student)을 특성을 match시키거나 분포를 match시키는 방향으로 학습 진행  \n",
    "\n",
    "`Offline Distillation 진행 순서`  \n",
    "1. large teacher model을 training set를 사용하여 훈련시킴  \n",
    "2. 학습된 teacher model을 사용해 knowledge를 logits 또는 중간층의 feature 형태로 추출하고 이를 통해 student model 학습   \n",
    "\n",
    "`단점` student model의 성능을 결국 teacher model의 성능을 따르게 되는데 좋은 성능의 teacher model은 복잡하고 용량이 커서 학습에 오랜시간이 걸린다.   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Distillation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`main idea` large-scale model(teacher model)과 small scale model(student model)을 동시에 학습을 시키는 것으로 Offline Distillation에서 complex high-capacity teacher model이 필요하다는 단점을 극복하기 위해 만들어진 기법  \n",
    "- Online Distillation은 one-phase end-to-end 구조를 띄고 있다는 장점이 잇지만 \n",
    "\n",
    "`example of Online Distillation`  \n",
    "- 일반화 성능을 높이기 위해서는 ensemble of soft logits을 사용하여 deep mutual learning을 확장시킴  \n",
    "- 연산량을 줄이기 위해서는 각 branch는 student model을 나타내고 different branch는 같은 backbone network를 공유하는 multibracnh architecture를 제안함  \n",
    "\n",
    "`※` 고용량의 teacher model을 Online 환경에서 다루지 못하여 Online 환경에서 teacher model과 student model의 관계를 파악하는 것이 흥미로운 주제임"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Distillation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`main idea` 작은 모델(student)이 스스로 학습을 하는 것으로 같은 network를 teacher model과 student model이 사용한다.  \n",
    "`+` Online Distillation의 특별한 case로 여겨질 수 있음\n",
    "\n",
    "`example of Self-Distillation`  \n",
    "- Snapshot distillation은 self-distillation의 특별한 이형으로 초기 에포크의 네트워크 knowledge가 teacher model이 되어 마지막 부분의 에포크의 네트워크에게 knowledge를 전달하는 구조  \n",
    "- feature embedding space에 있는 데이터 유사도를 반영하는 기법  \n",
    "- class-wise self-knowledge distillation은 학습된 모델의 intra-class samples의 결과 분포와 augmented samples의 결과 분포를 match시키는 기법"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher–Student Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](TSA1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
