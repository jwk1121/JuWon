{
 "cells": [
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Knowledge Distillation paper review - Distillation\n",
    "subtitle: Large Scale Model → Small Scale Model\n",
    "categories:\n",
    "    - DL\n",
    "    - Knowledge Distillation\n",
    "    - paper review \n",
    "author: JuWon\n",
    "date: 6/28/2023\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation review - Distillation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Knowledge Distillation에 대해 정리한 survey논문 리뷰\n",
    "- Knowledge Distillation model (KD model)의 목적을 간략히 설명하면 large scale model(teacher model)을 KD model을 통해 small scale model(student model)로 압축(compression)하여 파라미터를 최적화 시키는 것\n",
    "- 이번 페이지에서는 small scale model에게 knowledge를 옮길 때 어떠한 방식으로 student model을 학습시키는 지에 대해 설명할 것"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation Schemes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](DS1.png)\n",
    "\n",
    "`※` 이러한 3가지의 Distillation Scheme은 서로를 보완하기 위해서 합쳐서 사용될 때도 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Distillation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`main idea` 잘 학습된 모델(knowledgeable teacher)가 작은 모델(student)을 특성을 match시키거나 분포를 match시키는 방향으로 학습 진행  \n",
    "\n",
    "`Offline Distillation 진행 순서`  \n",
    "1. large teacher model을 training set를 사용하여 훈련시킴  \n",
    "2. 학습된 teacher model을 사용해 knowledge를 logits 또는 중간층의 feature 형태로 추출하고 이를 통해 student model 학습   \n",
    "\n",
    "`단점` student model의 성능을 결국 teacher model의 성능을 따르게 되는데 좋은 성능의 teacher model은 복잡하고 용량이 커서 학습에 오랜시간이 걸린다.   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Distillation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`main idea` large-scale model(teacher model)과 small scale model(student model)을 동시에 학습을 시키는 것으로 Offline Distillation에서 complex high-capacity teacher model이 필요하다는 단점을 극복하기 위해 만들어진 기법"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Distillation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`main idea` 작은 모델(student)이 스스로 학습을 하는 것"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
