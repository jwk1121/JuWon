{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Knowledge Distillation paper review\n",
    "subtitle: Large Scale Model → Small Scale Model\n",
    "categories:\n",
    "    - DL\n",
    "    - Knowledge Distillation\n",
    "    - paper review \n",
    "author: JuWon\n",
    "date: 6/27/2023\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Knowledge Distillation에 대해 정리한 survey논문 리뷰\n",
    "- Knowledge Distillation model (KD model)의 목적을 간략히 설명하면 large scale model(teacher model)을 KD model을 통해 small scale model(student model)로 압축(compression)하여 파라미터를 최적화 시키는 것"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge\n",
    "\n",
    "![img](KnowledgeDistillation_overview.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response-Based Knowledge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](RBKD2.png)\n",
    "\n",
    "`main idea` 해당 모델은 large scale model의 last ouput layer를 사용하여 small scale model이 이를 모방(mimic)하게 하는 것 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Fuction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Distillation Loss** for soft logits  \n",
    "$L_{ResD} (p(z_{t}, T), p(z{s}, T)) = L_{R} (p(z_{t}, T), p(z_{S}, T))$ *우항은 Kullback-Leibler divergence Loss를 따름*  \n",
    "$where$  $logits = ln(odds) = ln (\\frac{p}{1-p})$, Softmax(T = t) = $p(z_{i}, T)$ =$\\frac{e^{a/t}}{e^{a/t} + e^{b/t} + e^{c/t}}$  \n",
    "이를 정리하면 large scale model의 output(soft target)과 small scale model의 output(soft target)의 차이를 최소화 시키는 방향으로 학습을 진행"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Student Loss** 는 small scale model의 output을 정답(label)과 매치시켜 학습을 진행  \n",
    "$L_{CE}(y, p(z_{s}, T = 1))$  \n",
    "$where$ Softmax(T = 1) = $\\frac{e^{a}}{e^{a} + e^{b} + e^{c}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-Based Knowledge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](RBKD3.png)  \n",
    "`main idea` feature activation을 직접적으로 매치시켜서 각 layer를 유사하도록 학습을 진행  \n",
    "*Feature-Based Knowledge는 얇고 깊은 network에 적합함*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 일반적인 Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Distillation Loss** for feature-based knowledge transfer  \n",
    "$L_{FeaD}(f_{t}(x), f_{s}(x))$ = $L_{F}(\\phi_{t}(f_{t}(x)), \\phi_{t}(f_{t}(x)))$  \n",
    "$where$  $\\phi$ 는 image shape을 바꿔주는 function,  \n",
    "$f_{t}(x), f_{s}(x)$는 중간 층(intermediate layers)의 activation map을 의미,  \n",
    "$L_{F}()$ 는 feature map간의 유사도를 확인해주는 function  \n",
    "이를 정리하면 상황에 따라 $f_{t}()$가 attention map, Feature map이 될 수 는 있지만 궁극적으로는 Teature Model의 layer와 Student Model의 layer를 매치시켜 최대한 유사하도록 학습을 진행"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`※` Teature Model의 hint layer와 Student Model의 guided layer를 어떻게 효율적으로 선택해야 하는지는 지속적인 연구가 필요 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation-Based Knowledge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](RBKD4.png)  \n",
    "`main idea` 다른 층 간의 상관관계 혹은 데이터 샘플간의 상관관계를 탐색하여 이를 바탕으로 학습을 진행  \n",
    "`example)`  \n",
    "1. feature map간의 데이터 상관관계를 탐색하고 Student model은 이를 사용해 상호간의 정보 흐름을 모방하며 학습  \n",
    "2. 각기 다른 Teacher Model의 상관관계와 중요도를 2개의 그래프로 표현하는데 이는 logits과 feature를 사용함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Distillation Loss** of relation-based knowledge based on the relations of **feature maps**  \n",
    "$L_{RelD}(f_{t}, f_{s})$ = $L_{R^1}(\\psi_{t}(\\hat f_{t}, \\check f_{t}), \\psi_{t}(\\hat f_{s}, \\check f_{s}))$  \n",
    "$where$ $\\hat f_{t,s}, \\check f_{t,s}$ 는 Teature, Student Model로부터 선택된 feature map 쌍,  \n",
    "$\\psi ()$ 는 feature map 간의 유사도를 확인해주는 function,  \n",
    "$L_{R^1}()$ 는 Teature, Student feature map간의 상관관계를 확인해주는 function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Distillation Loss** of relation-based knowledge based on the relations of **instance relationship**  \n",
    "$L_{RelD}(f_{t}, f_{s})$ = $L_{R^2}(\\psi_{t}(t_{i}, t_{j}), \\psi_{t}(s_{i}, s_{j}))$  \n",
    "$where$ $(t_{i},t_{j}) \\in F_{t}, (s_{i},s_{j}) \\in F_{s}$  \n",
    "$F_{t,s}$ 는 Teature, Student Model로부터의 sets of feature representation,  \n",
    "$\\psi ()$ 는 feature representation 간의 유사도를 확인해주는 function,  \n",
    "$L_{R^2}()$ 는 Teature, Student feature map간의 상관관계를 확인해주는 function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation Schemes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](DS1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Distillation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
