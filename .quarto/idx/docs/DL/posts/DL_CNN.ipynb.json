{"title":"CNN part1","markdown":{"yaml":{"title":"CNN part1","subtitle":"CNN part1","categories":["DL","CNN"],"author":"JuWon","date":"2/14/2022"},"headingText":"DL CNN 정리","containsRefs":false,"markdown":"\n\n\n- toc:true\n\n## CNN 구조\n\n\n간략하게 CNN의 구조를 설명하자면\n- conv layer: 커널층(선형변환처럼 feature를 늘려주는 역할)\n- ReLU layer: 비선형을 추가해 표현력을 늘려주는 역할\n- pooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n### conv layer\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임) \n- 해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음\n- 윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함\n  - 첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용\n  - 두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n---\n\n### ReLU layer\n\n- 사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.  \n$ReLU(x) = \\max(0,x)$\n\n### pooling layer\n- pooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\n- maxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n- maxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\n- pooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n  - 이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다\n\n## CNN 구현\n\n### path 사용법 + 데이터 준비\n- \n``` python\n(path/'원하는 경로').ls()\n```\n위 코드를 사용하면 폴더 안에 있는 데이터들의 이름을 출력\n- \n``` python\ntorchvision.io.read_image()\n```\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n### cnn에 사용되는 layer 알아보기\n\n- size계산 공식: 윈도우(커널)사이즈가 n이면 $size = height(width) - ( n - 1)$\n\n- 행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n### networks 설계 \n\n- 원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n```python\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n```\n$\\to$ overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n\n- 데이터 로더는 배치크기를 지정해줘야 함\n\n- 데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n  - stochastic gradient descent(구용어: mini-batch gradient descent)\n\n- Learner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음\n> ``` python \n  net[0].weight\n  ```\n  위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = 'cuda:0'라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다. \n   \n   `+` tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n## Loss function\n\n### BCEWithLogitsLoss\n- BCEWithLogitsLoss = Sigmoid + BCELoss\n- 손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n- 이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음\n\n## k개의 클래스를 분류하는 모델\n\n>다중(k개) 클래스를 분류\n- LossFunction: CrossEntrophyLoss  \n- ActivationFunction: SoftmaxFunction\n- 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n### SoftmaxFunction\n- 소프트맥스 함수가 계산하는 과정은 아래와 같음\n(3개를 분류할 경우) $softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}$\n\n### CrossEntrophyLoss\n- k개의 클래스를 분류하는 모델의 Loss 계산 방법\n\n``` python\nsftmax(_netout) # -> 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -> 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n```\n- 만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -> 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n\n- 최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n`+` 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임  \n`+` 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n### 정답(y)의 형태\n- type 1) int형을 사용하는 방법 (vector)\n\n- type 2) float형을 사용하는 방법 (one-hot encoded vector)\n\n- 만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n  - type 1)의 경우 <사슴: 0, 강아지: 1, 고양이: 2> (단, 데이터 형태는는 int(정수))\n  - type 2)의 경우 <사슴: [1, 0, 0],\n강아지: [0, 1, 0], 고양이: [0, 0, 1] > (단, 데이터의 형태는 float(실수))\n\n#### vector(int)\n\n#### one-hot encoded vector(float)\n\n### 다중 클래스 분류 모델\n\n- 0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼 \n\n### 소프트맥스와 시그모이드\n\n- 이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n- 이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n- 소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n### 메트릭이용\n- fastai에서 지원\n- fastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\n- y의 형태를 주의해서 사용해야 함\n  - 앞서 말한 것처럼 두 가지 타입이 있음\n    - vector + int\n    - one-hot encoded vector + float\n\ntype 1) y의 형태가 vector + int일 때\n- metrics = accuracy를 사용해야 함\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때\n- metrics = accuracy_multi를 사용해야 함\n- error_rate는 사용못함\n\n- torch.nn.functional.one_hot() 함수 조건\n  - 기본적으로 크기가 n인 벡터가 들어오길 기대 \n  - 정수가 들어오는 것을 기대\n- 하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n","srcMarkdownNoYaml":"\n\n# DL CNN 정리\n\n- toc:true\n\n## CNN 구조\n\n\n간략하게 CNN의 구조를 설명하자면\n- conv layer: 커널층(선형변환처럼 feature를 늘려주는 역할)\n- ReLU layer: 비선형을 추가해 표현력을 늘려주는 역할\n- pooling layer: max 또는 avg를 통해 데이터를 요약해주는 역할\n\n### conv layer\n\nconv_tensor[1,1] = [[0, 1],[5, 6]]의 평균임을 알 수 있음 (conv.weight가 모두 1/4이어서 그런거임) \n- 해당 값은 (0 + 1 + 5 + 6) / 4 임을 알 수 있음\n- 윈도우(커널)는 한 칸씩 움직이면서 weight를 곱하고 bias를 더함\n  - 첫 번째 3이라는 값은 [[0, 1],[5, 6]]에 conv을 적용\n  - 두 번째 4라는 값은 [[1, 2],[6, 7]]에 conv을 적용\n\n---\n\n### ReLU layer\n\n- 사실 ReLU는 DNN에서와 같이 음수는 0으로 양수는 그대로 되는 함수이다.  \n$ReLU(x) = \\max(0,x)$\n\n### pooling layer\n- pooling layer에는 maxpooling이 있고 avgpooling이 있지만 이번에는 maxpooling을 다룰것임\n- maxpooling은 데이터 요약보다는 크기를 줄이는 느낌이 있음\n\n- maxpooling_tensor[1,1] = [[0, 1],[5, 6]] 중 가장 큰 값을 이므로 5이다.\n- pooling은 convlayer와 달리 pooling box가 겹치지 않게 움직임\n  - 이러한 특성으로 인해 5번째 열과 행에 있는 값들은 pooling box에 들어가지 못해 버려지게 된다\n\n## CNN 구현\n\n### path 사용법 + 데이터 준비\n- \n``` python\n(path/'원하는 경로').ls()\n```\n위 코드를 사용하면 폴더 안에 있는 데이터들의 이름을 출력\n- \n``` python\ntorchvision.io.read_image()\n```\n해당 함수에 데이터 이름(이미지 이름)을 넣어주면 이미지를 tensor형태로 출력\n\n### cnn에 사용되는 layer 알아보기\n\n- size계산 공식: 윈도우(커널)사이즈가 n이면 $size = height(width) - ( n - 1)$\n\n- 행과 열이 2의 배수이므로 maxpool이 2일때는 버려지는 행과 열은 없다.\n\n### networks 설계 \n\n- 원래라면 아래와 같은 코드를 사용하여 network를 학습시키겠지만 CNN과 같이 파라미터가 많은 network는 CPU로 연산시 학습할때 시간이 오래걸림\n```python\nloss_fn=torch.nn.BCELoss()\noptimizr= torch.optim.Adam(net.parameters())\nt1= time.time()\nfor epoc in range(100): \n    ## 1 \n    yhat=net(x_tr)\n    ## 2 \n    loss=loss_fn(yhat,y_tr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n```\n$\\to$ overview때 배운 fastai에 있는 데이터로더를 사용하면 GPU를 사용해 연산을 할 수 있다.\n\n\n- 데이터 로더는 배치크기를 지정해줘야 함\n\n- 데이터가 각각 12665, 2115개씩 들어가 있으므로 한번 업데이트할 때 총 데이터의 1 /10을 쓰도록 아래와 같이 배치 크기를 줌\n  - stochastic gradient descent(구용어: mini-batch gradient descent)\n\n- Learner 오브젝트에 들어간 net은 gpu상에 있도록 되어있음\n> ``` python \n  net[0].weight\n  ```\n  위 코드를 출력하면 weight tensor가 출력되는데 가장 밑을 확인하면 device = 'cuda:0'라는 것이 있음. 이는 해당 tensor가 gpu상에 위치해있는 것을 의미한다. \n   \n   `+` tensor 연산을 할 때에는 모든 tensor가 같은 곳에 위치해있어야 한다.\n\n## Loss function\n\n### BCEWithLogitsLoss\n- BCEWithLogitsLoss = Sigmoid + BCELoss\n- 손실함수로 이를 사용하면 network 마지막에 시그모이드 함수를 추가하지 않아도 됨\n- 이를 사용하면 수치적으로 더 안정이 된다는 장점이 있음\n\n## k개의 클래스를 분류하는 모델\n\n>다중(k개) 클래스를 분류\n- LossFunction: CrossEntrophyLoss  \n- ActivationFunction: SoftmaxFunction\n- 마지막 출력층: torch.nn.Linear(n, k)\n\n\n\n### SoftmaxFunction\n- 소프트맥스 함수가 계산하는 과정은 아래와 같음\n(3개를 분류할 경우) $softmax=\\frac{e^{a 또는 b 또는 c}}{e^{a} + e^{b} + e^{c}}$\n\n### CrossEntrophyLoss\n- k개의 클래스를 분류하는 모델의 Loss 계산 방법\n\n``` python\nsftmax(_netout) # -> 0 ~ 1 사이의 값 k개 출력\n\ntorch.log(sftmax(_netout)) # -> 0 ~ 1사이의 값을 로그에 넣게 되면 -∞ ~ 0사이의 값 k개 출력\n\n- torch.log(sftmax(_netout)) * _y_onehot \n# 만약 값이 log(sftmax(_netout)) = [-2.2395, -2.2395, -0.2395] 이렇게 나오고 \n#y_onehot이 [1, 0, 0]이라면 해당 코드의 결과, 즉 loss는 2.2395이 된다. \n\n```\n- 만약 모델이 첫 번째 값이 확실하게 정답이라고 생각한다면 로그의 결과값은 0이 된다 -> 해당 값이 정답일경우 0 * 1 = 0이므로 loss는 0이 된다\n\n- 최종적으로는 위 코드를 통해 얻은 Loss를 평균을 내어 출력한다.\n\n`+` 위의 설명은 정답이 원-핫 인코딩 형식으로 되어있을 때의 Loss계산 방법임  \n`+` 정답이 vector + 정수형으로 되어 있을 때의 Loss계산 방법은 잘 모르겠음\n\n### 정답(y)의 형태\n- type 1) int형을 사용하는 방법 (vector)\n\n- type 2) float형을 사용하는 방법 (one-hot encoded vector)\n\n- 만약 사슴, 강아지, 고양이를 분류하는 모델이라면\n  - type 1)의 경우 <사슴: 0, 강아지: 1, 고양이: 2> (단, 데이터 형태는는 int(정수))\n  - type 2)의 경우 <사슴: [1, 0, 0],\n강아지: [0, 1, 0], 고양이: [0, 0, 1] > (단, 데이터의 형태는 float(실수))\n\n#### vector(int)\n\n#### one-hot encoded vector(float)\n\n### 다중 클래스 분류 모델\n\n- 0으로 분류한 것들은 첫번째 열(0)의 값이 가장 큼 \n\n### 소프트맥스와 시그모이드\n\n- 이진 분류시에는 소프트맥스와 시그모이드 모두 activation function으로 사용할 수 있지만 소프트맥스를 사용하면 출력층이 2개가 되므로 파라미터 낭비가 심해진다.\n- 이진 분류시에는 시그모이드를 사용하는 것이 적합함.\n- 소프트맥스는 3개 이상을 분류해야 할 경우에 사용하면 됨\n\n### 메트릭이용\n- fastai에서 지원\n- fastai를 사용해 학습(lrnr.fit())을 할때 loss_value만 나오는 것이 아니라 error_rate과 정확도가 나오게 할 수 있는 옵션\n- y의 형태를 주의해서 사용해야 함\n  - 앞서 말한 것처럼 두 가지 타입이 있음\n    - vector + int\n    - one-hot encoded vector + float\n\ntype 1) y의 형태가 vector + int일 때\n- metrics = accuracy를 사용해야 함\n\ntype 2) y의 형태가 one-hot encoded vector + float일 때\n- metrics = accuracy_multi를 사용해야 함\n- error_rate는 사용못함\n\n- torch.nn.functional.one_hot() 함수 조건\n  - 기본적으로 크기가 n인 벡터가 들어오길 기대 \n  - 정수가 들어오는 것을 기대\n- 하지만 원-핫 인코딩을 사용할 때에는 실수형으로 저장 되어야 하므로 마지막에 실수형으로 바꿔줘야 함\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"include-after-body":["../signup.html"],"output-file":"DL_CNN.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":true,"toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"visual","theme":"cosmo","title-block-banner":true,"toc-location":"left","title":"CNN part1","subtitle":"CNN part1","categories":["DL","CNN"],"author":"JuWon","date":"2/14/2022"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}